# Triton Server configuration settings
# This file will be used to configure the Triton server in the start.sh script

# HTTP/REST API settings
--http-port=8000
--http-thread-count=8
--http-timeout-ms=300000

# GRPC settings
--grpc-port=8001
--grpc-infer-allocation-pool-size=16
--grpc-use-ssl=0
--grpc-keepalive-time=7200
--grpc-keepalive-timeout=20

# Metrics settings
--metrics-port=8002
--metrics-interval-ms=2000

# Model repository settings
--model-repository=/models
--model-control-mode=explicit
--repository-poll-secs=60
--exit-on-error=false
--strict-model-config=false
--model-load-thread-count=4

# Cache settings
--response-cache-byte-size=1073741824  # 1GB
--cache-management-mode=memory
--min-supported-compute-capability=7.0

# Logging settings
--log-verbose=0 
--log-info=true
--log-error=true
--log-warning=true
--log-format=default

# Performance settings
--pinned-memory-pool-byte-size=268435456  # 256MB
--cuda-memory-pool-byte-size=0:1073741824  # 1GB for GPU ID 0
--backend-directory=/opt/tritonserver/backends
--backend-config=tensorflow,version=2
--allow-gpu-metrics=true
--allow-metrics=true 