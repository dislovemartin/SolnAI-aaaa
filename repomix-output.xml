<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: **/*.log, tmp/, chimera-venv/, venv/, __pycache__/, .pytest_cache/
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
.cursor/
  rules/
    001_chimera_infrastructure_spec.mdc
    002_microservice_contracts_spec.mdc
    003_docs_alignment_strategy_spec.mdc
    spec.mdc
.github/
  workflows/
    main.yml
certs/
  ca.crt
  ca.srl
  server.crt
  server.csr
infrastructure/
  gpu-config/
    mig-config.yaml
  k8s/
    batch-inference-service.yaml
    gpu-monitoring-agent.yaml
    gpu-scheduler-config.yaml
    nats-jetstream.yaml
    personalization-engine.yaml
    priority-classes.yaml
    realtime-inference-service.yaml
    triton-inference-server.yaml
    vector-store-backup-cronjob.yaml
  monitoring/
    grafana/
      dashboards/
        chimera_overview.json
        chimera_performance.json
        inter_service_communication.json
      provisioning/
        datasources/
          datasources.yml
    loki/
      loki-config.yml
    otel/
      otel-collector-config.yml
    promtail/
      promtail-config.yml
    docker-compose.monitoring.yml
microservices/
  python/
    common/
      nats_lib/
        nats_lib/
          circuit_breaker.py
          config.py
          exceptions.py
          nats_client.py
        tests/
          integration/
            test_nats_integration.py
          unit/
            test_circuit_breaker.py
            test_nats_client.py
          conftest.py
        circuit_breaker.py
        config.py
        exceptions.py
        nats_client.py
        pyproject.toml
        pytest.ini
        README.md
        requirements-test.txt
        requirements.txt
        setup.py
    gpu-monitoring-agent/
      app.py
      Dockerfile
      README.md
      requirements.txt
    knowledge-graph/
      app/
        config.py
        main.py
        models.py
        nats_client.py
        neo4j_client.py
      Dockerfile
      README.md
      requirements.txt
    ml-orchestrator/
      app/
        checkpoint.py
        config.py
        main.py
        models.py
        nats_client.py
        triton.py
      Dockerfile
      README.md
      requirements.txt
    personalization-engine/
      app/
        config.py
        main.py
        models.py
        nats_client.py
        vector_store.py
      docs/
        vector_store_backup_strategy.md
      tests/
        conftest.py
        test_vector_store.py
      Dockerfile
      README.md
      requirements-test.txt
      requirements.txt
  rust/
    ingestion-service/
      src/
        config.rs
        error.rs
        main.rs
        models.rs
        nats.rs
        routes.rs
      Cargo.toml
      Dockerfile
      README.md
  ARCHITECTURE.md
  README.md
nats_lib/
  circuit_breaker.py
scripts/
  end_to_end_workflow.py
  nats_stream_setup.py
  run_integration_tests.sh
tests/
  integration/
    mock_services/
      triton/
        mock_triton_server.py
    conftest.py
    docker-compose.test.yml
    Dockerfile.test
    requirements.txt
    run_services.sh
    test_e2e_flows.py
    test_inter_service_comm.py
  unit/
    test_circuit_breaker.py
    test_nats_client.py
  conftest.py
.gitignore
.windsurfrules
info.md
README.md
requirements.txt
SolnAI.code-workspace
specification.md
vector_store_resilience.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="microservices/python/common/nats_lib/nats_lib/circuit_breaker.py">
"""Circuit breaker implementation for NATS operations."""
import enum
from typing import Any, Callable, Dict, Optional

import prometheus_client
import pybreaker
from loguru import logger

from nats_lib.config import CircuitBreakerConfig
from nats_lib.exceptions import NatsCircuitOpenError


class CircuitBreakerState(enum.IntEnum):
    """Circuit breaker states."""
    
    CLOSED = 0
    OPEN = 1
    HALF_OPEN = 2


class CircuitBreakerMetrics:
    """Prometheus metrics for circuit breaker."""

    def __init__(
        self,
        service_name: str,
        operation: str,
        subject_pattern: str
    ):
        """Initialize circuit breaker metrics.

        Args:
            service_name: Name of the service using the circuit breaker.
            operation: Operation being protected (e.g., publish, request).
            subject_pattern: NATS subject pattern being monitored.
        """
        # Sanitize metric name components
        service_name = service_name.replace("-", "_").replace(".", "_")
        operation = operation.replace("-", "_").replace(".", "_")
        subject_pattern = subject_pattern.replace("-", "_").replace(".", "_").replace("*", "all").replace(">", "all")

        # Create metric name prefix
        prefix = f"nats_circuit_breaker_{service_name}_{operation}_{subject_pattern}"

        # Create labels dictionary
        self.labels = {
            "service": service_name,
            "operation": operation,
            "subject_pattern": subject_pattern
        }

        # Initialize metrics
        self.state = prometheus_client.Gauge(
            f"{prefix}_state",
            "Current state of the circuit breaker",
            labelnames=list(self.labels.keys())
        )
        self.total_failures = prometheus_client.Counter(
            f"{prefix}_failures_total",
            "Total number of circuit breaker failures",
            labelnames=list(self.labels.keys())
        )
        self.total_successes = prometheus_client.Counter(
            f"{prefix}_successes_total",
            "Total number of circuit breaker successes",
            labelnames=list(self.labels.keys())
        )
        self.total_state_changes = prometheus_client.Counter(
            f"{prefix}_state_changes_total",
            "Total number of circuit breaker state changes",
            labelnames=list(self.labels.keys()) + ["from_state", "to_state"]
        )

        # Set initial state
        self.state.labels(**self.labels).set(CircuitBreakerState.CLOSED.value)

    def record_state_change(self, old_state: str, new_state: str) -> None:
        """Record a state change in the circuit breaker.

        Args:
            old_state: Previous state name.
            new_state: New state name.
        """
        self.state.labels(**self.labels).set(CircuitBreakerState[new_state].value)
        self.total_state_changes.labels(
            **self.labels,
            from_state=old_state,
            to_state=new_state
        ).inc()
        logger.debug(f"Circuit breaker state changed from {old_state} to {new_state}")

    def record_failure(self) -> None:
        """Record a failure in the circuit breaker."""
        self.total_failures.labels(**self.labels).inc()
        logger.debug("Circuit breaker failure")

    def record_success(self) -> None:
        """Record a success in the circuit breaker."""
        self.total_successes.labels(**self.labels).inc()
        logger.debug("Circuit breaker success")


class NatsCircuitBreaker:
    """Circuit breaker for NATS operations."""
    
    def __init__(
        self,
        name: str,
        config: CircuitBreakerConfig,
        service_name: str,
        operation: str,
        subject_pattern: str,
    ):
        """Initialize the circuit breaker."""
        self.name = name
        self.config = config
        self.metrics = CircuitBreakerMetrics(
            service_name=service_name,
            operation=operation,
            subject_pattern=subject_pattern,
        )
        
        # Initialize pybreaker
        self.breaker = pybreaker.CircuitBreaker(
            fail_max=config.fail_max,
            reset_timeout=config.reset_timeout,
            exclude=config.exclude_exceptions or (),
            listeners=[self._state_change_listener()],
        )
    
    def _state_change_listener(self) -> pybreaker.CircuitBreakerListener:
        """Create a listener for circuit breaker state changes."""
        class StateChangeListener(pybreaker.CircuitBreakerListener):
            def __init__(self, metrics: CircuitBreakerMetrics):
                self.metrics = metrics
            
            def state_change(self, breaker: pybreaker.CircuitBreaker, old_state: str, new_state: str) -> None:
                state_map = {
                    "closed": CircuitBreakerState.CLOSED,
                    "open": CircuitBreakerState.OPEN,
                    "half-open": CircuitBreakerState.HALF_OPEN,
                }
                
                new_state_value = state_map[new_state]
                self.metrics.state.labels(**self.metrics.labels).set(new_state_value.value)
                
                if new_state == "open":
                    self.metrics.total_state_changes.labels(
                        **self.metrics.labels,
                        from_state=old_state,
                        to_state=new_state
                    ).inc()
                
                logger.info(
                    f"Circuit breaker state changed from {old_state} to {new_state}",
                    breaker=breaker.name,
                    old_state=old_state,
                    new_state=new_state,
                )
            
            def failure(self, breaker: pybreaker.CircuitBreaker, exc: Exception) -> None:
                self.metrics.total_failures.labels(**self.metrics.labels).inc()
                logger.warning(
                    f"Circuit breaker failure",
                    breaker=breaker.name,
                    error=str(exc),
                )
            
            def success(self, breaker: pybreaker.CircuitBreaker) -> None:
                self.metrics.total_successes.labels(**self.metrics.labels).inc()
                logger.debug(
                    f"Circuit breaker success",
                    breaker=breaker.name,
                )
        
        return StateChangeListener(self.metrics)
    
    async def call(self, func: Callable, *args: Any, **kwargs: Any) -> Any:
        """Call a function through the circuit breaker."""
        try:
            return await self.breaker.call(func, *args, **kwargs)
        except pybreaker.CircuitBreakerError:
            raise NatsCircuitOpenError(f"Circuit breaker {self.name} is open")
</file>

<file path="microservices/python/common/nats_lib/nats_lib/config.py">
"""Configuration classes for the NATS client."""
from dataclasses import dataclass
from typing import Optional, Tuple, Type


@dataclass
class CircuitBreakerConfig:
    """Circuit breaker configuration."""
    
    fail_max: int = 3
    reset_timeout: int = 60
    exclude_exceptions: Optional[Tuple[Type[Exception], ...]] = None


@dataclass
class NatsConfig:
    """NATS client configuration."""
    
    urls: str
    stream_domain: str
    publish_breaker: CircuitBreakerConfig
    request_breaker: CircuitBreakerConfig
    user: Optional[str] = None
    password: Optional[str] = None
    token: Optional[str] = None
    use_tls: bool = False
    request_timeout: int = 30
    num_shards: int = 1
</file>

<file path="microservices/python/common/nats_lib/nats_lib/exceptions.py">
"""Custom exceptions for the NATS client."""


class NatsError(Exception):
    """Base exception for NATS client errors."""


class NatsConnectionError(NatsError):
    """Raised when there is an error connecting to NATS."""


class NatsCircuitOpenError(NatsError):
    """Raised when a circuit breaker is open."""


class NatsPublishError(NatsError):
    """Raised when there is an error publishing a message."""


class NatsSubscribeError(NatsError):
    """Raised when there is an error subscribing to a subject."""


class NatsRequestError(NatsError):
    """Raised when there is an error making a request."""
</file>

<file path="microservices/python/common/nats_lib/nats_lib/nats_client.py">
"""Enhanced NATS client with circuit breaker support."""
import json
from typing import Any, Callable, Dict, Optional, Union

import nats
from loguru import logger
from nats.js.api import StreamConfig
from nats_lib.circuit_breaker import NatsCircuitBreaker
from nats_lib.config import NatsConfig
from nats_lib.exceptions import (NatsCircuitOpenError, NatsConnectionError,
                                 NatsPublishError, NatsRequestError,
                                 NatsSubscribeError)


class EnhancedNatsClient:
    """Enhanced NATS client with circuit breaker support."""
    
    def __init__(self, config: NatsConfig, service_name: str):
        """Initialize the enhanced NATS client."""
        self._config = config
        self._service_name = service_name
        self._nc = None
        self._js = None
        
        # Initialize circuit breakers
        self._publish_breaker = NatsCircuitBreaker(
            name=f"{service_name}_publish",
            config=config.publish_breaker,
            service_name=service_name,
            operation="publish",
            subject_pattern="*",
        )
        
        self._request_breaker = NatsCircuitBreaker(
            name=f"{service_name}_request",
            config=config.request_breaker,
            service_name=service_name,
            operation="request",
            subject_pattern="*",
        )
    
    async def connect(self) -> None:
        """Connect to NATS server."""
        try:
            self._nc = await nats.connect(
                servers=self._config.urls.split(","),
                user=self._config.user,
                password=self._config.password,
                token=self._config.token,
                tls=self._config.use_tls,
            )
            self._js = await self._nc.jetstream()
            logger.info(
                "Connected to NATS",
                url=self.connected_url,
                service=self._service_name,
            )
        except Exception as e:
            logger.error(
                "Failed to connect to NATS",
                error=str(e),
                service=self._service_name,
            )
            raise NatsConnectionError(f"Failed to connect to NATS: {e}")
    
    async def close(self) -> None:
        """Close NATS connection."""
        if self._nc and not self._nc.is_closed:
            await self._nc.close()
    
    def is_connected(self) -> bool:
        """Check if connected to NATS."""
        return self._nc is not None and not self._nc.is_closed
    
    @property
    def connected_url(self) -> Optional[str]:
        """Get the connected server URL."""
        if self.is_connected() and self._nc.connected_url:
            return self._nc.connected_url.netloc
        return None
    
    async def publish(self, subject: str, payload: Dict[str, Any]) -> None:
        """Publish a message with circuit breaker protection."""
        if not self.is_connected():
            raise NatsConnectionError("Not connected to NATS")
        
        async def _publish() -> None:
            try:
                await self._nc.publish(
                    subject,
                    json.dumps(payload).encode(),
                )
            except Exception as e:
                logger.error(
                    "Failed to publish message",
                    subject=subject,
                    error=str(e),
                    service=self._service_name,
                )
                raise NatsPublishError(f"Failed to publish message: {e}")
        
        try:
            await self._publish_breaker.call(_publish)
        except NatsCircuitOpenError as e:
            logger.error(
                "Circuit breaker open for publish",
                subject=subject,
                service=self._service_name,
            )
            raise
    
    async def request(
        self,
        subject: str,
        payload: Dict[str, Any],
        timeout: Optional[float] = None,
    ) -> Dict[str, Any]:
        """Make a request with circuit breaker protection."""
        if not self.is_connected():
            raise NatsConnectionError("Not connected to NATS")
        
        async def _request() -> Dict[str, Any]:
            try:
                response = await self._nc.request(
                    subject,
                    json.dumps(payload).encode(),
                    timeout=timeout or self._config.request_timeout,
                )
                return json.loads(response.data.decode())
            except Exception as e:
                logger.error(
                    "Failed to make request",
                    subject=subject,
                    error=str(e),
                    service=self._service_name,
                )
                raise NatsRequestError(f"Failed to make request: {e}")
        
        try:
            return await self._request_breaker.call(_request)
        except NatsCircuitOpenError as e:
            logger.error(
                "Circuit breaker open for request",
                subject=subject,
                service=self._service_name,
            )
            raise
    
    async def subscribe(
        self,
        subject: str,
        callback: Callable[[Dict[str, Any]], None],
        queue: Optional[str] = None,
    ) -> None:
        """Subscribe to a subject."""
        if not self.is_connected():
            raise NatsConnectionError("Not connected to NATS")
        
        async def _message_handler(msg: nats.aio.msg.Msg) -> None:
            try:
                payload = json.loads(msg.data.decode())
                await callback(payload)
            except Exception as e:
                logger.error(
                    "Error in message handler",
                    subject=subject,
                    error=str(e),
                    service=self._service_name,
                )
        
        try:
            await self._nc.subscribe(
                subject,
                queue=queue,
                cb=_message_handler,
            )
            logger.info(
                "Subscribed to subject",
                subject=subject,
                queue=queue,
                service=self._service_name,
            )
        except Exception as e:
            logger.error(
                "Failed to subscribe",
                subject=subject,
                error=str(e),
                service=self._service_name,
            )
            raise NatsSubscribeError(f"Failed to subscribe: {e}")
    
    def _wrap_subscriber_callback(
        self,
        callback: Callable[[Dict[str, Any]], None],
    ) -> Callable[[nats.aio.msg.Msg], None]:
        """Wrap a subscriber callback to handle message decoding."""
        async def _wrapper(msg: nats.aio.msg.Msg) -> None:
            try:
                payload = json.loads(msg.data.decode())
                await callback(payload)
            except Exception as e:
                logger.error(
                    "Error in subscriber callback",
                    error=str(e),
                    service=self._service_name,
                )
        return _wrapper
</file>

<file path="microservices/python/common/nats_lib/tests/integration/test_nats_integration.py">
"""Integration tests for the EnhancedNatsClient with a real NATS server."""
import asyncio
from typing import AsyncGenerator, Dict

import docker
import pytest
from docker.models.containers import Container

from nats_lib.config import CircuitBreakerConfig, NatsConfig
from nats_lib.exceptions import NatsCircuitOpenError, NatsConnectionError
from nats_lib.nats_client import EnhancedNatsClient


@pytest.fixture(scope="session")
def docker_client() -> docker.DockerClient:
    """Create a Docker client."""
    return docker.DockerClient(base_url='unix://var/run/docker.sock')


@pytest.fixture(scope="session")
async def nats_container(
    docker_client: docker.DockerClient
) -> AsyncGenerator[Container, None]:
    """Start a NATS container for testing."""
    container = docker_client.containers.run(
        "nats:latest",
        detach=True,
        ports={"4222/tcp": 4222},
        name="test-nats",
        remove=True
    )
    
    # Wait for NATS to be ready
    await asyncio.sleep(2)
    
    yield container
    
    container.stop()


@pytest.fixture
async def test_client(
    nats_container: Container
) -> AsyncGenerator[EnhancedNatsClient, None]:
    """Create a test client connected to the NATS container."""
    config = NatsConfig(
        urls="nats://localhost:4222",
        stream_domain="test",
        publish_breaker=CircuitBreakerConfig(fail_max=3, reset_timeout=1),
        request_breaker=CircuitBreakerConfig(fail_max=3, reset_timeout=1)
    )
    
    client = EnhancedNatsClient(config, "test_service")
    await client.connect()
    
    yield client
    
    await client.close()


async def test_publish_subscribe_integration(test_client: EnhancedNatsClient):
    """Test publish-subscribe pattern with real NATS server."""
    subject = "test.pubsub"
    test_message = {"data": "test"}
    received_messages = []
    
    # Set up subscriber
    async def message_handler(msg: Dict) -> None:
        received_messages.append(msg)
    
    await test_client.subscribe(subject, message_handler)
    
    # Publish message
    await test_client.publish(subject, test_message)
    
    # Wait for message to be received
    await asyncio.sleep(0.1)
    
    assert len(received_messages) == 1
    assert received_messages[0] == test_message


async def test_request_reply_integration(test_client: EnhancedNatsClient):
    """Test request-reply pattern with real NATS server."""
    subject = "test.request"
    request_data = {"request": "test"}
    response_data = {"response": "success"}
    
    # Set up reply handler
    async def reply_handler(msg: Dict) -> None:
        await test_client.publish(msg["reply"], response_data)
    
    await test_client.subscribe(subject, reply_handler)
    
    # Send request
    response = await test_client.request(subject, request_data)
    assert response == response_data


async def test_circuit_breaker_integration(
    test_client: EnhancedNatsClient,
    nats_container: Container
):
    """Test circuit breaker with real NATS failures."""
    subject = "test.circuit"
    message = {"data": "test"}
    
    # Stop NATS container to simulate failure
    nats_container.stop()
    
    # Attempt publishes until circuit opens
    for _ in range(test_client._config.publish_breaker.fail_max):
        with pytest.raises((NatsConnectionError, Exception)):
            await test_client.publish(subject, message)
    
    # Verify circuit is open
    with pytest.raises(NatsCircuitOpenError):
        await test_client.publish(subject, message)
    
    # Restart NATS container
    nats_container.start()
    await asyncio.sleep(2)  # Wait for NATS to be ready
    
    # Wait for circuit reset timeout
    await asyncio.sleep(test_client._config.publish_breaker.reset_timeout)
    
    # Verify circuit closes after successful publish
    await test_client.publish(subject, message)


async def test_multiple_subscribers_integration(test_client: EnhancedNatsClient):
    """Test multiple subscribers with queue groups."""
    subject = "test.queue"
    queue = "test_group"
    test_message = {"data": "test"}
    received_counts = {"sub1": 0, "sub2": 0}
    
    # Set up two subscribers in same queue group
    async def make_handler(sub_id: str) -> None:
        async def handler(msg: Dict) -> None:
            received_counts[sub_id] += 1
        return handler
    
    await test_client.subscribe(subject, await make_handler("sub1"), queue)
    await test_client.subscribe(subject, await make_handler("sub2"), queue)
    
    # Publish multiple messages
    for _ in range(10):
        await test_client.publish(subject, test_message)
        await asyncio.sleep(0.1)
    
    # Verify load balancing between subscribers
    total_received = sum(received_counts.values())
    assert total_received == 10
    # Each subscriber should receive approximately half the messages
    assert 3 <= received_counts["sub1"] <= 7
    assert 3 <= received_counts["sub2"] <= 7
</file>

<file path="microservices/python/common/nats_lib/tests/unit/test_circuit_breaker.py">
"""Unit tests for the NatsCircuitBreaker class."""
import asyncio
from unittest.mock import AsyncMock

import prometheus_client
import pytest

from nats_lib.circuit_breaker import CircuitBreakerState, NatsCircuitBreaker
from nats_lib.exceptions import NatsCircuitOpenError


@pytest.fixture
def circuit_breaker(circuit_breaker_config) -> NatsCircuitBreaker:
    """Create a test circuit breaker instance."""
    return NatsCircuitBreaker(
        service_name="test_service",
        operation="test_op",
        subject_pattern="test.subject",
        config=circuit_breaker_config
    )


async def test_circuit_breaker_initial_state(circuit_breaker: NatsCircuitBreaker):
    """Test initial circuit breaker state."""
    # Check initial state metric
    metric_name = 'nats_circuit_breaker_test_service_test_op_test_subject_state'
    labels = {
        'service': 'test_service',
        'operation': 'test_op',
        'subject_pattern': 'test.subject'
    }
    state = prometheus_client.REGISTRY.get_sample_value(metric_name, labels)
    assert state == CircuitBreakerState.CLOSED.value


async def test_circuit_breaker_successful_call(circuit_breaker: NatsCircuitBreaker):
    """Test successful function call through circuit breaker."""
    mock_func = AsyncMock(return_value="success")
    result = await circuit_breaker.call(mock_func)
    assert result == "success"


async def test_circuit_breaker_failure_threshold(circuit_breaker: NatsCircuitBreaker):
    """Test circuit breaker opens after reaching failure threshold."""
    mock_func = AsyncMock(side_effect=Exception("test error"))
    
    # Call until breaker opens
    for _ in range(circuit_breaker.breaker.fail_max):
        with pytest.raises(Exception):
            await circuit_breaker.call(mock_func)
    
    # Verify breaker is open
    with pytest.raises(NatsCircuitOpenError):
        await circuit_breaker.call(mock_func)
    
    # Check state metric
    metric_name = 'nats_circuit_breaker_test_service_test_op_test_subject_state'
    labels = {
        'service': 'test_service',
        'operation': 'test_op',
        'subject_pattern': 'test.subject'
    }
    state = prometheus_client.REGISTRY.get_sample_value(metric_name, labels)
    assert state == CircuitBreakerState.OPEN.value
    
    # Check opens counter
    opens = prometheus_client.REGISTRY.get_sample_value(
        'nats_circuit_breaker_test_service_test_op_test_subject_opens_total',
        {'service': 'test_service', 'operation': 'test_op', 'subject_pattern': 'test.subject'}
    )
    assert opens == 1


async def test_circuit_breaker_reset_timeout(
    circuit_breaker: NatsCircuitBreaker,
    event_loop: asyncio.AbstractEventLoop
):
    """Test circuit breaker transitions to half-open after reset timeout."""
    mock_func = AsyncMock(side_effect=Exception("test error"))
    
    # Open the breaker
    for _ in range(circuit_breaker.breaker.fail_max):
        with pytest.raises(Exception):
            await circuit_breaker.call(mock_func)
    
    # Wait for reset timeout
    await asyncio.sleep(circuit_breaker.breaker.reset_timeout)
    
    # Verify half-open state
    metric_name = 'nats_circuit_breaker_test_service_test_op_test_subject_state'
    labels = {
        'service': 'test_service',
        'operation': 'test_op',
        'subject_pattern': 'test.subject'
    }
    state = prometheus_client.REGISTRY.get_sample_value(metric_name, labels)
    assert state == CircuitBreakerState.HALF_OPEN.value


async def test_circuit_breaker_excluded_exception(circuit_breaker: NatsCircuitBreaker):
    """Test excluded exceptions don't count towards failure threshold."""
    mock_func = AsyncMock(side_effect=ValueError("excluded error"))
    
    # Call multiple times with excluded exception
    for _ in range(circuit_breaker.breaker.fail_max + 1):
        with pytest.raises(ValueError):
            await circuit_breaker.call(mock_func)
    
    # Verify breaker remains closed
    metric_name = 'nats_circuit_breaker_test_service_test_op_test_subject_state'
    labels = {
        'service': 'test_service',
        'operation': 'test_op',
        'subject_pattern': 'test.subject'
    }
    state = prometheus_client.REGISTRY.get_sample_value(metric_name, labels)
    assert state == CircuitBreakerState.CLOSED.value


async def test_circuit_breaker_recovery(circuit_breaker: NatsCircuitBreaker):
    """Test circuit breaker recovery after successful call in half-open state."""
    mock_func = AsyncMock()
    mock_func.side_effect = [Exception("error")] * circuit_breaker.breaker.fail_max + ["success"]
    
    # Open the breaker
    for _ in range(circuit_breaker.breaker.fail_max):
        with pytest.raises(Exception):
            await circuit_breaker.call(mock_func)
    
    # Wait for reset timeout
    await asyncio.sleep(circuit_breaker.breaker.reset_timeout)
    
    # Successful call in half-open state
    result = await circuit_breaker.call(mock_func)
    assert result == "success"
    
    # Verify closed state
    metric_name = 'nats_circuit_breaker_test_service_test_op_test_subject_state'
    labels = {
        'service': 'test_service',
        'operation': 'test_op',
        'subject_pattern': 'test.subject'
    }
    state = prometheus_client.REGISTRY.get_sample_value(metric_name, labels)
    assert state == CircuitBreakerState.CLOSED.value
</file>

<file path="microservices/python/common/nats_lib/tests/unit/test_nats_client.py">
"""Unit tests for the EnhancedNatsClient class."""
import json
from typing import Any, Dict
from unittest.mock import AsyncMock, patch

import pytest
from nats.aio.msg import Msg

from nats_lib.config import NatsConfig
from nats_lib.exceptions import NatsCircuitOpenError, NatsConnectionError
from nats_lib.nats_client import EnhancedNatsClient


@pytest.fixture
async def enhanced_client(
    nats_config: NatsConfig,
    mock_nats: AsyncMock
) -> EnhancedNatsClient:
    """Create a test enhanced NATS client instance."""
    client = EnhancedNatsClient(
        config=nats_config,
        service_name="test_service"
    )
    await client.connect()
    return client


async def test_client_connect_success(
    enhanced_client: EnhancedNatsClient,
    mock_nats: AsyncMock
):
    """Test successful NATS connection."""
    # Mock the connect method to return the mock client
    mock_nats.connect.return_value = mock_nats

    # Connect to NATS
    await enhanced_client.connect()

    # Verify connection state
    assert enhanced_client.is_connected()
    assert enhanced_client.connected_url == "localhost:4222"
    mock_nats.connect.assert_called_once()


async def test_client_connect_failure(nats_config: NatsConfig):
    """Test NATS connection failure."""
    with patch("nats.connect", side_effect=Exception("Connection failed")):
        client = EnhancedNatsClient(nats_config, "test_service")
        with pytest.raises(NatsConnectionError):
            await client.connect()


async def test_client_publish_success(enhanced_client: EnhancedNatsClient):
    """Test successful message publish."""
    subject = "test.subject"
    payload = {"key": "value"}
    
    await enhanced_client.publish(subject, payload)
    
    enhanced_client._nc.publish.assert_called_once_with(
        subject,
        json.dumps(payload).encode()
    )


async def test_client_publish_circuit_open(
    enhanced_client: EnhancedNatsClient
):
    """Test publish when circuit breaker is open."""
    subject = "test.subject"
    payload = {"key": "value"}
    
    # Trigger circuit breaker to open
    enhanced_client._nc.publish.side_effect = Exception("NATS error")
    for _ in range(enhanced_client._config.publish_breaker.fail_max):
        with pytest.raises(Exception):
            await enhanced_client.publish(subject, payload)
    
    # Verify circuit breaker is open
    with pytest.raises(NatsCircuitOpenError):
        await enhanced_client.publish(subject, payload)


async def test_client_request_success(enhanced_client: EnhancedNatsClient):
    """Test successful request-reply."""
    subject = "test.request"
    payload = {"request": "data"}
    response_data = {"response": "ok"}
    
    # Mock response message
    mock_msg = AsyncMock(spec=Msg)
    mock_msg.data = json.dumps(response_data).encode()
    enhanced_client._nc.request.return_value = mock_msg
    
    response = await enhanced_client.request(subject, payload)
    assert response == response_data
    
    enhanced_client._nc.request.assert_called_once_with(
        subject,
        json.dumps(payload).encode(),
        timeout=enhanced_client._config.request_timeout
    )


async def test_client_request_circuit_open(
    enhanced_client: EnhancedNatsClient
):
    """Test request when circuit breaker is open."""
    subject = "test.request"
    payload = {"request": "data"}
    
    # Trigger circuit breaker to open
    enhanced_client._nc.request.side_effect = Exception("NATS error")
    for _ in range(enhanced_client._config.request_breaker.fail_max):
        with pytest.raises(Exception):
            await enhanced_client.request(subject, payload)
    
    # Verify circuit breaker is open
    with pytest.raises(NatsCircuitOpenError):
        await enhanced_client.request(subject, payload)


async def test_client_subscribe_success(enhanced_client: EnhancedNatsClient):
    """Test successful subscription."""
    subject = "test.subject"
    queue = "test_queue"

    async def callback(msg: Dict[str, Any]) -> None:
        pass

    # Subscribe with the callback
    await enhanced_client.subscribe(subject, callback, queue)

    # Assert that subscribe was called with the correct arguments
    enhanced_client._nc.subscribe.assert_called_once()
    call_args = enhanced_client._nc.subscribe.call_args
    assert call_args[0][0] == subject  # First positional arg is subject
    assert call_args[1]['queue'] == queue  # Keyword arg queue
    assert callable(call_args[1]['cb'])  # Keyword arg cb is callable


async def test_client_subscribe_callback_error(
    enhanced_client: EnhancedNatsClient
):
    """Test subscription callback error handling."""
    subject = "test.subject"
    
    async def callback(msg: Dict[str, Any]) -> None:
        raise ValueError("Callback error")
    
    # Subscribe with error-raising callback
    await enhanced_client.subscribe(subject, callback)
    
    # Get the wrapped callback
    wrapped_cb = enhanced_client._nc.subscribe.call_args[1]["cb"]
    
    # Create a mock message
    mock_msg = AsyncMock(spec=Msg)
    mock_msg.data = json.dumps({"test": "data"}).encode()
    
    # Verify callback error is handled
    await wrapped_cb(mock_msg)  # Should not raise
</file>

<file path="microservices/python/common/nats_lib/pyproject.toml">
[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "nats_lib"
version = "0.1.0"
authors = [{ name = "SolnAI", email = "info@solnai.com" }]
description = "Enhanced NATS client with circuit breaker support"
readme = "README.md"
requires-python = ">=3.8"
classifiers = [
    "Programming Language :: Python :: 3",
    "Operating System :: OS Independent",
]

[project.urls]
"Homepage" = "https://github.com/solnai/nats_lib"
"Bug Tracker" = "https://github.com/solnai/nats_lib/issues"

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
asyncio_mode = "auto"
addopts = [
    "--cov=nats_lib",
    "--cov-report=term-missing",
    "--cov-report=xml:coverage.xml",
    "--cov-report=html:coverage_html",
    "-v",
]
</file>

<file path="microservices/python/common/nats_lib/pytest.ini">
[pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
asyncio_mode = auto
addopts = 
    --cov=nats_lib
    --cov-report=term-missing
    --cov-report=xml:coverage.xml
    --cov-report=html:coverage_html
    -v
</file>

<file path="microservices/python/common/nats_lib/requirements-test.txt">
pytest==8.0.0
pytest-asyncio==0.23.5
pytest-cov==4.1.0
docker==7.0.0
nats-py==2.6.0
pybreaker==1.0.1
prometheus-client==0.19.0
</file>

<file path="nats_lib/circuit_breaker.py">
"""Circuit breaker implementation for NATS operations."""
from enum import Enum
from typing import Any, Callable

import prometheus_client
import pybreaker
from loguru import logger

from nats_lib.exceptions import NatsCircuitOpenError


class CircuitBreakerState(Enum):
    """Circuit breaker states."""
    CLOSED = 0
    OPEN = 1
    HALF_OPEN = 2


class CircuitBreakerMetrics:
    """Prometheus metrics for circuit breaker."""

    def __init__(
        self,
        service_name: str,
        operation: str,
        subject_pattern: str
    ):
        """Initialize circuit breaker metrics."""
        # Sanitize metric name components
        service_name = service_name.replace('-', '_').replace('.', '_')
        operation = (operation.replace('-', '_').replace('.', '_')
                    .replace('*', 'all'))
        subject_pattern = (subject_pattern.replace('-', '_').replace('.', '_')
                         .replace('*', 'all'))

        # Create metric name prefix
        prefix = f"nats_circuit_breaker_{service_name}_{operation}_{subject_pattern}"

        # Labels for all metrics
        self.labels = {
            'service': service_name,
            'operation': operation,
            'subject_pattern': subject_pattern
        }

        # Initialize metrics directly with the default registry
        self.state = prometheus_client.Gauge(
            f"{prefix}_state",
            "Current state of the circuit breaker",
            labelnames=list(self.labels.keys())
        )
        self.total_failures = prometheus_client.Counter(
            f"{prefix}_failures_total",
            "Total number of circuit breaker failures",
            labelnames=list(self.labels.keys())
        )
        self.total_successes = prometheus_client.Counter(
            f"{prefix}_successes_total",
            "Total number of circuit breaker successes",
            labelnames=list(self.labels.keys())
        )
        self.total_state_changes = prometheus_client.Counter(
            f"{prefix}_state_changes_total",
            "Total number of circuit breaker state changes",
            labelnames=list(self.labels.keys())
        )
        self.opens = prometheus_client.Counter(
            f"{prefix}_opens_total",
            "Total number of times the circuit breaker has opened",
            labelnames=list(self.labels.keys())
        )

        # Set initial state
        self.state.labels(**self.labels).set(CircuitBreakerState.CLOSED.value)

    def record_state_change(self, old_state: str, new_state: str) -> None:
        """Record a state change in the circuit breaker."""
        self.total_state_changes.labels(**self.labels).inc()
        self.state.labels(**self.labels).set(CircuitBreakerState[new_state].value)
        if new_state == "OPEN":
            self.opens.labels(**self.labels).inc()
        logger.debug(f"Circuit breaker state changed from {old_state} to {new_state}")

    def record_failure(self) -> None:
        """Record a failure in the circuit breaker."""
        self.total_failures.labels(**self.labels).inc()
        logger.debug("Circuit breaker failure")

    def record_success(self) -> None:
        """Record a success in the circuit breaker."""
        self.total_successes.labels(**self.labels).inc()
        logger.debug("Circuit breaker success")


class NatsCircuitBreaker:
    """Circuit breaker for NATS operations."""

    def __init__(
        self,
        name: str,
        config: Any,
        service_name: str,
        operation: str,
        subject_pattern: str
    ):
        """Initialize the circuit breaker.

        Args:
            name: Name of the circuit breaker.
            config: Circuit breaker configuration.
            service_name: Name of the service using the circuit breaker.
            operation: Operation being protected (e.g., publish, request).
            subject_pattern: NATS subject pattern being monitored.
        """
        self.metrics = CircuitBreakerMetrics(
            service_name=service_name,
            operation=operation,
            subject_pattern=subject_pattern
        )

        self.breaker = pybreaker.CircuitBreaker(
            fail_max=config.fail_max,
            reset_timeout=config.reset_timeout,
            exclude=[exc for exc in config.exclude_exceptions],
            listeners=[self._state_change_listener()]
        )

    def _state_change_listener(self) -> pybreaker.CircuitBreakerListener:
        """Create a listener for circuit breaker state changes.

        Returns:
            CircuitBreakerListener that updates metrics on state changes.
        """
        class MetricsListener(pybreaker.CircuitBreakerListener):
            def __init__(self, metrics: CircuitBreakerMetrics):
                self.metrics = metrics

            def state_change(self, cb: pybreaker.CircuitBreaker, old_state: str, new_state: str):
                """Handle state change event."""
                from_state = CircuitBreakerState[old_state.upper()]
                to_state = CircuitBreakerState[new_state.upper()]
                self.metrics.record_state_change(from_state.name, to_state.name)
                logger.debug(f"Circuit breaker state changed from {old_state} to {new_state}")

            def failure(self, cb: pybreaker.CircuitBreaker, exc: Exception):
                """Handle failure event."""
                self.metrics.record_failure()
                logger.debug(f"Circuit breaker failure: {exc}")

            def success(self, cb: pybreaker.CircuitBreaker):
                """Handle success event."""
                self.metrics.record_success()
                logger.debug("Circuit breaker success")

        return MetricsListener(self.metrics)

    async def call(self, func: Callable, *args, **kwargs) -> Any:
        """Execute a function through the circuit breaker.

        Args:
            func: Function to execute.
            *args: Positional arguments for the function.
            **kwargs: Keyword arguments for the function.

        Returns:
            Result of the function call.

        Raises:
            NatsCircuitOpenError: If the circuit breaker is open.
            Exception: Any exception raised by the function.
        """
        try:
            return await self.breaker.call(func, *args, **kwargs)
        except pybreaker.CircuitBreakerError as e:
            raise NatsCircuitOpenError(str(e)) from e
</file>

<file path="tests/unit/test_circuit_breaker.py">
import pytest

from nats_lib.circuit_breaker import NatsCircuitBreaker


@pytest.fixture
def circuit_breaker(circuit_breaker_config) -> NatsCircuitBreaker:
    """Create a test circuit breaker instance."""
    return NatsCircuitBreaker(
        name="test_breaker",
        service_name="test_service",
        operation="test_op",
        subject_pattern="test.subject",
        config=circuit_breaker_config
    )
</file>

<file path="tests/unit/test_nats_client.py">
"""Unit tests for the NATS client."""

import json
from typing import Any, Dict
from unittest.mock import AsyncMock, patch

import pytest
from nats.aio.msg import Msg

from nats_lib.config import CircuitBreakerConfig, NatsConfig
from nats_lib.nats_client import EnhancedNatsClient


async def test_client_request_success(enhanced_client: EnhancedNatsClient):
    """Test successful request-reply."""
    subject = "test.request"
    payload = {"request": "data"}
    response_data = {"response": "success"}

    # Mock response message
    mock_msg = AsyncMock(spec=Msg)
    mock_msg.data = json.dumps(response_data).encode()
    enhanced_client._nc.request.return_value = mock_msg

    response = await enhanced_client.request(subject, payload)
    assert response == response_data


async def test_client_subscribe_success(enhanced_client: EnhancedNatsClient):
    """Test successful subscription."""
    subject = "test.subject"
    queue = "test_queue"

    async def callback(msg: Dict[str, Any]) -> None:
        pass

    # Store the wrapped callback
    wrapped_callback = enhanced_client._wrap_subscriber_callback(callback)

    # Subscribe with the callback
    await enhanced_client.subscribe(subject, callback, queue)

    # Assert that subscribe was called with the correct arguments
    enhanced_client._nc.subscribe.assert_called_once_with(
        subject, queue=queue, cb=wrapped_callback
    )


@pytest.fixture
async def enhanced_client(mock_nats: AsyncMock) -> EnhancedNatsClient:
    """Create a test enhanced NATS client."""
    config = NatsConfig(
        urls="nats://localhost:4222",
        stream_domain="test",
        publish_breaker=CircuitBreakerConfig(fail_max=3, reset_timeout=1),
        request_breaker=CircuitBreakerConfig(fail_max=3, reset_timeout=1),
    )

    with patch("nats_lib.nats_client.nats.connect", return_value=mock_nats):
        client = EnhancedNatsClient(config, "test_service")
        await client.connect()
        yield client


async def test_client_connect_success(
    enhanced_client: EnhancedNatsClient, mock_nats: AsyncMock
):
    """Test successful NATS connection."""
    assert enhanced_client.is_connected()
    assert enhanced_client.connected_url == "localhost:4222"
    mock_nats.connect.assert_called_once()
</file>

<file path="tests/conftest.py">
"""Test configuration and fixtures for nats_lib tests."""

import asyncio
from typing import AsyncGenerator, Generator
from unittest.mock import AsyncMock, MagicMock, patch
from urllib.parse import urlparse

import pytest
from prometheus_client import REGISTRY

from nats_lib.config import CircuitBreakerConfig, NatsConfig


@pytest.fixture
def clean_prometheus_registry():
    """Clean up the Prometheus registry before each test."""
    collectors = list(REGISTRY._collector_to_names.keys())
    for collector in collectors:
        REGISTRY.unregister(collector)


@pytest.fixture
def event_loop() -> Generator[asyncio.AbstractEventLoop, None, None]:
    """Create an instance of the default event loop for each test case."""
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    yield loop
    loop.close()


@pytest.fixture
def circuit_breaker_config():
    """Create a test circuit breaker configuration."""
    return CircuitBreakerConfig(
        fail_max=3, reset_timeout=1, exclude_exceptions=(ValueError,)
    )


@pytest.fixture
def nats_config():
    """Create a test NATS configuration."""
    return NatsConfig(
        urls="nats://localhost:4222",
        stream_domain="test",
        publish_breaker=CircuitBreakerConfig(fail_max=3, reset_timeout=1),
        request_breaker=CircuitBreakerConfig(fail_max=3, reset_timeout=1),
    )


@pytest.fixture
def mock_nats() -> AsyncMock:
    """Create a mock NATS client."""
    mock = AsyncMock()

    # Create JetStream mock that can be awaited
    js_mock = MagicMock()
    js_mock.publish = AsyncMock()
    js_mock.subscribe = AsyncMock()
    js_mock.request = AsyncMock()

    # Make jetstream() return the mock directly without needing to be awaited
    mock.jetstream = MagicMock(return_value=js_mock)

    # Set up other mock methods
    mock.publish = AsyncMock()
    mock.subscribe = AsyncMock()
    mock.request = AsyncMock()
    mock.connected_url = "localhost:4222"
    mock.is_closed = False

    return mock


@pytest.fixture(autouse=True)
def clean_registry():
    """Clean up the Prometheus registry before each test."""
    REGISTRY.clear()
</file>

<file path=".cursor/rules/001_chimera_infrastructure_spec.mdc">
---
description: 
globs: infrastructure/k8s/**/*.yaml ,   infrastructure/gpu-config/**/*.yaml,  infrastructure/monitoring/**/*.yml
alwaysApply: false
---
---
description: Chimera Platform Infrastructure Automation
globs:
  - infrastructure/k8s/**/*.yaml
  - infrastructure/gpu-config/**/*.yaml
  - infrastructure/monitoring/**/*.yml
alwaysApply: true
---
# 001_chimera_infrastructure_spec.mdc

id: 001_chimera_infrastructure_spec
title: Infrastructure Automation and Validation
category: Infrastructure
tags: [kubernetes, gpu, scheduling, monitoring, security, CI/CD, automation]

rule: >
  Act as a Kubernetes Infrastructure Architect responsible for validating, optimizing, and automating deployments:

  ## Objectives
  - Ensure Kubernetes YAML files are syntactically correct and optimized for GPU workloads
  - Validate MIG GPU configurations for accuracy and adherence to best practices
  - Automate validation checks for scheduling priorities, node affinities, and GPU resource allocations
  - Enforce compliance with GDPR, HIPAA, and SOC 2 Type II in configurations

  ## Implementation Actions
  - Generate reports highlighting issues, optimizations, or best practices adherence
  - Suggest improvements or updates via comments in YAML blocks
  - Provide corrective YAML snippets when issues or enhancements are identified

  ## Output Requirements
  - YAML code blocks with precise context for each suggested modification
  - Comments structured clearly for immediate developer action
</file>

<file path=".cursor/rules/002_microservice_contracts_spec.mdc">
---
description: 
globs: microservices/python/**/*.py , microservices/rust/**/*.rs ,scripts/**/*.py
alwaysApply: false
---
---
description: Chimera Microservices Review & Validation
globs:
  - microservices/python/**/*.py
  - microservices/rust/**/*.rs
  - scripts/**/*.py 
alwaysApply: true
---
# 002_microservice_contracts_spec.mdc

id: 002_microservice_contracts_spec
title: Microservices Architecture Enforcement
category: Backend
tags: [python, rust, grpc, pubsub, contracts, reliability, devops]

rule: >
  You are a System Reliability Engineer verifying cross-microservice architecture contracts.

  ## Goals
  - Ensure interface integrity between Python and Rust services (via NATS, Triton, Redis, Neo4j)
  - Detect architectural drift, broken imports, or non-conforming pub/sub patterns
  - Recommend interface type hints and error boundaries
  - Verify model schemas match serialization and pub/sub expectations

  ## Actions
  - Review Python + Rust microservices for:
    - Event model consistency (e.g., `nats_client.py`, `nats.rs`)
    - Serialization format integrity (e.g., JSON schema, protobuf, msgpack)
    - Fault boundaries and retry logic for downstream systems
  - Suggest missing or incorrect type hints
  - Annotate broken assumptions or brittle service dependencies

  ## Output
  - Code suggestions in context with rationale
  - `Ω`-level annotations for cognitive modeling flaws
</file>

<file path=".cursor/rules/003_docs_alignment_strategy_spec.mdc">
---
description: documentation and system implementation
globs: 
alwaysApply: false
---
---
description: Docs & Strategy Consistency
globs:
  - README.md
  - docs/**/*.md
  - specification.md
  - vector_store_resilience.md
  - ARCHITECTURE.md
alwaysApply: false
---
# 003_docs_alignment_strategy_spec.mdc

id: 003_docs_alignment_strategy_spec
title: Docs–Code Architecture Alignment Auditor
category: Strategy
tags: [documentation, architecture, markdown, specs, synchronization]

rule: >
  You act as an AI System Strategist responsible for maintaining bidirectional coherence between documentation and system implementation.

  ## Goals
  - Validate that strategic docs (e.g. ARCHITECTURE.md, spec.md) match implementation reality
  - Highlight divergences between vector store/backup design vs actual code and configs
  - Align observed system behavior with claimed design guarantees (e.g. fault tolerance, scalability)

  ## Required Checks
  - Architectural consistency with deployed YAML specs and Dockerfiles
  - Verify that backup strategies and personalization workflows match runtime and CRON logic
  - Detect outdated or vague design language

  ## Output Directives
  - Highlight mismatches with markdown diff blocks or callouts
  - Add `Ψ:` annotations for traceability inconsistencies
</file>

<file path=".cursor/rules/spec.mdc">
---
description: refine the prompt
globs: 
alwaysApply: false
---
# 990_synapse_personalization_spec_prompt.mdc

id: 990_synapse_personalization_spec_prompt
title: Expert Prompt – Synapse Web Personalization Engine Spec Generator
category: 9■■ Templates
tags: [spec, architecture, AI, personalization, vector-db, RAG, proactive, markdown, cursor]

rule: >
  You will act as an elite Technical Architect and AI Systems Engineer tasked with generating the complete technical specification for the `Synapse Web AI-Driven Personalization Engine`.

  ## 🔧 Objective
  Output a production-grade **Markdown** specification aligned with enterprise standards, strategic foresight, and implementation readiness. This is used as a canonical document in the Synapse Web system for:
  - Developer implementation
  - System integration
  - Architecture validation
  - Documentation publication

  ## 📂 Scope
  - Generate the full specification as **Markdown** with section headers and code blocks
  - Follow the structure below precisely
  - Use concise, structured, engineering-oriented phrasing
  - Provide interface stubs in **Python** (backend) and **TypeScript** (frontend)
  - Infer and inject adaptive data schemas if not explicitly defined
  - Highlight any unresolved issues in the final section with **confidence + risk levels**
  - Do not ask for confirmation or clarification mid-generation
  - Assume full developer authorization, cross-component visibility, and system context

  ## 📄 Required Specification Sections
  1. Overview & Purpose
  2. System Capabilities
  3. Technical Requirements
      - Functional
      - Non-functional
      - Security/Compliance
  4. System Architecture
      - Component Breakdown
      - Data Flow
      - Model & RAG Integration
      - Cross-component Interfaces
  5. Data Schema & Storage
      - User profile schema (adaptive if missing)
      - Embedding/vector storage format
      - Vector indexing strategy (HNSW, IVFADC, sharding, caching)
      - Explicit/implicit feedback event schema
  6. APIs & Interfaces
      - Inputs (signals, behaviors, settings)
      - Outputs (personalization, layout shifts)
      - API format (OpenAPI/GraphQL)
      - Code stubs in Python & TypeScript
  7. Proactive Intelligence Layer
      - Trigger logic, prediction patterns, suggestion model
  8. Feedback & Learning Loop
      - Explicit + Implicit feedback modeling
      - Feedback ingestion & learning cycle
  9. Operational Infrastructure
      - Deployment, GPU usage, latency pipelines
      - Fallbacks, observability, canary deploys
 10. Open Questions & Refinement Requests
      - List missing info / unresolved architecture
      - Confidence Level (High/Medium/Low)
      - Risk Level (High/Medium/Low)
      - Suggest fix paths or placeholders

  ## 🧠 Clarification Logic
  Before ending, analyze your own output for:
  - Section cohesion and cross-consistency
  - Missing system dependencies or unvalidated flows
  - Mismatched assumptions
  
  Include this self-check result in the final "Open Questions" section with confidence/risk scores.

  ## ✍️ Output Directives
  Begin with:
  ```markdown
  # Synapse Web – AI Personalization Engine Technical Specification (v1)
  ```
  End with:
  ```markdown
  ## Open Questions and Refinement Requests
  ```

  ## 📘 Style
  Apply user’s communication format:
  - Token-efficient, execution-optimized
  - Markdown-first, terminal-style clarity
  - Systemic abstraction: `Ω`, `Λ`, `Φ`, `Ψ`, `Ξ`
  - Strategy-forward framing, speculative future readiness
  - Architecture as code

context_ref:
  - Ω*: reasoning layers
  - Ψ: cognitive trace
  - Φ*: design pattern capture
  - Λ: reusable rules
  - TDD.loop: fail-fix-repeat
  - M.sync: memory sync
  - Chimera: real-time AI backend
  - Cursor: markdown-native development context
  - RAG, Vector DB, Proactive UX

status: production-ready
```

---
</file>

<file path=".github/workflows/main.yml">
name: Chimera Platform CI/CD

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to deploy to'
        required: true
        default: 'staging'
        type: choice
        options:
          - staging
          - production

env:
  REGISTRY: ghcr.io
  REGISTRY_PREFIX: chimera

jobs:
  # Code Quality and Security Scans
  code-quality:
    name: Code Quality and Security
    runs-on: ubuntu-latest
    strategy:
      matrix:
        include:
          - lang: python
            dir: microservices/python
          - lang: rust
            dir: microservices/rust
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        if: ${{ matrix.lang == 'python' }}
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'

      - name: Set up Rust
        if: ${{ matrix.lang == 'rust' }}
        uses: actions-rs/toolchain@v1
        with:
          toolchain: stable
          profile: minimal
          components: clippy, rustfmt
          override: true

      - name: Python Linting
        if: ${{ matrix.lang == 'python' }}
        run: |
          python -m pip install --upgrade pip
          pip install flake8 black isort mypy
          flake8 ${{ matrix.dir }}
          black --check ${{ matrix.dir }}
          isort --check ${{ matrix.dir }}

      - name: Rust Linting
        if: ${{ matrix.lang == 'rust' }}
        run: |
          cd ${{ matrix.dir }}/ingestion-service
          cargo fmt -- --check
          cargo clippy -- -D warnings

      - name: Security scan
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '${{ matrix.dir }}'
          format: 'sarif'
          output: 'trivy-results.sarif'
          severity: 'CRITICAL,HIGH'

      - name: Upload security scan results
        uses: github/codeql-action/upload-sarif@v2
        with:
          sarif_file: 'trivy-results.sarif'

  # Unit Tests
  unit-tests:
    name: Unit Tests
    needs: code-quality
    runs-on: ubuntu-latest
    strategy:
      matrix:
        service:
          - ingestion-service
          - ml-orchestrator
          - knowledge-graph
          - personalization-engine
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        if: ${{ !contains(matrix.service, 'ingestion') }}
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'

      - name: Set up Rust
        if: ${{ contains(matrix.service, 'ingestion') }}
        uses: actions-rs/toolchain@v1
        with:
          toolchain: stable
          profile: minimal
          override: true

      - name: Run Python tests
        if: ${{ !contains(matrix.service, 'ingestion') }}
        run: |
          cd microservices/python/${{ matrix.service }}
          pip install -r requirements.txt
          pip install pytest pytest-asyncio pytest-cov
          python -m pytest tests/unit --cov=app --cov-report=xml

      - name: Run Rust tests
        if: ${{ contains(matrix.service, 'ingestion') }}
        run: |
          cd microservices/rust/${{ matrix.service }}
          cargo test --no-fail-fast

      - name: Upload test coverage
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          fail_ci_if_error: false

  # Build Docker Images
  build-images:
    name: Build Docker Images
    needs: unit-tests
    runs-on: ubuntu-latest
    strategy:
      matrix:
        service:
          - ingestion-service
          - ml-orchestrator
          - knowledge-graph
          - personalization-engine
        include:
          - service: ingestion-service
            dir: microservices/rust/ingestion-service
          - service: ml-orchestrator
            dir: microservices/python/ml-orchestrator
          - service: knowledge-graph
            dir: microservices/python/knowledge-graph
          - service: personalization-engine
            dir: microservices/python/personalization-engine
    permissions:
      contents: read
      packages: write
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2

      - name: Login to GitHub Container Registry
        uses: docker/login-action@v2
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v4
        with:
          images: ${{ env.REGISTRY }}/${{ github.repository_owner }}/${{ env.REGISTRY_PREFIX }}-${{ matrix.service }}

      - name: Build and push Docker image
        uses: docker/build-push-action@v4
        with:
          context: ${{ matrix.dir }}
          push: ${{ github.event_name != 'pull_request' }}
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

  # Integration Tests
  integration-tests:
    name: Integration Tests
    needs: build-images
    runs-on: ubuntu-latest
    if: github.event_name != 'pull_request'
    services:
      nats:
        image: nats:2.10.7-alpine
        ports:
          - 4222:4222
        options: --name=nats --health-cmd="nats-server check connection --host localhost --port 4222" --health-interval=5s
      redis:
        image: redislabs/redisearch:latest
        ports:
          - 6379:6379
        options: --name=redis --health-cmd="redis-cli ping" --health-interval=5s
      neo4j:
        image: neo4j:5.13.0
        ports:
          - 7474:7474
          - 7687:7687
        env:
          NEO4J_AUTH: neo4j/testpassword
        options: --name=neo4j --health-cmd="wget http://localhost:7474 -O /dev/null" --health-interval=10s
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'

      - name: Login to GitHub Container Registry
        uses: docker/login-action@v2
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Run integration tests
        run: |
          cd tests/integration
          pip install -r requirements.txt
          python -m pytest test_e2e_flows.py -v

      - name: Generate Allure Report
        if: always()
        uses: simple-elf/allure-report-action@master
        with:
          allure_results: tests/integration/results
          allure_report: allure-report
          allure_history: allure-history

      - name: Upload test artifacts
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: test-results
          path: |
            tests/integration/results
            allure-report

  # Deploy to Staging Environment
  deploy-staging:
    name: Deploy to Staging
    needs: integration-tests
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/develop' || github.event.inputs.environment == 'staging'
    environment: staging
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up kubectl
        uses: azure/k8s-set-context@v3
        with:
          kubeconfig: ${{ secrets.KUBE_CONFIG_STAGING }}

      - name: Deploy Kubernetes resources
        run: |
          export REGISTRY="${{ env.REGISTRY }}/${{ github.repository_owner }}/${{ env.REGISTRY_PREFIX }}"
          export VERSION="${{ github.sha }}"
          
          # Update Kubernetes manifests with the correct image tags
          find infrastructure/k8s -type f -name "*.yaml" -exec sed -i "s/\${REGISTRY}/$REGISTRY/g" {} \;
          find infrastructure/k8s -type f -name "*.yaml" -exec sed -i "s/latest/$VERSION/g" {} \;
          
          # Apply Kubernetes manifests
          kubectl apply -f infrastructure/k8s/nats-jetstream.yaml
          kubectl apply -f infrastructure/k8s/triton-inference-server.yaml
          kubectl apply -f infrastructure/k8s/ingestion-service.yaml
          kubectl apply -f infrastructure/k8s/ml-orchestrator.yaml
          kubectl apply -f infrastructure/k8s/knowledge-graph.yaml
          kubectl apply -f infrastructure/k8s/personalization-engine.yaml
          
          # Verify deployment
          kubectl rollout status deployment/ingestion-service
          kubectl rollout status deployment/ml-orchestrator
          kubectl rollout status deployment/knowledge-graph
          kubectl rollout status deployment/personalization-engine

      - name: Notify deployment
        uses: rtCamp/action-slack-notify@v2
        env:
          SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK }}
          SLACK_CHANNEL: deployments
          SLACK_COLOR: good
          SLACK_TITLE: Deployment to Staging Completed
          SLACK_MESSAGE: 'Chimera platform has been deployed to staging environment'

  # Deploy to Production Environment
  deploy-production:
    name: Deploy to Production
    needs: integration-tests
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' || github.event.inputs.environment == 'production'
    environment: production
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up kubectl
        uses: azure/k8s-set-context@v3
        with:
          kubeconfig: ${{ secrets.KUBE_CONFIG_PRODUCTION }}

      - name: Deploy Kubernetes resources
        run: |
          export REGISTRY="${{ env.REGISTRY }}/${{ github.repository_owner }}/${{ env.REGISTRY_PREFIX }}"
          export VERSION="${{ github.sha }}"
          
          # Update Kubernetes manifests with the correct image tags
          find infrastructure/k8s -type f -name "*.yaml" -exec sed -i "s/\${REGISTRY}/$REGISTRY/g" {} \;
          find infrastructure/k8s -type f -name "*.yaml" -exec sed -i "s/latest/$VERSION/g" {} \;
          
          # Apply Kubernetes manifests
          kubectl apply -f infrastructure/k8s/nats-jetstream.yaml
          kubectl apply -f infrastructure/k8s/triton-inference-server.yaml
          kubectl apply -f infrastructure/k8s/ingestion-service.yaml
          kubectl apply -f infrastructure/k8s/ml-orchestrator.yaml
          kubectl apply -f infrastructure/k8s/knowledge-graph.yaml
          kubectl apply -f infrastructure/k8s/personalization-engine.yaml
          
          # Verify deployment
          kubectl rollout status deployment/ingestion-service
          kubectl rollout status deployment/ml-orchestrator
          kubectl rollout status deployment/knowledge-graph
          kubectl rollout status deployment/personalization-engine

      - name: Notify deployment
        uses: rtCamp/action-slack-notify@v2
        env:
          SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK }}
          SLACK_CHANNEL: deployments
          SLACK_COLOR: good
          SLACK_TITLE: Deployment to Production Completed
          SLACK_MESSAGE: 'Chimera platform has been deployed to production environment'
</file>

<file path="certs/ca.crt">
-----BEGIN CERTIFICATE-----
MIIDBTCCAe2gAwIBAgIUQkr+pTBErY7y0AFYFQpzwBIF0ogwDQYJKoZIhvcNAQEL
BQAwEjEQMA4GA1UEAwwHTkFUUyBDQTAeFw0yNTA0MDIwMzM5MDNaFw0yNTA1MDIw
MzM5MDNaMBIxEDAOBgNVBAMMB05BVFMgQ0EwggEiMA0GCSqGSIb3DQEBAQUAA4IB
DwAwggEKAoIBAQCh9i25M/1zx7jPBI0ICxVMSRmX1z+E9dPIiiNYf/FkKtRySdyZ
PMGGBA4O85uBQFcivJzchCv7rq+qMM3HF4ACNM2DeNBIAec+kraf5Nqy7h1KXSYz
YtO96w3l3vezM/fm52YnH5POA8NbBa9t6fwGi+6JPYWZRcnNOFpKy2w1r5pFlrMu
NbtobJJobGz3GUPbhOb8V/qAwAILPz+NpIzYwOfEaxVsJnPoOaC3y3iUg+GWSwzu
14KBLwCxUfYkoonT8w5w81FWNnzqhSl1glktBNqEWOEQ7Iy/NjFQbqr2DdNTInMw
rviUAFXou1JpNzC5vUh4yCzI3+vSARyi7+bXAgMBAAGjUzBRMB0GA1UdDgQWBBT3
YThN+oRR6GPQmZpOIfMf1UuqxDAfBgNVHSMEGDAWgBT3YThN+oRR6GPQmZpOIfMf
1UuqxDAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQB72jIhwzlU
JKRis7gGMIIyi+aVGruothr6yolvAe95F3bFUs/QV2o2ZoZv9S1+Dwh1I+h9ENKg
wHxbmD89hstLZhSs/OCUY5J2eez6eVkOok6SdKgwv0WdKSxL+h1vGMjIqEX/Ik/I
6M0m/pmSOdT2vfIuMT8Z2iqjcuqIylwf0TPAV5dgrDIIxxToWp6vx8WYokz3OPLG
C3m6iZyv6+EzdC7qDSPVh5/SDSKVzdc1IerTit2UfGhQQrPBIg+B/Ab0eu8UIlH/
gjES37xxW1c/gnR5CQjsmN8J6ZWf2U6OELBr/oySPemI97lK4zYrZ4cofmDLH61k
PIyttV4ihc/H
-----END CERTIFICATE-----
</file>

<file path="certs/ca.srl">
7CEE4F79EEF976507E34292B6730599A4154D30E
</file>

<file path="certs/server.crt">
-----BEGIN CERTIFICATE-----
MIIDBTCCAe2gAwIBAgIUQkr+pTBErY7y0AFYFQpzwBIF0ogwDQYJKoZIhvcNAQEL
BQAwEjEQMA4GA1UEAwwHTkFUUyBDQTAeFw0yNTA0MDIwMzM5MDNaFw0yNTA1MDIw
MzM5MDNaMBIxEDAOBgNVBAMMB05BVFMgQ0EwggEiMA0GCSqGSIb3DQEBAQUAA4IB
DwAwggEKAoIBAQCh9i25M/1zx7jPBI0ICxVMSRmX1z+E9dPIiiNYf/FkKtRySdyZ
PMGGBA4O85uBQFcivJzchCv7rq+qMM3HF4ACNM2DeNBIAec+kraf5Nqy7h1KXSYz
YtO96w3l3vezM/fm52YnH5POA8NbBa9t6fwGi+6JPYWZRcnNOFpKy2w1r5pFlrMu
NbtobJJobGz3GUPbhOb8V/qAwAILPz+NpIzYwOfEaxVsJnPoOaC3y3iUg+GWSwzu
14KBLwCxUfYkoonT8w5w81FWNnzqhSl1glktBNqEWOEQ7Iy/NjFQbqr2DdNTInMw
rviUAFXou1JpNzC5vUh4yCzI3+vSARyi7+bXAgMBAAGjUzBRMB0GA1UdDgQWBBT3
YThN+oRR6GPQmZpOIfMf1UuqxDAfBgNVHSMEGDAWgBT3YThN+oRR6GPQmZpOIfMf
1UuqxDAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQB72jIhwzlU
JKRis7gGMIIyi+aVGruothr6yolvAe95F3bFUs/QV2o2ZoZv9S1+Dwh1I+h9ENKg
wHxbmD89hstLZhSs/OCUY5J2eez6eVkOok6SdKgwv0WdKSxL+h1vGMjIqEX/Ik/I
6M0m/pmSOdT2vfIuMT8Z2iqjcuqIylwf0TPAV5dgrDIIxxToWp6vx8WYokz3OPLG
C3m6iZyv6+EzdC7qDSPVh5/SDSKVzdc1IerTit2UfGhQQrPBIg+B/Ab0eu8UIlH/
gjES37xxW1c/gnR5CQjsmN8J6ZWf2U6OELBr/oySPemI97lK4zYrZ4cofmDLH61k
PIyttV4ihc/H
-----END CERTIFICATE-----
</file>

<file path="certs/server.csr">
-----BEGIN CERTIFICATE REQUEST-----
MIICfzCCAWcCAQAwOjEnMCUGA1UEAwwebmF0cy5kZWZhdWx0LnN2Yy5jbHVzdGVy
LmxvY2FsMQ8wDQYDVQQKDAZTb2xuQUkwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAw
ggEKAoIBAQCvv3jTpFvFIeUXKSNKavCGGTOurlSe5mOfiFdBkTzdo3COcCyAJ4hw
nhcuVXjx8rJdaNjbCyLNVVWQuI0LXqpnYn+sXHGtEyB6Y3CHG4OO4+jxO/u99zAF
l7k9uQueGfV0F6fv7qS2zSFhnWarVIs9NHJ6puXkYL+EHs5Ths8sDDmo2AelzR0d
p68l2m4g6NHXN1JW6CqcSP55Ubj9SJbcotUNI6/Dfea3HpV6Lra9JLkOMzatNFBL
SUNJEnQyuzbJ3Kr7HvAOE0u9O5f4PqsssLtVHaTo2F5z8D1xpgyO+nsFkcvU72xA
EwLnJ9LpSdBOArYnCS1kv0KGp5Cx+O/LAgMBAAGgADANBgkqhkiG9w0BAQsFAAOC
AQEABbR2dx3vxDdWyINYZi9BsXt350DhxEQC7v9UpeA5pLNsHF2TSkcCFGJ8booH
VsWeqGnBj6jYWZOHVZZR0HWA8hGZ9pOSq0T90vcI8BMo5n+ChkASdbsaJYo4dFze
YiUbvQWbAeIZVvMjFuNy/vCc8o5N43YLLqB8EB1o2izChn2EhczZ0UZwcRYFPFny
1oDsll8m/g/9Rn+FkbmJKbc4yY2si1NrldVLWcgt8Uc3BKDZeNS37wzhUz7QUi3k
PPD8lJSSM1dL4mVpYIFG6e1FrVCDj8tYTbdSvQOZ2r7uk+ZpBjs6itZwbDUpUm3A
CZOrU7RX7fNc2mN2yFV6ylr9ZA==
-----END CERTIFICATE REQUEST-----
</file>

<file path="infrastructure/gpu-config/mig-config.yaml">
apiVersion: nvidia.com/v1
kind: MigConfig
metadata:
  name: all-3g.10gb
spec:
  # A100 80GB configuration with 3 MIG instances
  gpuConfigs:
    - deviceName: all
      devices: all
      migEnabled: true
      migGeometry:
        - gi: 3g.10gb
          ci: 0,0,0,0,0,0,0
---
apiVersion: nvidia.com/v1
kind: MigConfig
metadata:
  name: all-1g.20gb
spec:
  # A100 80GB configuration with 4 MIG instances
  gpuConfigs:
    - deviceName: all
      devices: all
      migEnabled: true
      migGeometry:
        - gi: 1g.20gb
          ci: 0,0,0,0
---
apiVersion: nvidia.com/v1
kind: MigConfig
metadata:
  name: mixed-config
spec:
  # Mixed configuration with different MIG profiles
  gpuConfigs:
    - deviceName: all
      devices: all
      migEnabled: true
      migGeometry:
        - gi: 2g.20gb
          ci: 0,0
        - gi: 1g.10gb
          ci: 0,0
---
apiVersion: batch/v1
kind: Job
metadata:
  name: nvidia-mig-manager
spec:
  template:
    spec:
      containers:
      - name: nvidia-mig-manager
        image: nvcr.io/nvidia/k8s/mig-manager:v0.5.0
        command: ["nvidia-mig-manager"]
        args:
          - "--mode=named"
          - "--config-file=/config/config.yaml"
        env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
        securityContext:
          privileged: true
        volumeMounts:
          - name: config-volume
            mountPath: /config
      volumes:
        - name: config-volume
          configMap:
            name: mig-config
      restartPolicy: OnFailure
      nodeSelector:
        nvidia.com/gpu.product: NVIDIA-A100-SXM4-80GB
</file>

<file path="infrastructure/k8s/batch-inference-service.yaml">
apiVersion: apps/v1
kind: Deployment
metadata:
  name: batch-inference
  namespace: inference
  labels:
    app: chimera
    tier: inference
    latency: batch
spec:
  replicas: 12  # Scale based on workload
  selector:
    matchLabels:
      app: chimera
      tier: inference
      latency: batch
  template:
    metadata:
      labels:
        app: chimera
        tier: inference
        latency: batch
    spec:
      priorityClassName: batch-inference
      schedulerName: chimera-scheduler  # Use our custom GPU-aware scheduler
      terminationGracePeriodSeconds: 300  # 5 minutes for graceful shutdown/checkpointing
      containers:
      - name: batch-processor
        image: ${REGISTRY}/chimera/batch-processor:latest
        env:
        - name: NATS_SERVERS
          value: "nats://nats-0.nats:4222,nats://nats-1.nats:4222,nats://nats-2.nats:4222,nats://nats-3.nats:4222,nats://nats-4.nats:4222"
        - name: NATS_STREAM_SUBJECT
          value: "chimera.inference.batch.*"
        - name: TRITON_URL
          value: "triton-inference-server.inference.svc.cluster.local:8001"
        - name: ENABLE_TLS
          value: "true"
        - name: CHECKPOINT_BUCKET
          value: "chimera-checkpoints"
        - name: CHECKPOINT_INTERVAL_SEC
          value: "60"
        - name: AWS_REGION
          value: "eu-west-1"  # Adjust based on deployment
        - name: BATCH_PROCESSOR_ID
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        volumeMounts:
        - name: nats-certs
          mountPath: /etc/nats/certs
          readOnly: true
        - name: dshm
          mountPath: /dev/shm
        - name: checkpoint-volume
          mountPath: /data/checkpoints
        resources:
          limits:
            nvidia.com/gpu: 1
          requests:
            nvidia.com/gpu: 1
            cpu: "4"
            memory: "16Gi"
        lifecycle:
          preStop:
            exec:
              command: ["/bin/sh", "-c", "python /app/checkpoint.py --final"]
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 20
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 10
      volumes:
      - name: nats-certs
        secret:
          secretName: nats-client-tls
      - name: dshm
        emptyDir:
          medium: Memory
          sizeLimit: 4Gi
      - name: checkpoint-volume
        emptyDir: {}
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 80
            preference:
              matchExpressions:
              - key: node.kubernetes.io/instance-type
                operator: In
                values:
                - p3.8xlarge
                - g4dn.12xlarge
                - g5.12xlarge
          - weight: 60
            preference:
              matchExpressions:
              - key: nvidia.com/gpu.product
                operator: In
                values:
                - NVIDIA-A100-SXM4-80GB
                - NVIDIA-A100-PCIe-80GB
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: gpu-workload
                operator: In
                values:
                - batch
              - key: chimera/data-residency
                operator: In
                values:
                - eu-gdpr  # For GDPR compliance
      tolerations:
      - key: "node.kubernetes.io/spot-instance"
        operator: "Exists"
        effect: "NoSchedule"
      - key: "nvidia.com/gpu"
        operator: "Exists"
        effect: "NoSchedule"
---
apiVersion: v1
kind: Service
metadata:
  name: batch-inference
  namespace: inference
  labels:
    app: chimera
    tier: inference
spec:
  selector:
    app: chimera
    tier: inference
    latency: batch
  ports:
  - port: 8080
    targetPort: 8080
    name: http
  type: ClusterIP
</file>

<file path="infrastructure/k8s/gpu-monitoring-agent.yaml">
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: gpu-monitoring-agent
  namespace: monitoring
  labels:
    app: gpu-monitoring-agent
    component: chimera
spec:
  selector:
    matchLabels:
      app: gpu-monitoring-agent
  template:
    metadata:
      labels:
        app: gpu-monitoring-agent
        component: chimera
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8000"
        prometheus.io/path: "/metrics"
    spec:
      priorityClassName: system-node-critical
      containers:
      - name: gpu-monitor
        image: ${REGISTRY}/chimera/gpu-monitoring-agent:latest
        imagePullPolicy: Always
        env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: MONITORING_INTERVAL_SECONDS
          value: "15"
        ports:
        - containerPort: 8000
          name: metrics
        securityContext:
          runAsNonRoot: true
          runAsUser: 1000
          allowPrivilegeEscalation: false
        resources:
          limits:
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 100Mi
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 30
        volumeMounts:
        - name: gpu-fs
          mountPath: /usr/local/nvidia
      volumes:
      - name: gpu-fs
        hostPath:
          path: /usr/local/nvidia
      nodeSelector:
        nvidia.com/gpu.present: "true"
      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
---
apiVersion: v1
kind: Service
metadata:
  name: gpu-monitoring-service
  namespace: monitoring
  labels:
    app: gpu-monitoring-agent
    component: chimera
spec:
  selector:
    app: gpu-monitoring-agent
  ports:
  - name: metrics
    port: 8000
    targetPort: 8000
  type: ClusterIP
</file>

<file path="infrastructure/k8s/gpu-scheduler-config.yaml">
apiVersion: v1
kind: ConfigMap
metadata:
  name: chimera-scheduler-config
  namespace: kube-system
data:
  scheduler-config.yaml: |
    apiVersion: kubescheduler.config.k8s.io/v1beta3
    kind: KubeSchedulerConfiguration
    profiles:
    - schedulerName: chimera-scheduler
      plugins:
        score:
          enabled:
          - name: NodeResourcesBalancedAllocation
            weight: 2
          - name: NodeResourcesFit
            weight: 3
          - name: NodeAffinity
            weight: 5
          - name: PodTopologySpread
            weight: 4
      pluginConfig:
      - name: NodeResourcesFit
        args:
          scoringStrategy:
            type: MostAllocated
            resources:
            - name: cpu
              weight: 1
            - name: memory
              weight: 1
            - name: nvidia.com/gpu
              weight: 10
      - name: PodTopologySpread
        args:
          defaultingType: System
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: chimera-scheduler
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      component: chimera-scheduler
  template:
    metadata:
      labels:
        component: chimera-scheduler
    spec:
      containers:
      - name: kube-scheduler
        image: k8s.gcr.io/kube-scheduler:v1.22.0
        args:
          - kube-scheduler
          - --config=/etc/kubernetes/scheduler-config.yaml
          - --v=3
        volumeMounts:
          - name: config
            mountPath: /etc/kubernetes/
            readOnly: true
      volumes:
        - name: config
          configMap:
            name: chimera-scheduler-config
</file>

<file path="infrastructure/k8s/nats-jetstream.yaml">
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: nats
  labels:
    app: nats
spec:
  serviceName: nats
  replicas: 3
  selector:
    matchLabels:
      app: nats
  template:
    metadata:
      labels:
        app: nats
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - nats
              topologyKey: "kubernetes.io/hostname"
      containers:
      - name: nats
        image: nats:2.10-alpine
        args:
        - "-js"
        - "-c"
        - "/etc/nats/nats-server.conf"
        ports:
        - containerPort: 4222
          name: client
        - containerPort: 6222
          name: cluster
        - containerPort: 8222
          name: monitor
        - containerPort: 7777
          name: metrics
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        volumeMounts:
        - name: config-volume
          mountPath: /etc/nats
        - name: jetstream-storage
          mountPath: /data
        - name: tls-volume
          mountPath: /etc/nats/certs
          readOnly: true
        resources:
          requests:
            cpu: "500m"
            memory: "1Gi"
          limits:
            cpu: "1"
            memory: "2Gi"
        livenessProbe:
          httpGet:
            path: /
            port: 8222
          initialDelaySeconds: 10
          timeoutSeconds: 5
        readinessProbe:
          httpGet:
            path: /
            port: 8222
          initialDelaySeconds: 5
          periodSeconds: 10
      volumes:
      - name: config-volume
        configMap:
          name: nats-config
      - name: tls-volume
        secret:
          secretName: nats-server-tls
          defaultMode: 0400
      - name: jetstream-storage
        emptyDir: {}
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: nats-config
data:
  nats-server.conf: |
    # Basic server configuration
    port: 4222
    http: 8222
    server_name: nats-server

    # Cluster port on all interfaces
    cluster {
      name: nats-cluster
      port: 6222
      routes: [
        nats://nats-0.nats:6222
        nats://nats-1.nats:6222
        nats://nats-2.nats:6222
      ]
      # Static cluster_advertise that will be determined at runtime by routing system
      # cluster_advertise is commented out as the server can autodetect in k8s
      connect_retries: 30
      
      # TLS configuration for cluster routes (REQUIRED for compliance)
      tls {
        cert_file: "/etc/nats/certs/server.crt"
        key_file:  "/etc/nats/certs/server.key"
        ca_file:   "/etc/nats/certs/ca.crt"
        timeout:   5
      }
    }

    # JetStream configuration
    jetstream {
      store_dir: /data
      max_memory_store: 1G
      max_file_store: 8G
      domain: chimera
    }

    # Logging options
    debug: false
    trace: false
    logtime: true

    # TLS config for client connections (REQUIRED for compliance)
    tls {
      cert_file: "/etc/nats/certs/server.crt"
      key_file:  "/etc/nats/certs/server.key"
      ca_file:   "/etc/nats/certs/ca.crt"
      verify:    true
      timeout:   5
    }

    # Authorization for client connections
    authorization {
      # Default permissions
      permissions {
        publish: ["chimera.>", "_INBOX.>"]
        subscribe: ["chimera.>", "_INBOX.>"]
      }
      timeout: 2
    }
---
apiVersion: v1
kind: Service
metadata:
  name: nats
  labels:
    app: nats
spec:
  selector:
    app: nats
  clusterIP: None
  ports:
  - name: client
    port: 4222
  - name: cluster
    port: 6222
  - name: monitor
    port: 8222
---
apiVersion: v1
kind: Service
metadata:
  name: nats-client
  labels:
    app: nats
spec:
  selector:
    app: nats
  ports:
  - name: client
    port: 4222
    targetPort: 4222
  - name: monitor
    port: 8222
    targetPort: 8222
</file>

<file path="infrastructure/k8s/personalization-engine.yaml">
apiVersion: apps/v1
kind: Deployment
metadata:
  name: personalization-engine
  labels:
    app: personalization-engine
    component: ai-platform
spec:
  replicas: 2
  selector:
    matchLabels:
      app: personalization-engine
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  template:
    metadata:
      labels:
        app: personalization-engine
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8002"
        prometheus.io/path: "/metrics"
    spec:
      containers:
      - name: personalization-engine
        image: ${REGISTRY}/chimera/personalization-engine:latest
        imagePullPolicy: Always
        resources:
          limits:
            cpu: "2"
            memory: "4Gi"
            nvidia.com/gpu: "1"
          requests:
            cpu: "500m"
            memory: "1Gi"
        ports:
        - containerPort: 8002
          name: http
        env:
        - name: PORT
          value: "8002"
        - name: LOG_LEVEL
          value: "INFO"
        - name: REDIS_URL
          valueFrom:
            secretKeyRef:
              name: redis-credentials
              key: url
        - name: NATS_URL
          valueFrom:
            configMapKeyRef:
              name: nats-config
              key: url
        - name: NATS_USER
          valueFrom:
            secretKeyRef:
              name: nats-credentials
              key: user
              optional: true
        - name: NATS_PASSWORD
          valueFrom:
            secretKeyRef:
              name: nats-credentials
              key: password
              optional: true
        - name: EMBEDDING_MODEL
          value: "sentence-transformers/all-MiniLM-L6-v2"
        - name: EMBEDDING_DIMENSION
          value: "384"
        livenessProbe:
          httpGet:
            path: /health
            port: http
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /health
            port: http
          initialDelaySeconds: 10
          periodSeconds: 15
          timeoutSeconds: 5
        volumeMounts:
        - name: model-cache
          mountPath: /app/.cache
      volumes:
      - name: model-cache
        emptyDir:
          medium: Memory
          sizeLimit: 1Gi
---
apiVersion: v1
kind: Service
metadata:
  name: personalization-engine
  labels:
    app: personalization-engine
spec:
  ports:
  - port: 8002
    targetPort: 8002
    protocol: TCP
    name: http
  selector:
    app: personalization-engine
  type: ClusterIP
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: personalization-engine-ingress
  annotations:
    kubernetes.io/ingress.class: "nginx"
    nginx.ingress.kubernetes.io/proxy-body-size: "50m"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "300"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
spec:
  rules:
  - host: personalization.chimera.ai
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: personalization-engine
            port:
              number: 8002
</file>

<file path="infrastructure/k8s/priority-classes.yaml">
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: realtime-inference
value: 1000000
globalDefault: false
description: "This priority class is for real-time inference workloads"
---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: batch-inference
value: 500000
globalDefault: false
description: "This priority class is for batch inference workloads"
---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: spot-batch-inference
value: 100000
globalDefault: false
description: "This priority class is for batch inference workloads on spot instances"
</file>

<file path="infrastructure/k8s/realtime-inference-service.yaml">
apiVersion: apps/v1
kind: Deployment
metadata:
  name: realtime-inference
  namespace: inference
  labels:
    app: chimera
    tier: inference
    latency: critical
spec:
  replicas: 8
  selector:
    matchLabels:
      app: chimera
      tier: inference
      latency: critical
  template:
    metadata:
      labels:
        app: chimera
        tier: inference
        latency: critical
    spec:
      priorityClassName: realtime-inference
      schedulerName: chimera-scheduler  # Use our custom GPU-aware scheduler
      containers:
      - name: inference-container
        image: ${REGISTRY}/chimera/realtime-processor:latest
        env:
        - name: NATS_SERVERS
          value: "nats://nats-0.nats:4222,nats://nats-1.nats:4222,nats://nats-2.nats:4222,nats://nats-3.nats:4222,nats://nats-4.nats:4222"
        - name: NATS_STREAM_SUBJECT
          value: "chimera.inference.realtime.*"
        - name: TRITON_URL
          value: "triton-inference-server.inference.svc.cluster.local:8001"
        - name: ENABLE_TLS
          value: "true"
        - name: MAX_BATCH_SIZE
          value: "8"
        - name: MAX_LATENCY_MS
          value: "50"
        - name: REQUEST_TIMEOUT_MS
          value: "150"
        volumeMounts:
        - name: nats-certs
          mountPath: /etc/nats/certs
          readOnly: true
        - name: dshm
          mountPath: /dev/shm
        resources:
          limits:
            nvidia.com/gpu: 1
          requests:
            nvidia.com/gpu: 1
            cpu: "4"
            memory: "16Gi"
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
      volumes:
      - name: nats-certs
        secret:
          secretName: nats-client-tls
      - name: dshm
        emptyDir:
          medium: Memory
          sizeLimit: 4Gi
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: nvidia.com/gpu.product
                operator: In
                values:
                - NVIDIA-H100-PCIe
                - NVIDIA-H100-SXM5-80GB
              - key: chimera/data-residency
                operator: In
                values:
                - eu-gdpr  # For GDPR compliance
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 50
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: latency
                  operator: In
                  values:
                  - critical
              topologyKey: "kubernetes.io/hostname"
      tolerations:
      - key: "nvidia.com/gpu"
        operator: "Exists"
        effect: "NoSchedule"
---
apiVersion: v1
kind: Service
metadata:
  name: realtime-inference
  namespace: inference
  labels:
    app: chimera
    tier: inference
spec:
  selector:
    app: chimera
    tier: inference
    latency: critical
  ports:
  - port: 8080
    targetPort: 8080
    name: http
  type: ClusterIP
</file>

<file path="infrastructure/k8s/triton-inference-server.yaml">
apiVersion: apps/v1
kind: Deployment
metadata:
  name: triton-inference-server
  labels:
    app: triton
spec:
  replicas: 1
  selector:
    matchLabels:
      app: triton
  template:
    metadata:
      labels:
        app: triton
    spec:
      containers:
      - name: triton
        image: nvcr.io/nvidia/tritonserver:24.02-py3
        args:
          - "tritonserver"
          - "--model-repository=/models"
          - "--http-port=8000"
          - "--grpc-port=8001"
          - "--metrics-port=8002"
          - "--log-verbose=1"
          - "--model-control-mode=explicit"
          - "--strict-model-config=false"
          - "--log-error=true"
        ports:
        - containerPort: 8000
          name: http
        - containerPort: 8001
          name: grpc
        - containerPort: 8002
          name: metrics
        volumeMounts:
        - name: model-repo
          mountPath: /models
        resources:
          limits:
            nvidia.com/gpu: 1
          requests:
            memory: "4Gi"
            cpu: "2"
        readinessProbe:
          httpGet:
            path: /v2/health/ready
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 5
        livenessProbe:
          httpGet:
            path: /v2/health/live
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 5
      volumes:
      - name: model-repo
        persistentVolumeClaim:
          claimName: triton-model-pvc
      nodeSelector:
        nvidia.com/gpu.product: NVIDIA-A100-SXM4-80GB
---
apiVersion: v1
kind: Service
metadata:
  name: triton-service
spec:
  selector:
    app: triton
  ports:
  - name: http
    port: 8000
    targetPort: 8000
  - name: grpc
    port: 8001
    targetPort: 8001
  - name: metrics
    port: 8002
    targetPort: 8002
  type: ClusterIP
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: triton-model-pvc
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 50Gi
  storageClassName: standard
</file>

<file path="infrastructure/k8s/vector-store-backup-cronjob.yaml">
apiVersion: batch/v1
kind: CronJob
metadata:
  name: vector-store-backup
  labels:
    app: personalization-engine
    component: vector-store-backup
spec:
  schedule: "0 1 * * *"  # Daily at 1 AM UTC
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: personalization-engine
            component: vector-store-backup
          annotations:
            prometheus.io/scrape: "true"
            prometheus.io/port: "9090"
            prometheus.io/path: "/metrics"
        spec:
          containers:
          - name: backup-job
            image: ${REGISTRY}/chimera/personalization-engine:latest
            imagePullPolicy: Always
            command: ["/usr/local/bin/python"]
            args:
            - "-c"
            - |
              import asyncio
              import os
              from prometheus_client import start_http_server
              from app.vector_store import VectorStore
              from loguru import logger

              # Start Prometheus metrics server
              start_http_server(9090)
              logger.info("Started Prometheus metrics server on port 9090")

              async def run_backup():
                  try:
                      # Initialize vector store
                      vector_store = VectorStore(
                          redis_url=os.getenv("REDIS_URL"),
                          backup_dir=os.getenv("BACKUP_DIR", "/data/backups"),
                          s3_bucket=os.getenv("S3_BUCKET"),
                          s3_prefix=os.getenv("S3_PREFIX", "vector_store_backups")
                      )
                      await vector_store.initialize()

                      # Create backup
                      backup_id = await vector_store.backup(upload_to_s3=True)
                      logger.info(f"Backup {backup_id} completed successfully")

                      # Clean up old backups
                      await vector_store.cleanup_old_backups(
                          retain_days=int(os.getenv("RETAIN_DAYS", "7")),
                          retain_weekly=int(os.getenv("RETAIN_WEEKLY", "4")),
                          retain_monthly=int(os.getenv("RETAIN_MONTHLY", "6"))
                      )
                      logger.info("Cleaned up old backups")

                      # Close connections
                      await vector_store.close()

                  except Exception as e:
                      logger.error(f"Backup failed: {e}")
                      raise

              asyncio.run(run_backup())
            ports:
            - name: metrics
              containerPort: 9090
              protocol: TCP
            env:
            - name: REDIS_URL
              valueFrom:
                secretKeyRef:
                  name: redis-credentials
                  key: url
            - name: S3_BUCKET
              valueFrom:
                configMapKeyRef:
                  name: backup-config
                  key: s3_bucket
            - name: S3_PREFIX
              valueFrom:
                configMapKeyRef:
                  name: backup-config
                  key: s3_prefix
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: aws-credentials
                  key: access_key_id
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: aws-credentials
                  key: secret_access_key
            - name: AWS_DEFAULT_REGION
              valueFrom:
                configMapKeyRef:
                  name: backup-config
                  key: aws_region
            - name: RETAIN_DAYS
              value: "7"
            - name: RETAIN_WEEKLY
              value: "4"
            - name: RETAIN_MONTHLY
              value: "6"
            - name: BACKUP_DIR
              value: "/data/backups"
            - name: LOG_LEVEL
              value: "INFO"
            volumeMounts:
            - name: backup-storage
              mountPath: /data/backups
            readinessProbe:
              httpGet:
                path: /metrics
                port: metrics
              initialDelaySeconds: 5
              periodSeconds: 10
            livenessProbe:
              httpGet:
                path: /metrics
                port: metrics
              initialDelaySeconds: 15
              periodSeconds: 20
          volumes:
          - name: backup-storage
            persistentVolumeClaim:
              claimName: vector-store-backup-pvc
          restartPolicy: OnFailure
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: vector-store-backup-pvc
  labels:
    app: personalization-engine
    component: vector-store-backup
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 50Gi
  storageClassName: standard
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: backup-config
  labels:
    app: personalization-engine
    component: vector-store-backup
data:
  s3_bucket: "your-backup-bucket"
  s3_prefix: "vector_store_backups"
  aws_region: "us-west-2"
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: vector-store-backup-monitor
  labels:
    app: personalization-engine
    component: vector-store-backup
spec:
  selector:
    matchLabels:
      app: personalization-engine
      component: vector-store-backup
  endpoints:
  - port: metrics
    interval: 30s
---
apiVersion: v1
kind: Service
metadata:
  name: vector-store-backup-metrics
  labels:
    app: personalization-engine
    component: vector-store-backup
spec:
  ports:
  - name: metrics
    port: 9090
    targetPort: metrics
  selector:
    app: personalization-engine
    component: vector-store-backup
</file>

<file path="infrastructure/monitoring/grafana/dashboards/chimera_overview.json">
{
  "annotations": {
    "list": [
      {
        "builtIn": 1,
        "datasource": {
          "type": "grafana",
          "uid": "-- Grafana --"
        },
        "enable": true,
        "hide": true,
        "iconColor": "rgba(0, 211, 255, 1)",
        "name": "Annotations & Alerts",
        "type": "dashboard"
      }
    ]
  },
  "editable": true,
  "fiscalYearStartMonth": 0,
  "graphTooltip": 0,
  "id": 1,
  "links": [],
  "liveNow": false,
  "panels": [
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "line",
            "fillOpacity": 0,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "auto",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          }
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 0
      },
      "id": 1,
      "options": {
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "single",
          "sort": "none"
        }
      },
      "title": "Requests Per Second",
      "type": "timeseries",
      "targets": [
        {
          "datasource": "Prometheus",
          "editorMode": "code",
          "expr": "sum(rate(http_requests_total{job=~\"ingestion-service|ml-orchestrator|knowledge-graph|personalization-engine\"}[5m])) by (job)",
          "legendFormat": "{{job}}",
          "range": true,
          "refId": "A"
        }
      ]
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "line",
            "fillOpacity": 0,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "auto",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "s"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 0
      },
      "id": 2,
      "options": {
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "single",
          "sort": "none"
        }
      },
      "title": "Latency",
      "type": "timeseries",
      "targets": [
        {
          "datasource": "Prometheus",
          "editorMode": "code",
          "expr": "histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket{job=~\"ingestion-service|ml-orchestrator|knowledge-graph|personalization-engine\"}[5m])) by (job, le))",
          "legendFormat": "{{job}} (p95)",
          "range": true,
          "refId": "A"
        }
      ]
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "thresholds"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "orange",
                "value": 0.7
              },
              {
                "color": "red",
                "value": 0.85
              }
            ]
          },
          "unit": "percentunit"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 8
      },
      "id": 3,
      "options": {
        "orientation": "auto",
        "reduceOptions": {
          "calcs": [
            "lastNotNull"
          ],
          "fields": "",
          "values": false
        },
        "showThresholdLabels": false,
        "showThresholdMarkers": true
      },
      "pluginVersion": "9.5.1",
      "title": "CPU Usage",
      "type": "gauge",
      "targets": [
        {
          "datasource": "Prometheus",
          "editorMode": "code",
          "expr": "sum(rate(container_cpu_usage_seconds_total{container=~\"ingestion-service|ml-orchestrator|knowledge-graph|personalization-engine\"}[5m])) by (container) / sum(container_spec_cpu_quota{container=~\"ingestion-service|ml-orchestrator|knowledge-graph|personalization-engine\"} / container_spec_cpu_period{container=~\"ingestion-service|ml-orchestrator|knowledge-graph|personalization-engine\"}) by (container)",
          "legendFormat": "{{container}}",
          "range": true,
          "refId": "A"
        }
      ]
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "thresholds"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "orange",
                "value": 0.7
              },
              {
                "color": "red",
                "value": 0.85
              }
            ]
          },
          "unit": "percentunit"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 8
      },
      "id": 4,
      "options": {
        "orientation": "auto",
        "reduceOptions": {
          "calcs": [
            "lastNotNull"
          ],
          "fields": "",
          "values": false
        },
        "showThresholdLabels": false,
        "showThresholdMarkers": true
      },
      "pluginVersion": "9.5.1",
      "title": "Memory Usage",
      "type": "gauge",
      "targets": [
        {
          "datasource": "Prometheus",
          "editorMode": "code",
          "expr": "sum(container_memory_usage_bytes{container=~\"ingestion-service|ml-orchestrator|knowledge-graph|personalization-engine\"}) by (container) / sum(container_spec_memory_limit_bytes{container=~\"ingestion-service|ml-orchestrator|knowledge-graph|personalization-engine\"}) by (container)",
          "legendFormat": "{{container}}",
          "range": true,
          "refId": "A"
        }
      ]
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "line",
            "fillOpacity": 0,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "auto",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          }
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 16
      },
      "id": 5,
      "options": {
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "single",
          "sort": "none"
        }
      },
      "title": "NATS Messages",
      "type": "timeseries",
      "targets": [
        {
          "datasource": "Prometheus",
          "editorMode": "code",
          "expr": "sum(rate(nats_messages_total{job=\"nats\"}[5m])) by (subject)",
          "legendFormat": "{{subject}}",
          "range": true,
          "refId": "A"
        }
      ]
    },
    {
      "datasource": {
        "type": "jaeger",
        "uid": "jaeger"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "thresholds"
          },
          "custom": {
            "align": "auto",
            "cellOptions": {
              "type": "auto"
            },
            "inspect": false
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          }
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 16
      },
      "id": 6,
      "options": {
        "footer": {
          "countRows": false,
          "fields": "",
          "reducer": [
            "sum"
          ],
          "show": false
        },
        "showHeader": true
      },
      "pluginVersion": "9.5.1",
      "title": "Trace Analysis",
      "type": "table",
      "targets": [
        {
          "datasource": "Jaeger",
          "limit": 20,
          "operation": "ingestion-service",
          "queryType": "search",
          "refId": "A",
          "service": "chimera"
        }
      ]
    }
  ],
  "refresh": "",
  "schemaVersion": 38,
  "style": "dark",
  "tags": [],
  "templating": {
    "list": []
  },
  "time": {
    "from": "now-6h",
    "to": "now"
  },
  "timepicker": {},
  "timezone": "",
  "title": "Chimera Platform Overview",
  "uid": "chimera-overview",
  "version": 1,
  "weekStart": ""
}
</file>

<file path="infrastructure/monitoring/grafana/dashboards/chimera_performance.json">
{
  "annotations": {
    "list": [
      {
        "builtIn": 1,
        "datasource": "-- Grafana --",
        "enable": true,
        "hide": true,
        "iconColor": "rgba(0, 211, 255, 1)",
        "name": "Annotations & Alerts",
        "type": "dashboard"
      }
    ]
  },
  "editable": true,
  "gnetId": null,
  "graphTooltip": 0,
  "id": 20,
  "links": [],
  "panels": [
    {
      "collapsed": false,
      "datasource": null,
      "gridPos": {
        "h": 1,
        "w": 24,
        "x": 0,
        "y": 0
      },
      "id": 12,
      "panels": [],
      "title": "GPU Utilization & Performance",
      "type": "row"
    },
    {
      "aliasColors": {},
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": "Prometheus",
      "fieldConfig": {
        "defaults": {},
        "overrides": []
      },
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 1
      },
      "hiddenSeries": false,
      "id": 2,
      "legend": {
        "avg": false,
        "current": true,
        "max": true,
        "min": false,
        "show": true,
        "total": false,
        "values": true
      },
      "lines": true,
      "linewidth": 1,
      "nullPointMode": "null",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "7.5.5",
      "pointradius": 2,
      "points": false,
      "renderer": "flot",
      "seriesOverrides": [],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "expr": "avg(chimera_gpu_utilization_percent) by (gpu_type)",
          "interval": "",
          "legendFormat": "{{gpu_type}}",
          "refId": "A"
        }
      ],
      "thresholds": [
        {
          "colorMode": "critical",
          "fill": true,
          "line": true,
          "op": "lt",
          "value": 65,
          "visible": true
        },
        {
          "colorMode": "ok",
          "fill": true,
          "line": true,
          "op": "gt",
          "value": 85,
          "visible": true
        }
      ],
      "timeFrom": null,
      "timeRegions": [],
      "timeShift": null,
      "title": "GPU Utilization by Type",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "format": "percent",
          "label": null,
          "logBase": 1,
          "max": "100",
          "min": "0",
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "aliasColors": {},
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": "Prometheus",
      "fieldConfig": {
        "defaults": {},
        "overrides": []
      },
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 1
      },
      "hiddenSeries": false,
      "id": 4,
      "legend": {
        "avg": false,
        "current": true,
        "max": false,
        "min": false,
        "show": true,
        "total": false,
        "values": true
      },
      "lines": true,
      "linewidth": 1,
      "nullPointMode": "null",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "7.5.5",
      "pointradius": 2,
      "points": false,
      "renderer": "flot",
      "seriesOverrides": [],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "expr": "sum(rate(nats_jetstream_messages_total[1m])) by (stream)",
          "interval": "",
          "legendFormat": "{{stream}}",
          "refId": "A"
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeRegions": [],
      "timeShift": null,
      "title": "NATS JetStream Throughput",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "format": "ops",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "datasource": "Prometheus",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "thresholds"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "orange",
                "value": 150
              },
              {
                "color": "red",
                "value": 200
              }
            ]
          },
          "unit": "ms"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 9
      },
      "id": 6,
      "options": {
        "displayMode": "gradient",
        "orientation": "auto",
        "reduceOptions": {
          "calcs": [
            "lastNotNull"
          ],
          "fields": "",
          "values": false
        },
        "showUnfilled": true,
        "text": {}
      },
      "pluginVersion": "7.5.5",
      "targets": [
        {
          "expr": "histogram_quantile(0.95, sum(rate(chimera_inference_latency_bucket{type=\"realtime\"}[5m])) by (le))",
          "interval": "",
          "legendFormat": "p95",
          "refId": "A"
        },
        {
          "expr": "histogram_quantile(0.99, sum(rate(chimera_inference_latency_bucket{type=\"realtime\"}[5m])) by (le))",
          "interval": "",
          "legendFormat": "p99",
          "refId": "B"
        },
        {
          "expr": "histogram_quantile(0.50, sum(rate(chimera_inference_latency_bucket{type=\"realtime\"}[5m])) by (le))",
          "interval": "",
          "legendFormat": "p50",
          "refId": "C"
        }
      ],
      "title": "Real-time Inference Latency",
      "type": "bargauge"
    },
    {
      "datasource": "Prometheus",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "thresholds"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "yellow",
                "value": 80
              },
              {
                "color": "red",
                "value": 90
              }
            ]
          },
          "unit": "percent"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 9
      },
      "id": 8,
      "options": {
        "colorMode": "value",
        "graphMode": "area",
        "justifyMode": "auto",
        "orientation": "auto",
        "reduceOptions": {
          "calcs": [
            "lastNotNull"
          ],
          "fields": "",
          "values": false
        },
        "text": {},
        "textMode": "auto"
      },
      "pluginVersion": "7.5.5",
      "targets": [
        {
          "expr": "avg(chimera_gpu_memory_used_bytes / chimera_gpu_memory_total_bytes * 100) by (gpu_type)",
          "interval": "",
          "legendFormat": "{{gpu_type}}",
          "refId": "A"
        }
      ],
      "title": "GPU Memory Utilization",
      "type": "stat"
    },
    {
      "collapsed": false,
      "datasource": null,
      "gridPos": {
        "h": 1,
        "w": 24,
        "x": 0,
        "y": 17
      },
      "id": 14,
      "panels": [],
      "title": "NATS JetStream Health",
      "type": "row"
    },
    {
      "aliasColors": {},
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": "Prometheus",
      "fieldConfig": {
        "defaults": {},
        "overrides": []
      },
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 18
      },
      "hiddenSeries": false,
      "id": 16,
      "legend": {
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": true,
      "linewidth": 1,
      "nullPointMode": "null",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "7.5.5",
      "pointradius": 2,
      "points": false,
      "renderer": "flot",
      "seriesOverrides": [],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "expr": "sum(nats_jetstream_consumer_ack_pending) by (stream, consumer)",
          "interval": "",
          "legendFormat": "{{stream}} - {{consumer}}",
          "refId": "A"
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeRegions": [],
      "timeShift": null,
      "title": "JetStream Pending Acks by Consumer",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": "0",
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "aliasColors": {},
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": "Prometheus",
      "fieldConfig": {
        "defaults": {},
        "overrides": []
      },
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 18
      },
      "hiddenSeries": false,
      "id": 18,
      "legend": {
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": true,
      "linewidth": 1,
      "nullPointMode": "null",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "7.5.5",
      "pointradius": 2,
      "points": false,
      "renderer": "flot",
      "seriesOverrides": [],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "expr": "rate(nats_jetstream_consumer_nak_count[1m])",
          "interval": "",
          "legendFormat": "{{stream}} - {{consumer}}",
          "refId": "A"
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeRegions": [],
      "timeShift": null,
      "title": "JetStream Message NAK Rate",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": "0",
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    }
  ],
  "refresh": "10s",
  "schemaVersion": 27,
  "style": "dark",
  "tags": [
    "chimera",
    "performance"
  ],
  "templating": {
    "list": []
  },
  "time": {
    "from": "now-1h",
    "to": "now"
  },
  "timepicker": {},
  "timezone": "",
  "title": "Chimera Performance Dashboard",
  "uid": "chimera-perf",
  "version": 1
}
</file>

<file path="infrastructure/monitoring/grafana/dashboards/inter_service_communication.json">
{
  "annotations": {
    "list": [
      {
        "builtIn": 1,
        "datasource": {
          "type": "grafana",
          "uid": "-- Grafana --"
        },
        "enable": true,
        "hide": true,
        "iconColor": "rgba(0, 211, 255, 1)",
        "name": "Annotations & Alerts",
        "type": "dashboard"
      }
    ]
  },
  "editable": true,
  "fiscalYearStartMonth": 0,
  "graphTooltip": 0,
  "id": 2,
  "links": [],
  "liveNow": false,
  "panels": [
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "description": "Overview of NATS message volume across all subjects",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "Messages per second",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "line",
            "fillOpacity": 10,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "lineInterpolation": "smooth",
            "lineWidth": 2,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "auto",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "ops"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 0
      },
      "id": 1,
      "options": {
        "legend": {
          "calcs": [
            "mean",
            "max"
          ],
          "displayMode": "table",
          "placement": "right",
          "showLegend": true,
          "width": 200
        },
        "tooltip": {
          "mode": "single",
          "sort": "none"
        }
      },
      "title": "NATS Message Volume by Subject",
      "type": "timeseries",
      "targets": [
        {
          "datasource": "Prometheus",
          "editorMode": "code",
          "expr": "sum(rate(nats_messages_total{job=\"nats\"}[1m])) by (subject)",
          "legendFormat": "{{subject}}",
          "range": true,
          "refId": "A"
        }
      ]
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "description": "NATS connection status for all services",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "thresholds"
          },
          "mappings": [
            {
              "options": {
                "0": {
                  "color": "red",
                  "index": 0,
                  "text": "Disconnected"
                },
                "1": {
                  "color": "green",
                  "index": 1,
                  "text": "Connected"
                }
              },
              "type": "value"
            }
          ],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "red",
                "value": null
              },
              {
                "color": "orange",
                "value": 0.5
              },
              {
                "color": "green",
                "value": 1
              }
            ]
          }
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 0
      },
      "id": 2,
      "options": {
        "colorMode": "value",
        "graphMode": "area",
        "justifyMode": "auto",
        "orientation": "auto",
        "reduceOptions": {
          "calcs": [
            "lastNotNull"
          ],
          "fields": "",
          "values": false
        },
        "textMode": "auto"
      },
      "pluginVersion": "9.5.1",
      "title": "NATS Connection Status",
      "type": "stat",
      "targets": [
        {
          "datasource": "Prometheus",
          "editorMode": "code",
          "expr": "nats_connection_active{job=~\"ingestion-service|ml-orchestrator|knowledge-graph|personalization-engine\"}",
          "legendFormat": "{{job}}",
          "range": true,
          "refId": "A"
        }
      ]
    },
    {
      "datasource": {
        "type": "jaeger",
        "uid": "jaeger"
      },
      "description": "Service-to-service communication flows visualized as a DAG",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "thresholds"
          },
          "custom": {
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          }
        },
        "overrides": []
      },
      "gridPos": {
        "h": 12,
        "w": 24,
        "x": 0,
        "y": 8
      },
      "id": 3,
      "options": {
        "edges": {
          "constraints": [],
          "discreteMappings": [],
          "mappings": []
        },
        "hideEmpty": true,
        "nodes": {
          "arcs": [],
          "constraints": [],
          "discreteMappings": [],
          "mainStatistic": "",
          "mappings": [],
          "secondaryStatistic": ""
        },
        "onNodeClick": {
          "mode": "nodeData"
        },
        "view": {}
      },
      "title": "Service Communication Flow",
      "type": "nodeGraph",
      "targets": [
        {
          "datasource": "Jaeger",
          "limit": 20,
          "operation": "",
          "queryType": "search",
          "refId": "A",
          "service": "chimera",
          "tags": []
        }
      ]
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "description": "Communication latency between services (direct HTTP calls)",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "line",
            "fillOpacity": 20,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "lineInterpolation": "linear",
            "lineWidth": 2,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "auto",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "s"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 20
      },
      "id": 4,
      "options": {
        "legend": {
          "calcs": [
            "mean",
            "p95"
          ],
          "displayMode": "table",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "single",
          "sort": "none"
        }
      },
      "title": "HTTP Communication Latency (P95)",
      "type": "timeseries",
      "targets": [
        {
          "datasource": "Prometheus",
          "editorMode": "code",
          "expr": "histogram_quantile(0.95, sum(rate(http_client_request_duration_seconds_bucket{job=~\"ingestion-service|ml-orchestrator|knowledge-graph|personalization-engine\"}[5m])) by (job, target_service, le))",
          "legendFormat": "{{job}} -> {{target_service}}",
          "range": true,
          "refId": "A"
        }
      ]
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "description": "Communication latency via NATS messaging",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "line",
            "fillOpacity": 20,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "lineInterpolation": "linear",
            "lineWidth": 2,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "auto",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "s"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 20
      },
      "id": 5,
      "options": {
        "legend": {
          "calcs": [
            "mean",
            "p95"
          ],
          "displayMode": "table",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "single",
          "sort": "none"
        }
      },
      "title": "NATS Message Latency (P95)",
      "type": "timeseries",
      "targets": [
        {
          "datasource": "Prometheus",
          "editorMode": "code",
          "expr": "histogram_quantile(0.95, sum(rate(nats_message_latency_seconds_bucket{job=~\"ingestion-service|ml-orchestrator|knowledge-graph|personalization-engine\"}[5m])) by (job, subject, le))",
          "legendFormat": "{{job}} -> {{subject}}",
          "range": true,
          "refId": "A"
        }
      ]
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "description": "Error rates for communication between services",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "line",
            "fillOpacity": 20,
            "gradientMode": "scheme",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "lineInterpolation": "linear",
            "lineWidth": 2,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "auto",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "percentunit"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 24,
        "x": 0,
        "y": 28
      },
      "id": 6,
      "options": {
        "legend": {
          "calcs": [
            "mean",
            "max"
          ],
          "displayMode": "table",
          "placement": "right",
          "showLegend": true,
          "width": 200
        },
        "tooltip": {
          "mode": "single",
          "sort": "none"
        }
      },
      "title": "Communication Error Rate",
      "type": "timeseries",
      "targets": [
        {
          "datasource": "Prometheus",
          "editorMode": "code",
          "expr": "sum(rate(http_client_request_errors_total{job=~\"ingestion-service|ml-orchestrator|knowledge-graph|personalization-engine\"}[5m])) by (job, target_service) / sum(rate(http_client_requests_total{job=~\"ingestion-service|ml-orchestrator|knowledge-graph|personalization-engine\"}[5m])) by (job, target_service)",
          "legendFormat": "HTTP: {{job}} -> {{target_service}}",
          "range": true,
          "refId": "A"
        },
        {
          "datasource": "Prometheus",
          "editorMode": "code",
          "expr": "sum(rate(nats_message_errors_total{job=~\"ingestion-service|ml-orchestrator|knowledge-graph|personalization-engine\"}[5m])) by (job, subject) / sum(rate(nats_messages_total{job=~\"ingestion-service|ml-orchestrator|knowledge-graph|personalization-engine\"}[5m])) by (job, subject)",
          "hide": false,
          "legendFormat": "NATS: {{job}} -> {{subject}}",
          "range": true,
          "refId": "B"
        }
      ]
    }
  ],
  "refresh": "10s",
  "schemaVersion": 38,
  "style": "dark",
  "tags": [
    "communication",
    "nats",
    "http"
  ],
  "templating": {
    "list": []
  },
  "time": {
    "from": "now-1h",
    "to": "now"
  },
  "timepicker": {},
  "timezone": "",
  "title": "Inter-Service Communication",
  "uid": "inter-service-comm",
  "version": 1,
  "weekStart": ""
}
</file>

<file path="infrastructure/monitoring/grafana/provisioning/datasources/datasources.yml">
apiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
    editable: false
    version: 1

  - name: Loki
    type: loki
    access: proxy
    url: http://loki:3100
    editable: false
    version: 1
    jsonData:
      maxLines: 1000

  - name: Jaeger
    type: jaeger
    access: proxy
    url: http://jaeger:16686
    editable: false
    version: 1
    uid: jaeger
</file>

<file path="infrastructure/monitoring/loki/loki-config.yml">
auth_enabled: false

server:
  http_listen_port: 3100
  grpc_listen_port: 9096

common:
  path_prefix: /loki
  storage:
    filesystem:
      chunks_directory: /loki/chunks
      rules_directory: /loki/rules
  replication_factor: 1
  ring:
    instance_addr: 127.0.0.1
    kvstore:
      store: inmemory

schema_config:
  configs:
    - from: 2023-01-01
      store: boltdb-shipper
      object_store: filesystem
      schema: v12
      index:
        prefix: index_
        period: 24h

ruler:
  alertmanager_url: http://localhost:9093

limits_config:
  enforce_metric_name: false
  reject_old_samples: true
  reject_old_samples_max_age: 168h
  ingestion_rate_mb: 30
  ingestion_burst_size_mb: 60

compactor:
  working_directory: /loki/compactor
  shared_store: filesystem
  compaction_interval: 5m
  retention_enabled: true
  retention_delete_delay: 1h
  retention_delete_worker_count: 150
</file>

<file path="infrastructure/monitoring/otel/otel-collector-config.yml">
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318

  prometheus:
    config:
      scrape_configs:
        - job_name: 'otel-collector'
          scrape_interval: 10s
          static_configs:
            - targets: ['0.0.0.0:8888']

processors:
  batch:
    timeout: 1s
    send_batch_size: 1024

  memory_limiter:
    check_interval: 1s
    limit_mib: 1024
    spike_limit_mib: 512

  resource:
    attributes:
      - key: service.namespace
        value: chimera
        action: upsert

  probabilistic_sampler:
    hash_seed: 42
    sampling_percentage: 75

exporters:
  prometheus:
    endpoint: 0.0.0.0:8889
    namespace: chimera
    send_timestamps: true
    metric_expiration: 1m

  otlp:
    endpoint: jaeger:4317
    tls:
      insecure: true

  otlp/logs:
    endpoint: loki:3100
    tls:
      insecure: true

  logging:
    verbosity: detailed
    sampling_initial: 5
    sampling_thereafter: 200

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [memory_limiter, batch, probabilistic_sampler, resource]
      exporters: [otlp, logging]

    metrics:
      receivers: [otlp, prometheus]
      processors: [memory_limiter, batch, resource]
      exporters: [prometheus, logging]
      
    logs:
      receivers: [otlp]
      processors: [memory_limiter, batch, resource]
      exporters: [otlp/logs, logging]

  telemetry:
    logs:
      level: info
    metrics:
      address: 0.0.0.0:8888
</file>

<file path="infrastructure/monitoring/promtail/promtail-config.yml">
server:
  http_listen_port: 9080
  grpc_listen_port: 0

positions:
  filename: /tmp/positions.yaml

clients:
  - url: http://loki:3100/loki/api/v1/push

scrape_configs:
  - job_name: docker
    docker_sd_configs:
      - host: unix:///var/run/docker.sock
        refresh_interval: 5s
        filters:
          - name: label
            values: ["com.docker.compose.project=chimera"]
    relabel_configs:
      - source_labels: ['__meta_docker_container_name']
        regex: '/(.*)'
        target_label: 'container'
      - source_labels: ['__meta_docker_container_log_stream']
        target_label: 'logstream'
      - source_labels: ['__meta_docker_container_label_com_docker_compose_service']
        target_label: 'service'
      - source_labels: ['__meta_docker_container_label_com_docker_compose_project']
        target_label: 'project'

  - job_name: kubernetes
    kubernetes_sd_configs:
      - role: pod
    pipeline_stages:
      - json:
          expressions:
            level: level
            message: message
            timestamp: timestamp
      - labels:
          level:
      - timestamp:
          source: timestamp
          format: RFC3339Nano
    relabel_configs:
      - source_labels: [__meta_kubernetes_pod_label_app]
        target_label: app
      - source_labels: [__meta_kubernetes_pod_container_name]
        target_label: container
      - source_labels: [__meta_kubernetes_namespace]
        target_label: namespace
      - source_labels: [__meta_kubernetes_pod_name]
        target_label: pod
      - action: replace
        replacement: /var/log/pods/*$1/*.log
        separator: /
        source_labels:
          - __meta_kubernetes_pod_uid
          - __meta_kubernetes_pod_container_name
        target_label: __path__

  - job_name: microservices
    static_configs:
      - targets:
          - localhost
        labels:
          job: microservices
          __path__: /var/log/microservices/*.log
    pipeline_stages:
      - json:
          expressions:
            level: level
            ts: timestamp
            msg: message
            svc: service
      - labels:
          level:
          svc:
      - timestamp:
          source: ts
          format: RFC3339Nano
</file>

<file path="infrastructure/monitoring/docker-compose.monitoring.yml">
version: '3.8'

services:
  # Prometheus for metrics collection
  prometheus:
    image: prom/prometheus:v2.45.0
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
    restart: unless-stopped
    networks:
      - monitoring

  # Grafana for visualization
  grafana:
    image: grafana/grafana:10.1.0
    container_name: grafana
    ports:
      - "3000:3000"
    volumes:
      - ./grafana/provisioning:/etc/grafana/provisioning
      - ./grafana/dashboards:/var/lib/grafana/dashboards
      - grafana_data:/var/lib/grafana
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=chimera-admin
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_INSTALL_PLUGINS=grafana-piechart-panel,grafana-worldmap-panel
    restart: unless-stopped
    networks:
      - monitoring
    depends_on:
      - prometheus
      - loki

  # Loki for log aggregation
  loki:
    image: grafana/loki:2.9.1
    container_name: loki
    ports:
      - "3100:3100"
    volumes:
      - ./loki/loki-config.yml:/etc/loki/local-config.yaml
      - loki_data:/loki
    command: -config.file=/etc/loki/local-config.yaml
    restart: unless-stopped
    networks:
      - monitoring

  # Promtail for log collection from containers
  promtail:
    image: grafana/promtail:2.9.1
    container_name: promtail
    volumes:
      - ./promtail/promtail-config.yml:/etc/promtail/config.yml
      - /var/log:/var/log
      - /var/lib/docker/containers:/var/lib/docker/containers
    command: -config.file=/etc/promtail/config.yml
    restart: unless-stopped
    networks:
      - monitoring
    depends_on:
      - loki

  # Node exporter for host metrics
  node-exporter:
    image: prom/node-exporter:v1.6.0
    container_name: node-exporter
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--path.rootfs=/rootfs'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    restart: unless-stopped
    ports:
      - "9100:9100"
    networks:
      - monitoring

  # cAdvisor for container metrics
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.47.1
    container_name: cadvisor
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    devices:
      - /dev/kmsg
    restart: unless-stopped
    ports:
      - "8080:8080"
    networks:
      - monitoring

  # Jaeger for distributed tracing
  jaeger:
    image: jaegertracing/all-in-one:1.47
    container_name: jaeger
    ports:
      - "5775:5775/udp"
      - "6831:6831/udp"
      - "6832:6832/udp"
      - "5778:5778"
      - "16686:16686"
      - "14268:14268"
      - "14250:14250"
      - "9411:9411"
    environment:
      - COLLECTOR_ZIPKIN_HOST_PORT=:9411
    restart: unless-stopped
    networks:
      - monitoring

  # OpenTelemetry Collector
  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.86.0
    container_name: otel-collector
    command: ["--config=/etc/otel-collector-config.yml"]
    volumes:
      - ./otel/otel-collector-config.yml:/etc/otel-collector-config.yml
    ports:
      - "4317:4317"   # OTLP gRPC
      - "4318:4318"   # OTLP HTTP
      - "8888:8888"   # Metrics
      - "8889:8889"   # Prometheus exporter
    restart: unless-stopped
    networks:
      - monitoring
    depends_on:
      - jaeger
      - prometheus

volumes:
  prometheus_data:
  grafana_data:
  loki_data:

networks:
  monitoring:
    driver: bridge
</file>

<file path="microservices/python/common/nats_lib/tests/conftest.py">
"""Test configuration and fixtures for nats_lib tests."""
import asyncio
from typing import AsyncGenerator, Generator
from unittest.mock import AsyncMock, patch
from urllib.parse import urlparse

import pytest
from prometheus_client import REGISTRY

from nats_lib.config import CircuitBreakerConfig, NatsConfig


@pytest.fixture(autouse=True)
def clean_prometheus_registry():
    """Clean up Prometheus registry before each test."""
    collectors = list(REGISTRY._collector_to_names.keys())
    for collector in collectors:
        REGISTRY.unregister(collector)


@pytest.fixture
def event_loop() -> Generator[asyncio.AbstractEventLoop, None, None]:
    """Create an event loop for each test."""
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    yield loop
    loop.close()


@pytest.fixture
def circuit_breaker_config() -> CircuitBreakerConfig:
    """Create a test circuit breaker configuration."""
    return CircuitBreakerConfig(
        fail_max=3,
        reset_timeout=1,
        exclude_exceptions=(ValueError,)
    )


@pytest.fixture
def nats_config(circuit_breaker_config: CircuitBreakerConfig) -> NatsConfig:
    """Create a test NATS configuration."""
    return NatsConfig(
        urls="nats://localhost:4222",
        user="test_user",
        password="test_pass",
        stream_domain="test_domain",
        publish_breaker=circuit_breaker_config,
        request_breaker=circuit_breaker_config
    )


@pytest.fixture
async def mock_nats() -> AsyncGenerator[AsyncMock, None]:
    """Create a mock NATS client."""
    with patch("nats.connect") as mock_connect:
        # Create an async mock for the NATS client
        mock_client = AsyncMock()
        mock_client.is_closed = False
        mock_client.connected_url = urlparse("nats://localhost:4222")
        
        # Create an async mock for JetStream
        mock_js = AsyncMock()
        
        # Set up JetStream mock to return a coroutine
        async def mock_jetstream():
            return mock_js
        mock_client.jetstream = mock_jetstream
        
        # Set up connect mock to return a coroutine
        async def mock_connect_coro(*args, **kwargs):
            return mock_client
        mock_connect.side_effect = mock_connect_coro
        
        # Set up subscribe mock to store the callback
        def subscribe_side_effect(*args, **kwargs):
            mock_client._last_callback = kwargs.get('cb')
            return AsyncMock()
        mock_client.subscribe.side_effect = subscribe_side_effect
        
        # Set up publish mock to return a coroutine
        async def publish_coro(*args, **kwargs):
            return None
        mock_client.publish.side_effect = publish_coro
        
        # Set up request mock to return a coroutine
        async def request_coro(*args, **kwargs):
            return AsyncMock(data=b'{"response": "ok"}')
        mock_client.request.side_effect = request_coro
        
        yield mock_client
</file>

<file path="microservices/python/common/nats_lib/circuit_breaker.py">
from enum import Enum
from typing import Any, Callable

import pybreaker
from loguru import logger
from prometheus_client import REGISTRY, Counter, Gauge

from .config import CircuitBreakerConfig
from .exceptions import NatsCircuitOpenError


class CircuitBreakerMetrics:
    """Prometheus metrics for circuit breaker state and events."""

    def __init__(
        self,
        service_name: str,
        operation: str,
        subject_pattern: str
    ) -> None:
        """Initialize circuit breaker metrics."""
        # Sanitize metric name components
        service_name = service_name.replace("-", "_").replace(".", "_")
        operation = operation.replace("-", "_").replace(".", "_")
        subject_pattern = subject_pattern.replace("-", "_").replace(".", "_")
        subject_pattern = subject_pattern.replace("*", "all").replace(">", "all")

        # Create metric name prefix
        prefix = f"nats_circuit_breaker_{service_name}_{operation}_{subject_pattern}"

        # Create labels
        labels = {
            "service": service_name,
            "operation": operation,
            "subject_pattern": subject_pattern
        }

        # Initialize metrics
        self.state = Gauge(
            f"{prefix}_state",
            "Current state of the circuit breaker",
            labelnames=list(labels.keys())
        )
        self.total_failures = Counter(
            f"{prefix}_failures_total",
            "Total number of circuit breaker failures",
            labelnames=list(labels.keys())
        )
        self.total_successes = Counter(
            f"{prefix}_successes_total",
            "Total number of circuit breaker successes",
            labelnames=list(labels.keys())
        )
        self.total_state_changes = Counter(
            f"{prefix}_state_changes_total",
            "Total number of circuit breaker state changes",
            labelnames=list(labels.keys())
        )

        # Store labels for later use
        self._labels = labels

        # Register metrics with the default registry
        for metric in [self.state, self.total_failures, self.total_successes, self.total_state_changes]:
            if metric not in REGISTRY._names_to_collectors:
                REGISTRY.register(metric)

        # Set initial state
        self.state.labels(**self._labels).set(CircuitBreakerState.CLOSED.value)

    def record_state_change(self, new_state: CircuitBreakerState) -> None:
        """Record a state change in the circuit breaker."""
        self.state.labels(**self._labels).set(new_state.value)
        self.total_state_changes.labels(**self._labels).inc()
        logger.info(
            "Circuit breaker state changed",
            state=new_state.name,
            **self._labels
        )

    def record_failure(self) -> None:
        """Record a failure in the circuit breaker."""
        self.total_failures.labels(**self._labels).inc()
        logger.debug(
            "Circuit breaker failure",
            **self._labels
        )

    def record_success(self) -> None:
        """Record a success in the circuit breaker."""
        self.total_successes.labels(**self._labels).inc()
        logger.debug(
            "Circuit breaker success",
            **self._labels
        )


class CircuitBreakerState(Enum):
    """Circuit breaker states."""
    CLOSED = 0
    OPEN = 1
    HALF_OPEN = 2


class CircuitBreakerListener:
    """Listener for circuit breaker state changes."""

    def __init__(self, parent: "NatsCircuitBreaker") -> None:
        """Initialize the listener."""
        self.parent = parent
        self.metrics = CircuitBreakerMetrics(
            self.parent.service_name,
            self.parent.operation,
            self.parent.subject_pattern
        )

    def state_change(self, old_state: str, new_state: str) -> None:
        """Handle state change events."""
        state_map = {
            'closed': CircuitBreakerState.CLOSED,
            'open': CircuitBreakerState.OPEN,
            'half-open': CircuitBreakerState.HALF_OPEN
        }
        
        if new_state in state_map:
            self.metrics.record_state_change(state_map[new_state])
            
            if new_state == 'open':
                logger.error(
                    "Circuit breaker opened",
                    service=self.parent.service_name,
                    operation=self.parent.operation,
                    subject=self.parent.subject_pattern
                )
            elif new_state == 'closed':
                logger.info(
                    "Circuit breaker closed",
                    service=self.parent.service_name,
                    operation=self.parent.operation,
                    subject=self.parent.subject_pattern
                )

    def failure(self, exc: Exception) -> None:
        """Handle failure events."""
        self.metrics.record_failure()

    def success(self) -> None:
        """Handle success events."""
        self.metrics.record_success()


class NatsCircuitBreaker:
    """Circuit breaker for NATS operations."""

    def __init__(
        self,
        service_name: str,
        operation: str,
        subject_pattern: str,
        config: CircuitBreakerConfig
    ) -> None:
        """Initialize the circuit breaker."""
        self.service_name = service_name
        self.operation = operation
        self.subject_pattern = subject_pattern

        # Initialize metrics
        self.metrics = CircuitBreakerMetrics(
            service_name,
            operation,
            subject_pattern
        )

        # Create circuit breaker
        self.breaker = pybreaker.CircuitBreaker(
            fail_max=config.fail_max,
            reset_timeout=config.reset_timeout,
            exclude=config.exclude_exceptions,
            listeners=[CircuitBreakerListener(self)]
        )

    async def call(self, func: Callable, *args: Any, **kwargs: Any) -> Any:
        """Execute a function with circuit breaker protection."""
        try:
            result = await self.breaker.call(func, *args, **kwargs)
            self.metrics.record_success()
            return result
        except pybreaker.CircuitBreakerError:
            self.metrics.record_failure()
            raise NatsCircuitOpenError(
                f"Circuit breaker open for {self.operation} on {self.subject_pattern}"
            )
</file>

<file path="microservices/python/common/nats_lib/config.py">
import os
from dataclasses import dataclass
from typing import Any, Dict


@dataclass
class CircuitBreakerConfig:
    """Configuration for NATS circuit breaker."""
    fail_max: int = 5  # Maximum number of consecutive failures before opening
    reset_timeout: int = 60  # Time in seconds before attempting to half-open
    exclude_exceptions: tuple = ()  # Exceptions that should not count as failures
    
    @classmethod
    def from_env(cls) -> 'CircuitBreakerConfig':
        """Create config from environment variables."""
        return cls(
            fail_max=int(os.getenv('NATS_CIRCUIT_BREAKER_FAIL_MAX', '5')),
            reset_timeout=int(os.getenv('NATS_CIRCUIT_BREAKER_RESET_TIMEOUT', '60')),
        )

@dataclass
class NatsConfig:
    """Configuration for NATS client."""
    urls: str | list[str]
    user: str | None = None
    password: str | None = None
    token: str | None = None
    use_tls: bool = True
    stream_domain: str = "chimera"
    num_shards: int = 10
    
    # Circuit breaker configs for different operations
    publish_breaker: CircuitBreakerConfig = CircuitBreakerConfig()
    request_breaker: CircuitBreakerConfig = CircuitBreakerConfig()
    
    @classmethod
    def from_env(cls) -> 'NatsConfig':
        """Create config from environment variables."""
        urls = os.getenv('NATS_URL', 'nats://localhost:4222')
        if ',' in urls:
            urls = [url.strip() for url in urls.split(',')]
            
        return cls(
            urls=urls,
            user=os.getenv('NATS_USER'),
            password=os.getenv('NATS_PASSWORD'),
            token=os.getenv('NATS_TOKEN'),
            use_tls=os.getenv('NATS_USE_TLS', 'true').lower() == 'true',
            stream_domain=os.getenv('NATS_STREAM_DOMAIN', 'chimera'),
            num_shards=int(os.getenv('NATS_NUM_SHARDS', '10')),
            publish_breaker=CircuitBreakerConfig.from_env(),
            request_breaker=CircuitBreakerConfig.from_env(),
        )
</file>

<file path="microservices/python/common/nats_lib/exceptions.py">
class NatsLibError(Exception):
    """Base exception for NATS library errors."""
    pass


class NatsCircuitOpenError(NatsLibError):
    """Exception raised when a circuit breaker is open."""
    def __init__(self, operation: str, subject: str):
        self.operation = operation
        self.subject = subject
        super().__init__(
            f"Circuit breaker is open for {operation} operation on subject {subject}"
        )


class NatsConnectionError(NatsLibError):
    """Exception raised when there are NATS connection issues."""
    pass


class NatsTimeoutError(NatsLibError):
    """Exception raised when a NATS operation times out."""
    pass


class NatsOperationError(NatsLibError):
    """Exception raised when a NATS operation fails."""
    pass
</file>

<file path="microservices/python/common/nats_lib/nats_client.py">
import asyncio
import json
import os
from typing import Any, Callable, Dict, List, Optional, Union

import nats
from loguru import logger
from nats.aio.client import Client as NATS
from nats.aio.msg import Msg
from nats.js.api import ConsumerConfig
from nats.js.client import JetStreamContext

from .circuit_breaker import NatsCircuitBreaker
from .config import NatsConfig
from .exceptions import (NatsCircuitOpenError, NatsConnectionError,
                         NatsOperationError, NatsTimeoutError)


class EnhancedNatsClient:
    """Enhanced NATS client with circuit breaker support."""

    def __init__(
        self,
        config: NatsConfig,
        service_name: str,
    ):
        """Initialize the NATS client.
        
        Args:
            config: NATS configuration
            service_name: Name of the service using this client
        """
        self.config = config
        self.service_name = service_name
        
        # NATS connection objects
        self.nc: Optional[NATS] = None
        self.js: Optional[JetStreamContext] = None
        
        # Circuit breakers
        self.publish_breaker = NatsCircuitBreaker(
            name=f"{service_name}_publish",
            config=config.publish_breaker,
            service_name=service_name,
            operation="publish",
            subject_pattern="*"  # Wildcard for all subjects
        )
        
        self.request_breaker = NatsCircuitBreaker(
            name=f"{service_name}_request",
            config=config.request_breaker,
            service_name=service_name,
            operation="request",
            subject_pattern="*"  # Wildcard for all subjects
        )
        
        # Track subscriptions for reconnection
        self._subscriptions: List[Dict[str, Any]] = []

    async def connect(self) -> None:
        """Connect to the NATS server."""
        try:
            # Set up connection options
            options = {
                "servers": (
                    [self.config.urls]
                    if isinstance(self.config.urls, str)
                    else self.config.urls
                ),
                "reconnected_cb": self._on_reconnected,
                "disconnected_cb": self._on_disconnected,
                "error_cb": self._on_error,
                "closed_cb": self._on_closed,
                "max_reconnect_attempts": -1,  # Unlimited reconnect attempts
                "reconnect_time_wait": 2,  # Seconds between reconnect attempts
            }
            
            # Add authentication if provided
            if self.config.user and self.config.password:
                options["user"] = self.config.user
                options["password"] = self.config.password
            elif self.config.token:
                options["token"] = self.config.token
                
            # Add TLS if enabled
            if self.config.use_tls:
                tls_config = {
                    "ca_file": os.getenv(
                        "NATS_CA_FILE", "/etc/nats/certs/ca.crt"
                    ),
                    "cert_file": os.getenv(
                        "NATS_CERT_FILE", "/etc/nats/certs/client.crt"
                    ),
                    "key_file": os.getenv(
                        "NATS_KEY_FILE", "/etc/nats/certs/client.key"
                    ),
                }
                options["tls"] = tls_config
            
            # Connect to NATS
            self.nc = await nats.connect(**options)
            logger.info(f"Connected to NATS at {self.nc.connected_url.netloc}")
            
            # Initialize JetStream
            self.js = self.nc.jetstream(domain=self.config.stream_domain)
            logger.info(
                f"JetStream initialized with domain '{self.config.stream_domain}'"
            )
            
            # Restore subscriptions if any
            for sub in self._subscriptions:
                await self.subscribe(**sub)
            
        except Exception as e:
            logger.error(f"Failed to connect to NATS: {e}")
            self.nc = None
            self.js = None
            raise NatsConnectionError(f"Failed to connect to NATS: {e}")

    async def close(self) -> None:
        """Close the NATS connection."""
        if self.nc:
            try:
                await self.nc.drain()
                await self.nc.close()
            except Exception as e:
                logger.error(f"Error closing NATS connection: {e}")
            finally:
                self.nc = None
                self.js = None

    def is_connected(self) -> bool:
        """Check if connected to NATS."""
        return self.nc is not None and not self.nc.is_closed

    async def publish(
        self,
        subject: str,
        payload: Union[str, bytes, Dict[str, Any]],
        headers: Optional[Dict[str, str]] = None
    ) -> Optional[Dict[str, Any]]:
        """Publish a message with circuit breaker protection.
        
        Args:
            subject: Subject to publish to
            payload: Message payload
            headers: Optional message headers
            
        Returns:
            JetStream publish response if available
            
        Raises:
            NatsCircuitOpenError: If circuit breaker is open
            NatsConnectionError: If not connected to NATS
            NatsOperationError: If publish fails
        """
        if not self.is_connected():
            raise NatsConnectionError("Not connected to NATS")
            
        # Prepare payload
        if isinstance(payload, dict):
            payload = json.dumps(payload)
        if isinstance(payload, str):
            payload = payload.encode()
            
        try:
            # Publish with circuit breaker protection
            if self.js:
                result = await self.publish_breaker.call(
                    self.js.publish,
                    subject,
                    payload,
                    headers=headers
                )
                return {
                    "stream": result.stream,
                    "seq": result.seq,
                    "duplicate": result.duplicate,
                }
            else:
                await self.publish_breaker.call(
                    self.nc.publish,
                    subject,
                    payload,
                    headers=headers
                )
                return None
                
        except NatsCircuitOpenError:
            raise
        except Exception as e:
            logger.error(f"Failed to publish to {subject}: {e}")
            raise NatsOperationError(f"Failed to publish to {subject}: {e}")

    async def request(
        self,
        subject: str,
        payload: Union[str, bytes, Dict[str, Any]],
        timeout: float = 10.0,
        headers: Optional[Dict[str, str]] = None
    ) -> bytes:
        """Send a request with circuit breaker protection.
        
        Args:
            subject: Subject to send request to
            payload: Request payload
            timeout: Request timeout in seconds
            headers: Optional message headers
            
        Returns:
            Response data
            
        Raises:
            NatsCircuitOpenError: If circuit breaker is open
            NatsConnectionError: If not connected to NATS
            NatsTimeoutError: If request times out
            NatsOperationError: If request fails
        """
        if not self.is_connected():
            raise NatsConnectionError("Not connected to NATS")
            
        # Prepare payload
        if isinstance(payload, dict):
            payload = json.dumps(payload)
        if isinstance(payload, str):
            payload = payload.encode()
            
        try:
            # Send request with circuit breaker protection
            msg = await self.request_breaker.call(
                self.nc.request,
                subject,
                payload,
                timeout=timeout,
                headers=headers
            )
            return msg.data
            
        except NatsCircuitOpenError:
            raise
        except asyncio.TimeoutError:
            logger.warning(f"Request to {subject} timed out after {timeout}s")
            raise NatsTimeoutError(f"Request to {subject} timed out")
        except Exception as e:
            logger.error(f"Failed to send request to {subject}: {e}")
            raise NatsOperationError(f"Failed to send request to {subject}: {e}")

    async def subscribe(
        self,
        subject: str,
        callback: Callable[[Msg], Any],
        queue: Optional[str] = None,
        durable: bool = True,
        max_in_flight: int = 100,
    ) -> None:
        """Subscribe to a subject.
        
        Args:
            subject: Subject to subscribe to
            callback: Message handler function
            queue: Optional queue group
            durable: Whether to use durable subscription
            max_in_flight: Maximum in-flight messages
            
        Raises:
            NatsConnectionError: If not connected to NATS
            NatsOperationError: If subscription fails
        """
        if not self.is_connected():
            raise NatsConnectionError("Not connected to NATS")
            
        try:
            # Skip if already subscribed
            for sub in self._subscriptions:
                if (
                    sub["subject"] == subject
                    and sub["queue"] == queue
                    and sub["callback"] == callback
                ):
                    logger.debug(f"Already subscribed to {subject}")
                    return
            
            # Store subscription for reconnection
            sub_info = {
                "subject": subject,
                "callback": callback,
                "queue": queue,
                "durable": durable,
                "max_in_flight": max_in_flight,
            }
            
            if self.js:
                # Create consumer name
                consumer = (
                    f"{self.service_name}-{subject.replace('.', '-')}"
                    + (f"-{queue}" if queue else "")
                )
                
                # Subscribe with JetStream
                await self.js.subscribe(
                    subject,
                    queue=queue,
                    cb=callback,
                    stream=self.config.stream_domain,
                    config=ConsumerConfig(
                        durable_name=consumer if durable else None,
                        ack_policy="explicit",
                        ack_wait=30,
                        max_ack_pending=max_in_flight,
                        deliver_policy="all",
                    ),
                )
                
                logger.info(
                    f"Subscribed to {subject} with JetStream"
                    + (f" (queue: {queue})" if queue else "")
                )
            else:
                # Standard NATS subscription
                await self.nc.subscribe(
                    subject,
                    queue=queue,
                    cb=callback,
                )
                
                logger.info(
                    f"Subscribed to {subject}"
                    + (f" (queue: {queue})" if queue else "")
                )
            
            self._subscriptions.append(sub_info)
            
        except Exception as e:
            logger.error(f"Failed to subscribe to {subject}: {e}")
            raise NatsOperationError(f"Failed to subscribe to {subject}: {e}")

    def _on_reconnected(self) -> None:
        """Handle NATS reconnection."""
        logger.info("Reconnected to NATS")

    def _on_disconnected(self) -> None:
        """Handle NATS disconnection."""
        logger.warning("Disconnected from NATS")

    def _on_error(self, e: Exception) -> None:
        """Handle NATS errors."""
        logger.error(f"NATS error: {e}")

    def _on_closed(self) -> None:
        """Handle NATS connection closure."""
        logger.info("NATS connection closed")
        self.nc = None
        self.js = None
</file>

<file path="microservices/python/common/nats_lib/README.md">
# NATS Library with Circuit Breaker Support

This library provides an enhanced NATS client implementation with circuit breaker support for Python microservices.

## Features

- Circuit breaker pattern implementation using `pybreaker`
- Prometheus metrics for monitoring circuit breaker states and events
- Support for both standard NATS and JetStream operations
- Configurable circuit breaker parameters via environment variables
- TLS support with configurable certificates
- Automatic reconnection handling
- Subscription persistence across reconnections
- Comprehensive error handling and logging

## Installation

Add the following to your `requirements.txt`:

```
-e ../common/nats_lib
```

## Configuration

The library can be configured using environment variables:

### Circuit Breaker Configuration

- `NATS_CIRCUIT_BREAKER_FAIL_MAX`: Maximum number of consecutive failures before opening (default: 5)
- `NATS_CIRCUIT_BREAKER_RESET_TIMEOUT`: Time in seconds before attempting to half-open (default: 60)

### NATS Configuration

- `NATS_URL`: NATS server URL(s), comma-separated for multiple servers
- `NATS_USER`: NATS username for authentication
- `NATS_PASSWORD`: NATS password for authentication
- `NATS_TOKEN`: Alternative token-based authentication
- `NATS_USE_TLS`: Whether to use TLS (default: true)
- `NATS_STREAM_DOMAIN`: JetStream domain (default: "chimera")
- `NATS_NUM_SHARDS`: Number of shards for high-throughput streams (default: 10)

### TLS Configuration

- `NATS_CA_FILE`: Path to CA certificate (default: "/etc/nats/certs/ca.crt")
- `NATS_CERT_FILE`: Path to client certificate (default: "/etc/nats/certs/client.crt")
- `NATS_KEY_FILE`: Path to client key (default: "/etc/nats/certs/client.key")

## Usage

```python
from nats_lib import EnhancedNatsClient, NatsConfig

# Create configuration
config = NatsConfig(
    urls="nats://localhost:4222",
    user="myuser",
    password="mypass",
    stream_domain="myapp"
)

# Initialize client
client = EnhancedNatsClient(
    config=config,
    service_name="my-service"
)

# Connect to NATS
await client.connect()

# Publish with circuit breaker protection
try:
    await client.publish("my.subject", {"key": "value"})
except NatsCircuitOpenError:
    # Handle circuit breaker open state
    pass

# Subscribe to subjects
await client.subscribe(
    subject="my.subject",
    callback=my_message_handler,
    queue="my-queue",
    durable=True
)

# Send request with circuit breaker protection
try:
    response = await client.request("my.service", "request payload")
except NatsCircuitOpenError:
    # Handle circuit breaker open state
    pass
except NatsTimeoutError:
    # Handle request timeout
    pass

# Close connection
await client.close()
```

## Prometheus Metrics

The library exports the following Prometheus metrics:

- `nats_circuit_breaker_state`: Circuit breaker state (0=closed, 1=open, 2=half-open)
- `nats_circuit_breaker_opens_total`: Number of times circuit breaker opened
- `nats_circuit_breaker_rejected_calls_total`: Number of calls rejected due to open circuit

Labels:

- `service`: Name of the service
- `operation`: Operation type (publish/request)
- `subject_pattern`: NATS subject pattern

## Error Handling

The library provides custom exceptions for different error scenarios:

- `NatsCircuitOpenError`: Raised when a circuit breaker is open
- `NatsConnectionError`: Raised for connection issues
- `NatsTimeoutError`: Raised when a request times out
- `NatsOperationError`: Raised for other NATS operation failures

## Development

### Running Tests

```bash
python -m pytest tests/
```

### Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests for your changes
5. Run the test suite
6. Create a pull request
</file>

<file path="microservices/python/common/nats_lib/requirements.txt">
pybreaker==1.0.1
prometheus-client==0.17.1
loguru==0.7.2
nats-py==2.6.0
</file>

<file path="microservices/python/common/nats_lib/setup.py">
"""Setup configuration for nats_lib package."""
from setuptools import find_packages, setup

setup(
    name="nats_lib",
    version="0.1.0",
    description="Enhanced NATS client with circuit breaker support",
    author="SolnAI",
    packages=find_packages(exclude=["tests*"]),
    python_requires=">=3.8",
    install_requires=[
        "nats-py==2.6.0",
        "pybreaker==1.0.1",
        "prometheus-client==0.17.1",
        "loguru==0.7.2",
    ],
    extras_require={
        "test": [
            "pytest==8.0.0",
            "pytest-asyncio==0.23.5",
            "pytest-cov==4.1.0",
            "docker==7.0.0",
        ],
    },
)
</file>

<file path="microservices/python/gpu-monitoring-agent/app.py">
#!/usr/bin/env python3
"""
GPU Monitoring Agent for Chimera
Collects GPU metrics and exposes them for Prometheus scraping
"""
import time
import os
import socket
import threading
from flask import Flask, Response
import prometheus_client
from prometheus_client import Gauge, Counter, Histogram
import pynvml
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("gpu-monitor")

# Initialize Flask app for metrics endpoint
app = Flask(__name__)

# Initialize NVML
try:
    pynvml.nvmlInit()
    logger.info("NVML initialized successfully")
except Exception as e:
    logger.error(f"Failed to initialize NVML: {e}")
    raise

# Get hostname for labels
HOSTNAME = socket.gethostname()
NODE_NAME = os.environ.get("NODE_NAME", HOSTNAME)

# Prometheus metrics
GPU_UTIL = Gauge('chimera_gpu_utilization_percent', 
                'GPU Utilization Percentage', 
                ['gpu_id', 'gpu_type', 'node'])

GPU_MEMORY_USED = Gauge('chimera_gpu_memory_used_bytes', 
                       'GPU Memory Used in Bytes', 
                       ['gpu_id', 'gpu_type', 'node'])

GPU_MEMORY_TOTAL = Gauge('chimera_gpu_memory_total_bytes', 
                         'GPU Total Memory in Bytes', 
                         ['gpu_id', 'gpu_type', 'node'])

GPU_POWER_USAGE = Gauge('chimera_gpu_power_usage_watts', 
                        'GPU Power Usage in Watts', 
                        ['gpu_id', 'gpu_type', 'node'])

GPU_TEMPERATURE = Gauge('chimera_gpu_temperature_celsius', 
                        'GPU Temperature in Celsius', 
                        ['gpu_id', 'gpu_type', 'node'])

# Histogram to track GPU utilization distribution
GPU_UTIL_HISTOGRAM = Histogram('chimera_gpu_utilization_distribution', 
                              'Distribution of GPU utilization values',
                              ['gpu_id', 'gpu_type', 'node'],
                              buckets=[0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100])

# Counter for monitoring events
MONITORING_CYCLES = Counter('chimera_gpu_monitoring_cycles_total', 
                           'Total number of GPU monitoring cycles')

MONITORING_ERRORS = Counter('chimera_gpu_monitoring_errors_total', 
                           'Total number of GPU monitoring errors',
                           ['error_type'])

def collect_gpu_metrics():
    """Collect metrics from all available GPUs and update Prometheus metrics"""
    try:
        device_count = pynvml.nvmlDeviceGetCount()
        logger.debug(f"Found {device_count} GPU devices")
        
        for i in range(device_count):
            try:
                handle = pynvml.nvmlDeviceGetHandleByIndex(i)
                
                # Get GPU name/type
                try:
                    name = pynvml.nvmlDeviceGetName(handle)
                except Exception as e:
                    logger.warning(f"Failed to get name for GPU {i}: {e}")
                    name = f"unknown-gpu-{i}"
                
                # Get utilization rates (GPU & memory)
                try:
                    util = pynvml.nvmlDeviceGetUtilizationRates(handle)
                    GPU_UTIL.labels(gpu_id=str(i), gpu_type=name, node=NODE_NAME).set(util.gpu)
                    GPU_UTIL_HISTOGRAM.labels(gpu_id=str(i), gpu_type=name, node=NODE_NAME).observe(util.gpu)
                except Exception as e:
                    logger.warning(f"Failed to get utilization for GPU {i}: {e}")
                    MONITORING_ERRORS.labels(error_type="utilization").inc()
                
                # Get memory info
                try:
                    mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
                    GPU_MEMORY_USED.labels(gpu_id=str(i), gpu_type=name, node=NODE_NAME).set(mem_info.used)
                    GPU_MEMORY_TOTAL.labels(gpu_id=str(i), gpu_type=name, node=NODE_NAME).set(mem_info.total)
                except Exception as e:
                    logger.warning(f"Failed to get memory info for GPU {i}: {e}")
                    MONITORING_ERRORS.labels(error_type="memory").inc()
                
                # Get power usage
                try:
                    power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # Convert from mW to W
                    GPU_POWER_USAGE.labels(gpu_id=str(i), gpu_type=name, node=NODE_NAME).set(power)
                except Exception as e:
                    logger.debug(f"Failed to get power usage for GPU {i}: {e}")
                    # Not all GPUs support power measurement, so this is a debug log
                
                # Get temperature
                try:
                    temp = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)
                    GPU_TEMPERATURE.labels(gpu_id=str(i), gpu_type=name, node=NODE_NAME).set(temp)
                except Exception as e:
                    logger.warning(f"Failed to get temperature for GPU {i}: {e}")
                    MONITORING_ERRORS.labels(error_type="temperature").inc()
                
            except Exception as e:
                logger.error(f"Error processing GPU {i}: {e}")
                MONITORING_ERRORS.labels(error_type="device_access").inc()
        
        # Increment the monitoring cycle counter
        MONITORING_CYCLES.inc()
        
    except Exception as e:
        logger.error(f"Failed to collect GPU metrics: {e}")
        MONITORING_ERRORS.labels(error_type="collection").inc()

def monitoring_loop():
    """Background thread function to continuously collect GPU metrics"""
    interval = int(os.environ.get("MONITORING_INTERVAL_SECONDS", "15"))
    logger.info(f"Starting GPU monitoring loop with interval {interval} seconds")
    
    while True:
        try:
            collect_gpu_metrics()
        except Exception as e:
            logger.error(f"Error in monitoring loop: {e}")
            MONITORING_ERRORS.labels(error_type="loop").inc()
        
        time.sleep(interval)

@app.route('/metrics')
def metrics():
    """Endpoint for Prometheus to scrape metrics"""
    return Response(prometheus_client.generate_latest(), mimetype="text/plain")

@app.route('/health')
def health():
    """Health check endpoint"""
    return {"status": "healthy", "timestamp": time.time()}

if __name__ == '__main__':
    # Start the monitoring in a background thread
    monitoring_thread = threading.Thread(target=monitoring_loop, daemon=True)
    monitoring_thread.start()
    
    # Start the Flask app to expose metrics
    port = int(os.environ.get("PORT", "8000"))
    logger.info(f"Starting metrics server on port {port}")
    app.run(host='0.0.0.0', port=port)
</file>

<file path="microservices/python/gpu-monitoring-agent/Dockerfile">
FROM nvidia/cuda:11.8.0-base-ubuntu22.04

# Install system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3 \
    python3-pip \
    python3-dev \
    && rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /app

# Install Python dependencies
COPY requirements.txt .
RUN pip3 install --no-cache-dir -r requirements.txt

# Copy application code
COPY app.py .

# Expose metrics port
EXPOSE 8000

# Run the application
CMD ["python3", "app.py"]
</file>

<file path="microservices/python/gpu-monitoring-agent/README.md">
# GPU Monitoring Agent

A lightweight monitoring service for NVIDIA GPUs that collects metrics and exposes them for Prometheus scraping.

## Features

- Real-time monitoring of NVIDIA GPU metrics:
  - Utilization (percentage)
  - Memory usage (used and total)
  - Temperature (Celsius)
  - Power usage (Watts)
- Prometheus-compatible metrics endpoint
- Auto-discovery of all available GPUs on the host
- Health check endpoint
- Configurable monitoring interval

## Metrics Exposed

| Metric Name | Description | Labels |
|-------------|-------------|--------|
| `chimera_gpu_utilization_percent` | GPU utilization percentage | `gpu_id`, `gpu_type`, `node` |
| `chimera_gpu_memory_used_bytes` | GPU memory used in bytes | `gpu_id`, `gpu_type`, `node` |
| `chimera_gpu_memory_total_bytes` | Total GPU memory in bytes | `gpu_id`, `gpu_type`, `node` |
| `chimera_gpu_temperature_celsius` | GPU temperature in Celsius | `gpu_id`, `gpu_type`, `node` |
| `chimera_gpu_power_usage_watts` | GPU power usage in watts | `gpu_id`, `gpu_type`, `node` |
| `chimera_gpu_monitoring_cycles_total` | Counter for monitoring cycles | - |
| `chimera_gpu_monitoring_errors_total` | Counter for monitoring errors | `error_type` |

## Requirements

- NVIDIA GPU with compatible drivers
- Python 3.8+
- Dependencies listed in `requirements.txt`

## Installation

```bash
pip install -r requirements.txt
```

## Configuration

The service is configured using environment variables:

| Variable | Description | Default |
|----------|-------------|---------|
| `MONITORING_INTERVAL_SECONDS` | Interval between metric collections | `15` |
| `PORT` | Port to expose metrics on | `8000` |
| `NODE_NAME` | Node name for metrics | hostname |

## Usage

### Running Locally

```bash
python app.py
```

### Using Docker

```bash
docker build -t gpu-monitoring-agent .
docker run --runtime=nvidia -p 8000:8000 gpu-monitoring-agent
```

### Health Check

Access the health endpoint at: http://localhost:8000/health

### Prometheus Metrics

Access the metrics endpoint at: http://localhost:8000/metrics

## Prometheus Configuration

Add the following to your Prometheus configuration to scrape metrics:

```yaml
scrape_configs:
  - job_name: 'gpu-monitoring'
    scrape_interval: 15s
    static_configs:
      - targets: ['gpu-monitoring-agent:8000']
```

## Kubernetes Deployment

When deploying in Kubernetes, ensure:

1. The pod has access to NVIDIA GPUs
2. Node affinity is set to target nodes with GPUs
3. The NVIDIA runtime is configured

Example node affinity:

```yaml
nodeAffinity:
  requiredDuringSchedulingIgnoredDuringExecution:
    nodeSelectorTerms:
    - matchExpressions:
      - key: nvidia.com/gpu
        operator: Exists
```
</file>

<file path="microservices/python/gpu-monitoring-agent/requirements.txt">
flask==2.3.3
prometheus-client==0.17.1
nvidia-ml-py==12.535.77
gunicorn==21.2.0
</file>

<file path="microservices/python/knowledge-graph/app/config.py">
from functools import lru_cache
from typing import Dict, Any, Optional
import os
from pydantic import BaseModel, Field
from dotenv import load_dotenv

# Load environment variables from .env file if it exists
load_dotenv()

class Settings(BaseModel):
    """Application settings loaded from environment variables."""
    
    # Service configuration
    service_name: str = Field(default="knowledge-graph")
    environment: str = Field(default=os.getenv("ENVIRONMENT", "development"))
    log_level: str = Field(default=os.getenv("LOG_LEVEL", "INFO"))
    
    # API configuration
    host: str = Field(default=os.getenv("HOST", "0.0.0.0"))
    port: int = Field(default=int(os.getenv("PORT", "8001")))
    
    # Neo4j configuration
    neo4j_uri: str = Field(default=os.getenv("NEO4J_URI", "neo4j://localhost:7687"))
    neo4j_user: str = Field(default=os.getenv("NEO4J_USER", "neo4j"))
    neo4j_password: str = Field(default=os.getenv("NEO4J_PASSWORD", "password"))
    initialize_schema: bool = Field(default=os.getenv("INITIALIZE_SCHEMA", "true").lower() == "true")
    
    # NATS configuration
    nats_url: str = Field(default=os.getenv("NATS_URL", "nats://localhost:4222"))
    nats_user: Optional[str] = Field(default=os.getenv("NATS_USER"))
    nats_password: Optional[str] = Field(default=os.getenv("NATS_PASSWORD"))
    
    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"
        case_sensitive = False

@lru_cache()
def get_settings() -> Settings:
    """Create cached settings instance."""
    return Settings()
</file>

<file path="microservices/python/knowledge-graph/app/main.py">
from contextlib import asynccontextmanager
import asyncio
import json
import os
from typing import Dict, List, Optional, Any, Union

from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from loguru import logger

from app.config import Settings, get_settings
from app.models import (
    EntityCreate,
    RelationshipCreate,
    GraphQueryRequest,
    EntityResponse,
    RelationshipResponse,
    GraphQueryResponse,
    HealthResponse,
    BatchEntityOperation,
    BatchRelationshipOperation,
    OperationResponse,
)
from app.neo4j_client import Neo4jClient
from app.nats_client import NatsClient

# Global clients
neo4j_client: Optional[Neo4jClient] = None
nats_client: Optional[NatsClient] = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application startup and shutdown events."""
    global neo4j_client, nats_client
    settings = get_settings()
    
    # Initialize Neo4j client
    logger.info(f"Connecting to Neo4j at {settings.neo4j_uri}")
    neo4j_client = Neo4jClient(
        uri=settings.neo4j_uri,
        user=settings.neo4j_user,
        password=settings.neo4j_password,
    )
    await neo4j_client.connect()
    
    # Initialize schema if configured
    if settings.initialize_schema:
        await neo4j_client.initialize_schema()
    
    # Initialize NATS client
    logger.info(f"Connecting to NATS at {settings.nats_url}")
    nats_client = NatsClient(
        nats_url=settings.nats_url,
        user=settings.nats_user,
        password=settings.nats_password,
    )
    await nats_client.connect()
    
    # Initialize NATS streams and subscriptions
    if nats_client.js:
        try:
            # Set up subscriptions for NLP enriched data
            async def process_enriched_data(msg):
                try:
                    data = json.loads(msg.data.decode())
                    logger.info(f"Processing enriched data: {data.get('id', 'unknown')}")
                    
                    # Extract entities and relationships from enriched data
                    if entities := data.get("nlp_enrichment", {}).get("entities", []):
                        await process_entities_from_enriched_data(data, entities)
                    
                    await msg.ack()
                except Exception as e:
                    logger.error(f"Error processing enriched message: {e}")
                    await msg.nak(delay=5)
            
            # Subscribe to NLP enriched data
            await nats_client.subscribe(
                "nlp.enriched.*", 
                queue="knowledge-graph-processors",
                callback=process_enriched_data
            )
            
            logger.info("NATS subscriptions configured")
        except Exception as e:
            logger.error(f"Failed to configure NATS streams/subscriptions: {e}")
    
    # Start background processors
    asyncio.create_task(health_check_loop())
    
    logger.info("Knowledge Graph service started")
    
    yield
    
    # Cleanup
    if nats_client:
        await nats_client.close()
        logger.info("NATS connection closed")
    
    if neo4j_client:
        await neo4j_client.close()
        logger.info("Neo4j connection closed")
    
    logger.info("Knowledge Graph service shutdown")

app = FastAPI(
    title="Chimera Knowledge Graph Service",
    description="Manages the Knowledge Graph using Neo4j",
    version="0.1.0",
    lifespan=lifespan,
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

async def health_check_loop():
    """Background task to check system health periodically."""
    while True:
        try:
            neo4j_healthy = neo4j_client and await neo4j_client.is_healthy()
            nats_healthy = nats_client and nats_client.is_connected()
            
            if not neo4j_healthy:
                logger.warning("Neo4j connection unhealthy")
                if neo4j_client:
                    try:
                        await neo4j_client.connect()
                    except Exception as e:
                        logger.error(f"Failed to reconnect to Neo4j: {e}")
            
            if not nats_healthy:
                logger.warning("NATS connection unhealthy")
                if nats_client:
                    try:
                        await nats_client.connect()
                    except Exception as e:
                        logger.error(f"Failed to reconnect to NATS: {e}")
                        
        except Exception as e:
            logger.error(f"Error in health check loop: {e}")
            
        await asyncio.sleep(30)  # Check every 30 seconds

async def process_entities_from_enriched_data(data: Dict[str, Any], entities: List[Dict[str, Any]]):
    """Process entities from enriched data.
    
    Args:
        data: The enriched data
        entities: List of extracted entities
    """
    if not neo4j_client:
        logger.error("Neo4j client not initialized")
        return
    
    try:
        # Extract source information
        source_id = data.get("id", "unknown")
        source_type = data.get("content_type", "unknown")
        source_title = data.get("payload", {}).get("title", "Untitled")
        
        # Create source node (e.g., Paper, NewsArticle)
        source_props = {
            "id": source_id,
            "title": source_title,
            "source": data.get("source", "unknown"),
            "timestamp": data.get("timestamp"),
        }
        
        # Determine label based on content_type
        source_label = "Document"
        if "research_paper" in source_type:
            source_label = "Paper"
        elif "news_article" in source_type:
            source_label = "NewsArticle"
        elif "repository" in source_type:
            source_label = "Repository"
            
        # Create the source node
        source_node_id = await neo4j_client.create_node(
            labels=[source_label],
            properties=source_props
        )
        
        # Process each entity
        for entity in entities:
            entity_type = entity.get("type", "UNKNOWN")
            entity_text = entity.get("text", "").strip()
            
            if not entity_text:
                continue
                
            # Create entity node if it doesn't exist
            entity_props = {
                "name": entity_text,
                "type": entity_type,
                "confidence": entity.get("confidence", 1.0),
            }
            
            entity_node_id = await neo4j_client.merge_node(
                labels=["Entity"],
                match_properties={"name": entity_text, "type": entity_type},
                additional_properties=entity_props
            )
            
            # Create relationship between source and entity
            rel_props = {
                "confidence": entity.get("confidence", 1.0),
            }
            
            await neo4j_client.create_relationship(
                start_node_id=source_node_id,
                end_node_id=entity_node_id,
                rel_type="MENTIONS",
                properties=rel_props
            )
            
        logger.info(f"Processed {len(entities)} entities for {source_id}")
        
    except Exception as e:
        logger.error(f"Error processing entities: {e}")
        raise

@app.get("/health")
async def health_check() -> HealthResponse:
    """Health check endpoint."""
    neo4j_healthy = neo4j_client and await neo4j_client.is_healthy()
    nats_healthy = nats_client and nats_client.is_connected()
    
    return HealthResponse(
        service="knowledge-graph",
        status="healthy" if (neo4j_healthy and nats_healthy) else "degraded",
        components={
            "neo4j": "connected" if neo4j_healthy else "disconnected",
            "nats": "connected" if nats_healthy else "disconnected",
        },
        version="0.1.0"
    )

@app.post("/entities", response_model=EntityResponse)
async def create_entity(entity: EntityCreate) -> EntityResponse:
    """Create a new entity in the knowledge graph."""
    if not neo4j_client:
        raise HTTPException(500, "Neo4j client not initialized")
    
    try:
        # Create entity node
        node_id = await neo4j_client.create_node(
            labels=[entity.entity_type],
            properties=entity.properties.dict()
        )
        
        return EntityResponse(
            id=node_id,
            entity_type=entity.entity_type,
            properties=entity.properties.dict(),
            status="created"
        )
    
    except Exception as e:
        logger.error(f"Error creating entity: {e}")
        raise HTTPException(500, f"Failed to create entity: {str(e)}")

@app.post("/relationships", response_model=RelationshipResponse)
async def create_relationship(relationship: RelationshipCreate) -> RelationshipResponse:
    """Create a new relationship between entities in the knowledge graph."""
    if not neo4j_client:
        raise HTTPException(500, "Neo4j client not initialized")
    
    try:
        # Find start and end nodes
        start_node = await neo4j_client.find_node_by_properties(
            labels=relationship.start_node.entity_type.split("|"),
            properties=relationship.start_node.properties.dict()
        )
        
        end_node = await neo4j_client.find_node_by_properties(
            labels=relationship.end_node.entity_type.split("|"),
            properties=relationship.end_node.properties.dict()
        )
        
        if not start_node:
            raise HTTPException(404, "Start node not found")
            
        if not end_node:
            raise HTTPException(404, "End node not found")
        
        # Create relationship
        rel_id = await neo4j_client.create_relationship(
            start_node_id=start_node["id"],
            end_node_id=end_node["id"],
            rel_type=relationship.relationship_type,
            properties=relationship.properties.dict() if relationship.properties else {}
        )
        
        return RelationshipResponse(
            id=rel_id,
            start_node_id=start_node["id"],
            end_node_id=end_node["id"],
            relationship_type=relationship.relationship_type,
            properties=relationship.properties.dict() if relationship.properties else {},
            status="created"
        )
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error creating relationship: {e}")
        raise HTTPException(500, f"Failed to create relationship: {str(e)}")

@app.post("/query", response_model=GraphQueryResponse)
async def query_graph(query: GraphQueryRequest) -> GraphQueryResponse:
    """Query the knowledge graph."""
    if not neo4j_client:
        raise HTTPException(500, "Neo4j client not initialized")
    
    try:
        # Execute Cypher query
        result = await neo4j_client.execute_query(
            query=query.cypher,
            params=query.parameters if query.parameters else {}
        )
        
        return GraphQueryResponse(
            result=result,
            status="success"
        )
    
    except Exception as e:
        logger.error(f"Error executing query: {e}")
        raise HTTPException(500, f"Query execution failed: {str(e)}")

@app.post("/entities/batch", response_model=OperationResponse)
async def batch_entity_operations(operations: BatchEntityOperation) -> OperationResponse:
    """Perform batch operations on entities."""
    if not neo4j_client:
        raise HTTPException(500, "Neo4j client not initialized")
    
    try:
        successful = 0
        failed = 0
        
        # Process create operations
        for entity in operations.create or []:
            try:
                await neo4j_client.create_node(
                    labels=[entity.entity_type],
                    properties=entity.properties.dict()
                )
                successful += 1
            except Exception as e:
                logger.error(f"Error in batch create: {e}")
                failed += 1
        
        # Process update operations
        for update in operations.update or []:
            try:
                # Find nodes to update
                nodes = await neo4j_client.find_nodes_by_properties(
                    labels=update.entity_type.split("|"),
                    properties=update.match_properties.dict()
                )
                
                # Update each matching node
                for node in nodes:
                    await neo4j_client.update_node(
                        node_id=node["id"],
                        properties=update.new_properties.dict()
                    )
                    successful += 1
            except Exception as e:
                logger.error(f"Error in batch update: {e}")
                failed += 1
        
        # Process delete operations
        for delete in operations.delete or []:
            try:
                # Find nodes to delete
                nodes = await neo4j_client.find_nodes_by_properties(
                    labels=delete.entity_type.split("|"),
                    properties=delete.properties.dict()
                )
                
                # Delete each matching node
                for node in nodes:
                    await neo4j_client.delete_node(node_id=node["id"])
                    successful += 1
            except Exception as e:
                logger.error(f"Error in batch delete: {e}")
                failed += 1
        
        return OperationResponse(
            successful=successful,
            failed=failed,
            status="completed"
        )
    
    except Exception as e:
        logger.error(f"Error in batch operation: {e}")
        raise HTTPException(500, f"Batch operation failed: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    
    port = int(os.getenv("PORT", "8001"))
    host = os.getenv("HOST", "0.0.0.0")
    
    uvicorn.run("app.main:app", host=host, port=port, reload=True)
</file>

<file path="microservices/python/knowledge-graph/app/models.py">
from typing import Dict, List, Optional, Any, Union
from pydantic import BaseModel, Field
from uuid import uuid4
from datetime import datetime

class EntityProperties(BaseModel):
    """Base model for entity properties."""
    class Config:
        extra = "allow"  # Allow arbitrary additional fields

class NodeReference(BaseModel):
    """Reference to a node in the graph."""
    entity_type: str
    properties: EntityProperties

class EntityCreate(BaseModel):
    """Request model for creating an entity."""
    entity_type: str = Field(..., description="Type of entity (node label)")
    properties: EntityProperties = Field(..., description="Entity properties")

class EntityUpdate(BaseModel):
    """Request model for updating an entity."""
    entity_type: str = Field(..., description="Type of entity (node label)")
    match_properties: EntityProperties = Field(..., description="Properties to match existing entity")
    new_properties: EntityProperties = Field(..., description="New properties to set")

class EntityDelete(BaseModel):
    """Request model for deleting an entity."""
    entity_type: str = Field(..., description="Type of entity (node label)")
    properties: EntityProperties = Field(..., description="Properties to match entity to delete")

class RelationshipProperties(BaseModel):
    """Base model for relationship properties."""
    class Config:
        extra = "allow"  # Allow arbitrary additional fields

class RelationshipCreate(BaseModel):
    """Request model for creating a relationship."""
    start_node: NodeReference = Field(..., description="Start node reference")
    end_node: NodeReference = Field(..., description="End node reference")
    relationship_type: str = Field(..., description="Type of relationship")
    properties: Optional[RelationshipProperties] = Field(None, description="Relationship properties")

class RelationshipUpdate(BaseModel):
    """Request model for updating a relationship."""
    start_node: NodeReference = Field(..., description="Start node reference")
    end_node: NodeReference = Field(..., description="End node reference")
    relationship_type: str = Field(..., description="Type of relationship to match")
    new_properties: RelationshipProperties = Field(..., description="New properties to set")

class RelationshipDelete(BaseModel):
    """Request model for deleting relationships."""
    start_node: Optional[NodeReference] = Field(None, description="Optional start node reference")
    end_node: Optional[NodeReference] = Field(None, description="Optional end node reference")
    relationship_type: Optional[str] = Field(None, description="Optional relationship type to match")

class BatchEntityOperation(BaseModel):
    """Batch operation for entities."""
    create: Optional[List[EntityCreate]] = Field(None, description="Entities to create")
    update: Optional[List[EntityUpdate]] = Field(None, description="Entities to update")
    delete: Optional[List[EntityDelete]] = Field(None, description="Entities to delete")

class BatchRelationshipOperation(BaseModel):
    """Batch operation for relationships."""
    create: Optional[List[RelationshipCreate]] = Field(None, description="Relationships to create")
    update: Optional[List[RelationshipUpdate]] = Field(None, description="Relationships to update")
    delete: Optional[List[RelationshipDelete]] = Field(None, description="Relationships to delete")

class GraphQueryRequest(BaseModel):
    """Request model for querying the graph."""
    cypher: str = Field(..., description="Cypher query string")
    parameters: Optional[Dict[str, Any]] = Field(None, description="Query parameters")

class EntityResponse(BaseModel):
    """Response model for entity operations."""
    id: str = Field(..., description="Internal ID of the entity")
    entity_type: str = Field(..., description="Type of entity")
    properties: Dict[str, Any] = Field(..., description="Entity properties")
    status: str = Field(..., description="Operation status")

class RelationshipResponse(BaseModel):
    """Response model for relationship operations."""
    id: str = Field(..., description="Internal ID of the relationship")
    start_node_id: str = Field(..., description="ID of the start node")
    end_node_id: str = Field(..., description="ID of the end node")
    relationship_type: str = Field(..., description="Type of relationship")
    properties: Dict[str, Any] = Field(..., description="Relationship properties")
    status: str = Field(..., description="Operation status")

class GraphQueryResponse(BaseModel):
    """Response model for graph queries."""
    result: List[Dict[str, Any]] = Field(..., description="Query result")
    status: str = Field(..., description="Query status")

class OperationResponse(BaseModel):
    """Response model for batch operations."""
    successful: int = Field(..., description="Number of successful operations")
    failed: int = Field(..., description="Number of failed operations")
    status: str = Field(..., description="Operation status")

class HealthResponse(BaseModel):
    """Health check response model."""
    service: str = Field(..., description="Service name")
    status: str = Field(..., description="Service status (healthy, degraded, unhealthy)")
    components: Dict[str, str] = Field(default_factory=dict, description="Status of individual components")
    version: str = Field(..., description="Service version")
</file>

<file path="microservices/python/knowledge-graph/app/neo4j_client.py">
from typing import Dict, List, Optional, Any, Union
import asyncio
from loguru import logger
from neo4j import GraphDatabase, basic_auth
from neo4j.exceptions import ServiceUnavailable, DatabaseError
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

class Neo4jClient:
    """Client for interacting with Neo4j graph database."""

    def __init__(self, uri: str, user: str, password: str):
        """Initialize the Neo4j client.
        
        Args:
            uri: URI of the Neo4j server (e.g., 'neo4j://localhost:7687')
            user: Username for authentication
            password: Password for authentication
        """
        self.uri = uri
        self.user = user
        self.password = password
        self.driver = None
        self._connected = False
        self._schema_initialized = False

    async def connect(self) -> None:
        """Connect to the Neo4j database."""
        try:
            # Neo4j driver is synchronous, run in executor
            self.driver = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: GraphDatabase.driver(
                    self.uri,
                    auth=basic_auth(self.user, self.password)
                )
            )
            
            # Verify connection
            await self._verify_connectivity()
            self._connected = True
            logger.info(f"Connected to Neo4j at {self.uri}")
            
        except Exception as e:
            logger.error(f"Failed to connect to Neo4j: {e}")
            self.driver = None
            self._connected = False
            raise

    async def close(self) -> None:
        """Close the connection to Neo4j."""
        if self.driver:
            await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.driver.close()
            )
            self.driver = None
            self._connected = False
            logger.info("Neo4j connection closed")

    async def is_healthy(self) -> bool:
        """Check if the Neo4j connection is healthy."""
        if not self.driver:
            return False
            
        try:
            await self._verify_connectivity()
            return True
        except Exception:
            return False

    async def _verify_connectivity(self) -> None:
        """Verify that the connection to Neo4j is working."""
        if not self.driver:
            raise RuntimeError("Neo4j driver not initialized")
            
        try:
            await self.execute_query("RETURN 1 AS num")
        except Exception as e:
            logger.error(f"Neo4j connectivity check failed: {e}")
            raise

    async def initialize_schema(self) -> None:
        """Initialize the Neo4j schema with constraints and indices."""
        if not self.driver or not self._connected:
            raise RuntimeError("Neo4j client not connected")
            
        if self._schema_initialized:
            logger.info("Schema already initialized")
            return
            
        try:
            # Create constraints for uniqueness and faster lookups
            constraints = [
                "CREATE CONSTRAINT unique_paper IF NOT EXISTS FOR (p:Paper) REQUIRE p.id IS UNIQUE",
                "CREATE CONSTRAINT unique_entity IF NOT EXISTS FOR (e:Entity) REQUIRE (e.name, e.type) IS NODE KEY",
                "CREATE CONSTRAINT unique_repo IF NOT EXISTS FOR (r:Repository) REQUIRE r.id IS UNIQUE",
                "CREATE CONSTRAINT unique_company IF NOT EXISTS FOR (c:Company) REQUIRE c.name IS UNIQUE",
                "CREATE CONSTRAINT unique_person IF NOT EXISTS FOR (p:Person) REQUIRE p.name IS UNIQUE"
            ]
            
            # Create indices for common query properties
            indices = [
                "CREATE INDEX paper_title IF NOT EXISTS FOR (p:Paper) ON (p.title)",
                "CREATE INDEX entity_type IF NOT EXISTS FOR (e:Entity) ON (e.type)",
                "CREATE INDEX document_type IF NOT EXISTS FOR (d:Document) ON (d.type)",
                "CREATE INDEX source_type IF NOT EXISTS FOR (s:NewsArticle) ON (s.source)"
            ]
            
            # Execute all schema commands
            for query in constraints + indices:
                await self.execute_query(query)
                
            self._schema_initialized = True
            logger.info("Neo4j schema initialized with constraints and indices")
            
        except Exception as e:
            logger.error(f"Failed to initialize Neo4j schema: {e}")
            raise

    @retry(
        retry=retry_if_exception_type((ServiceUnavailable, DatabaseError)),
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=1, max=10)
    )
    async def execute_query(self, query: str, params: Dict[str, Any] = None) -> List[Dict[str, Any]]:
        """Execute a Cypher query against the Neo4j database.
        
        Args:
            query: Cypher query string
            params: Optional parameters for the query
            
        Returns:
            List of records as dictionaries
        """
        if not self.driver or not self._connected:
            raise RuntimeError("Neo4j client not connected")
            
        params = params or {}
        
        try:
            # Neo4j transaction functions are synchronous, run in executor
            result = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self._execute_query_sync(query, params)
            )
            
            return result
            
        except (ServiceUnavailable, DatabaseError) as e:
            logger.error(f"Neo4j query error (retrying): {e}")
            # These errors might be retried by the decorator
            raise
            
        except Exception as e:
            logger.error(f"Neo4j query error: {e}")
            raise RuntimeError(f"Query execution failed: {str(e)}")

    def _execute_query_sync(self, query: str, params: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Synchronous execution of a Cypher query (run in executor).
        
        Args:
            query: Cypher query string
            params: Parameters for the query
            
        Returns:
            List of records as dictionaries
        """
        with self.driver.session() as session:
            result = session.run(query, params)
            return [dict(record) for record in result]

    async def create_node(self, labels: List[str], properties: Dict[str, Any]) -> str:
        """Create a new node in the graph.
        
        Args:
            labels: List of node labels
            properties: Node properties
            
        Returns:
            Internal ID of the created node
        """
        labels_str = ':'.join(labels)
        query = f"""
        CREATE (n:{labels_str} $properties)
        RETURN id(n) AS id
        """
        
        result = await self.execute_query(query, {"properties": properties})
        
        if not result:
            raise RuntimeError("Failed to create node")
            
        return result[0]["id"]

    async def merge_node(
        self, 
        labels: List[str], 
        match_properties: Dict[str, Any],
        additional_properties: Dict[str, Any] = None
    ) -> str:
        """Create a node if it doesn't exist, or update an existing node.
        
        Args:
            labels: List of node labels
            match_properties: Properties to match existing nodes
            additional_properties: Additional properties to set if creating a new node
            
        Returns:
            Internal ID of the node
        """
        labels_str = ':'.join(labels)
        
        # Prepare properties: match_properties are used for both matching and setting,
        # additional_properties are only used when creating a new node
        all_properties = {**match_properties}
        if additional_properties:
            all_properties.update(additional_properties)
        
        query = f"""
        MERGE (n:{labels_str} {self._dict_to_properties_string(match_properties)})
        ON CREATE SET n = $all_properties
        RETURN id(n) AS id
        """
        
        result = await self.execute_query(
            query, 
            {
                "all_properties": all_properties
            }
        )
        
        if not result:
            raise RuntimeError("Failed to merge node")
            
        return result[0]["id"]

    def _dict_to_properties_string(self, properties: Dict[str, Any]) -> str:
        """Convert a dictionary to a Cypher properties string.
        
        Args:
            properties: Dictionary of property names and values
            
        Returns:
            Cypher properties string (e.g., "{name: $match_properties.name}")
        """
        props = []
        for key in properties:
            props.append(f"{key}: $match_properties.{key}")
            
        return "{" + ", ".join(props) + "}"

    async def find_node_by_properties(
        self, 
        labels: List[str], 
        properties: Dict[str, Any]
    ) -> Optional[Dict[str, Any]]:
        """Find a node by its labels and properties.
        
        Args:
            labels: List of node labels
            properties: Properties to match
            
        Returns:
            Node data or None if not found
        """
        labels_str = ':'.join(labels)
        
        # Build a match clause that checks each property
        where_clauses = []
        for key in properties:
            where_clauses.append(f"n.{key} = ${key}")
            
        where_str = " AND ".join(where_clauses)
        
        query = f"""
        MATCH (n:{labels_str})
        WHERE {where_str}
        RETURN id(n) AS id, properties(n) AS properties, labels(n) AS labels
        LIMIT 1
        """
        
        result = await self.execute_query(query, properties)
        
        if not result:
            return None
            
        return result[0]

    async def find_nodes_by_properties(
        self, 
        labels: List[str], 
        properties: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """Find all nodes matching labels and properties.
        
        Args:
            labels: List of node labels
            properties: Properties to match
            
        Returns:
            List of matching nodes
        """
        labels_str = ':'.join(labels)
        
        # Build a match clause that checks each property
        where_clauses = []
        for key in properties:
            # Skip None values
            if properties[key] is not None:
                where_clauses.append(f"n.{key} = ${key}")
            
        where_str = " AND ".join(where_clauses) if where_clauses else "1=1"
        
        query = f"""
        MATCH (n:{labels_str})
        WHERE {where_str}
        RETURN id(n) AS id, properties(n) AS properties, labels(n) AS labels
        """
        
        result = await self.execute_query(query, properties)
        return result

    async def update_node(self, node_id: str, properties: Dict[str, Any]) -> None:
        """Update node properties.
        
        Args:
            node_id: Internal ID of the node
            properties: New properties to set
        """
        query = """
        MATCH (n)
        WHERE id(n) = $node_id
        SET n += $properties
        """
        
        await self.execute_query(query, {"node_id": node_id, "properties": properties})

    async def delete_node(self, node_id: str) -> None:
        """Delete a node by its internal ID.
        
        Args:
            node_id: Internal ID of the node
        """
        query = """
        MATCH (n)
        WHERE id(n) = $node_id
        DETACH DELETE n
        """
        
        await self.execute_query(query, {"node_id": node_id})

    async def create_relationship(
        self, 
        start_node_id: str, 
        end_node_id: str, 
        rel_type: str,
        properties: Dict[str, Any] = None
    ) -> str:
        """Create a relationship between two nodes.
        
        Args:
            start_node_id: Internal ID of the start node
            end_node_id: Internal ID of the end node
            rel_type: Type of relationship
            properties: Relationship properties
            
        Returns:
            Internal ID of the created relationship
        """
        properties = properties or {}
        
        query = """
        MATCH (a), (b)
        WHERE id(a) = $start_id AND id(b) = $end_id
        CREATE (a)-[r:`{}`]->(b)
        SET r = $properties
        RETURN id(r) AS id
        """.format(rel_type)
        
        result = await self.execute_query(
            query, 
            {
                "start_id": start_node_id,
                "end_id": end_node_id,
                "properties": properties
            }
        )
        
        if not result:
            raise RuntimeError("Failed to create relationship")
            
        return result[0]["id"]
</file>

<file path="microservices/python/knowledge-graph/Dockerfile">
FROM python:3.10-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    build-essential \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements file
COPY requirements.txt .

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY app/ ./app/

# Expose the service port
EXPOSE 8003

# Set environment variables
ENV PORT=8003
ENV PYTHONUNBUFFERED=1

# Run the application
CMD ["python", "-m", "app.main"]
</file>

<file path="microservices/python/knowledge-graph/README.md">
# Knowledge Graph Service

A microservice that manages a knowledge graph using Neo4j to store entities and relationships extracted from content.

## Features

- Entity and relationship management via REST API
- Graph querying with customizable Cypher queries
- Automatic entity and relationship extraction from NLP-enriched data
- Batch operations for efficient data processing
- Integration with NATS message streaming
- Health monitoring and automatic reconnection

## Components

- **Neo4j Client**: Manages connections and operations with the Neo4j database
- **NATS Client**: Handles messaging for asynchronous processing of enriched data
- **Data Models**: Defines entity and relationship schemas
- **FastAPI Server**: Provides REST API endpoints

## API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/health` | GET | Health check endpoint |
| `/entities` | POST | Create a new entity |
| `/entities/{id}` | GET | Get entity by ID |
| `/entities/{id}` | DELETE | Delete entity by ID |
| `/relationships` | POST | Create a new relationship |
| `/relationships/{id}` | GET | Get relationship by ID |
| `/relationships/{id}` | DELETE | Delete relationship by ID |
| `/query` | POST | Execute a graph query |
| `/entities/batch` | POST | Batch operation for entities |
| `/relationships/batch` | POST | Batch operation for relationships |

## Message Processing

The service subscribes to the `nlp.enriched.*` NATS subject to automatically process NLP-enriched data:

1. Extracts entities from the enriched data
2. Creates nodes for the source document and identified entities
3. Establishes relationships between entities and documents
4. Updates existing entities with additional metadata when found

## Requirements

- Python 3.8+
- Neo4j 4.4+
- NATS Server 2.2+
- Dependencies listed in `requirements.txt`

## Installation

```bash
pip install -r requirements.txt
```

## Configuration

The service is configured using environment variables:

| Variable | Description | Default |
|----------|-------------|---------|
| `NEO4J_URI` | URI for Neo4j connection | `neo4j://localhost:7687` |
| `NEO4J_USER` | Neo4j username | `neo4j` |
| `NEO4J_PASSWORD` | Neo4j password | `password` |
| `NATS_URL` | URL for NATS connection | `nats://localhost:4222` |
| `NATS_USER` | NATS username | - |
| `NATS_PASSWORD` | NATS password | - |
| `INITIALIZE_SCHEMA` | Whether to initialize schema | `true` |
| `LOG_LEVEL` | Logging level | `info` |

## Usage

### Running Locally

```bash
uvicorn app.main:app --host 0.0.0.0 --port 8000
```

### Using Docker

```bash
docker build -t knowledge-graph-service .
docker run -p 8000:8000 knowledge-graph-service
```

## Schema

The knowledge graph uses the following main node types:

- `Document`: Source documents (papers, articles, etc.)
- `Person`: Individuals mentioned in content
- `Organization`: Companies, institutions, etc.
- `Technology`: Technical concepts, methods, tools
- `Topic`: Subject areas or domains
- `Location`: Geographical locations

Relationships between these entities are typed and can include:

- `MENTIONS`: Document mentions an entity
- `CREATED_BY`: Document created by person/organization
- `BELONGS_TO`: Entity belongs to organization/category
- `RELATED_TO`: General relationship between entities

## Example Queries

### Find connections between technologies

```cypher
MATCH (t1:Technology)-[r*1..3]-(t2:Technology)
WHERE t1.name = 'Machine Learning' AND t2.name = 'Computer Vision'
RETURN t1, r, t2
```

### Find key people in a domain

```cypher
MATCH (p:Person)-[:CREATED_BY|CONTRIBUTED_TO]->(d:Document)-[:CATEGORY]->(t:Topic)
WHERE t.name = 'Artificial Intelligence'
RETURN p.name, count(d) as publications
ORDER BY publications DESC
LIMIT 10
```

## Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Create a pull request
</file>

<file path="microservices/python/ml-orchestrator/app/checkpoint.py">
#!/usr/bin/env python3
"""
Checkpoint mechanism for batch inference jobs in Chimera.
Provides resilience for workloads running on spot instances.
"""
import os
import json
import time
import uuid
import argparse
import asyncio
import signal
import boto3
from botocore.exceptions import ClientError
from loguru import logger

# S3 client for persisting checkpoints
s3_client = boto3.client(
    's3',
    region_name=os.environ.get('AWS_REGION', 'eu-west-1'),
    endpoint_url=os.environ.get('S3_ENDPOINT', None)
)
CHECKPOINT_BUCKET = os.environ.get('CHECKPOINT_BUCKET', 'chimera-checkpoints')
LOCAL_CHECKPOINT_DIR = os.environ.get('LOCAL_CHECKPOINT_DIR', '/data/checkpoints')
BATCH_ID = os.environ.get('BATCH_PROCESSOR_ID', str(uuid.uuid4()))

class CheckpointManager:
    """Manages checkpoints for batch inference jobs to provide spot instance resilience."""
    
    def __init__(self, checkpoint_bucket, local_dir, batch_id):
        """Initialize the checkpoint manager.
        
        Args:
            checkpoint_bucket: S3 bucket for checkpoint storage
            local_dir: Local directory for checkpoint staging
            batch_id: Unique ID for this batch processor instance
        """
        self.checkpoint_bucket = checkpoint_bucket
        self.local_dir = local_dir
        self.batch_id = batch_id
        self.current_job = None
        
        # Ensure local directory exists
        os.makedirs(self.local_dir, exist_ok=True)
        
        # Set up signal handlers for graceful shutdown
        signal.signal(signal.SIGTERM, self._handle_sigterm)
        signal.signal(signal.SIGINT, self._handle_sigterm)
        
        logger.info(f"CheckpointManager initialized with batch ID: {self.batch_id}")
        
    def _handle_sigterm(self, signum, frame):
        """Handle termination signal by triggering final checkpoint."""
        logger.warning(f"Received signal {signum}, initiating final checkpoint")
        if self.current_job:
            asyncio.run(self.save_checkpoint(
                self.current_job, 
                progress=-1,  # -1 indicates interrupted 
                data={'status': 'interrupted', 'timestamp': time.time()}
            ))
        
    async def save_checkpoint(self, job_id, progress, data):
        """Save job progress checkpoint to S3.
        
        Args:
            job_id: Unique ID of the job
            progress: Progress indicator (0-100 or -1 for interrupted)
            data: Dictionary of checkpoint data
            
        Returns:
            True if successful, False otherwise
        """
        self.current_job = job_id
        checkpoint_key = f"batch/{job_id}/checkpoint-{progress}.json"
        local_path = os.path.join(self.local_dir, f"{job_id}-{progress}.json")
        
        # Prepare checkpoint data with metadata
        checkpoint_data = {
            'job_id': job_id,
            'progress': progress,
            'processor_id': self.batch_id,
            'timestamp': time.time(),
            'data': data
        }
        
        # First write locally for speed and durability
        try:
            with open(local_path, 'w') as f:
                json.dump(checkpoint_data, f)
                
            logger.debug(f"Local checkpoint saved: {local_path}")
        except Exception as e:
            logger.error(f"Error saving local checkpoint: {e}")
            return False
        
        # Then upload to S3 for persistence
        try:
            s3_client.put_object(
                Bucket=self.checkpoint_bucket,
                Key=checkpoint_key,
                Body=json.dumps(checkpoint_data),
                ContentType='application/json',
                ServerSideEncryption='AES256'  # Encryption at rest for GDPR/HIPAA
            )
            logger.info(f"Checkpoint saved to S3: {checkpoint_key}")
            return True
        except ClientError as e:
            logger.error(f"Error saving S3 checkpoint: {e}")
            # We still have the local copy, so not a complete failure
            return False
        
    async def load_latest_checkpoint(self, job_id):
        """Load the latest checkpoint for a job.
        
        Args:
            job_id: Unique ID of the job
            
        Returns:
            Checkpoint data dictionary or None if not found
        """
        try:
            # List all checkpoints for this job
            response = s3_client.list_objects_v2(
                Bucket=self.checkpoint_bucket,
                Prefix=f"batch/{job_id}/checkpoint-"
            )
            
            if 'Contents' not in response or not response['Contents']:
                logger.warning(f"No checkpoints found for job {job_id}")
                return None
            
            # Find the checkpoint with the highest progress number
            latest = max(response['Contents'], 
                        key=lambda obj: int(obj['Key'].split('-')[-1].split('.')[0]) 
                        if obj['Key'].split('-')[-1].split('.')[0].isdigit() 
                        else -1)
            
            # Get the checkpoint data
            checkpoint_obj = s3_client.get_object(
                Bucket=self.checkpoint_bucket,
                Key=latest['Key']
            )
            
            checkpoint_data = json.loads(checkpoint_obj['Body'].read().decode('utf-8'))
            logger.info(f"Loaded checkpoint: {latest['Key']}, progress: {checkpoint_data['progress']}")
            
            # Also save locally for future reference
            local_path = os.path.join(self.local_dir, f"{job_id}-{checkpoint_data['progress']}.json")
            with open(local_path, 'w') as f:
                json.dump(checkpoint_data, f)
                
            return checkpoint_data
        
        except ClientError as e:
            logger.error(f"Error loading checkpoint from S3: {e}")
            
            # Try to load from local storage as fallback
            try:
                # Find all local checkpoints for this job
                local_files = [f for f in os.listdir(self.local_dir) 
                            if f.startswith(f"{job_id}-") and f.endswith(".json")]
                
                if not local_files:
                    return None
                
                # Find the latest local checkpoint
                latest_local = max(local_files, 
                                key=lambda f: int(f.split('-')[-1].split('.')[0]) 
                                if f.split('-')[-1].split('.')[0].isdigit() 
                                else -1)
                
                # Load the checkpoint data
                with open(os.path.join(self.local_dir, latest_local), 'r') as f:
                    checkpoint_data = json.loads(f.read())
                
                logger.info(f"Loaded local checkpoint: {latest_local}, progress: {checkpoint_data['progress']}")
                return checkpoint_data
            
            except Exception as local_e:
                logger.error(f"Error loading local checkpoint: {local_e}")
                return None

def main():
    parser = argparse.ArgumentParser(description='Chimera Batch Checkpoint Utility')
    parser.add_argument('--job-id', dest='job_id', help='Job ID to checkpoint')
    parser.add_argument('--progress', dest='progress', type=int, default=0, 
                        help='Progress indicator (0-100)')
    parser.add_argument('--data', dest='data', default='{}',
                        help='JSON string of checkpoint data')
    parser.add_argument('--final', dest='final', action='store_true',
                        help='Perform final checkpoint before termination')
    parser.add_argument('--load', dest='load', action='store_true',
                        help='Load latest checkpoint')
    args = parser.parse_args()
    
    checkpoint_manager = CheckpointManager(
        CHECKPOINT_BUCKET,
        LOCAL_CHECKPOINT_DIR,
        BATCH_ID
    )
    
    if args.final:
        # Get current job from environment or file
        current_job = os.environ.get('CURRENT_JOB_ID')
        if not current_job:
            try:
                with open(os.path.join(LOCAL_CHECKPOINT_DIR, 'current_job'), 'r') as f:
                    current_job = f.read().strip()
            except:
                logger.error("No current job found for final checkpoint")
                return
        
        if current_job:
            logger.warning(f"Performing final checkpoint for job {current_job}")
            asyncio.run(checkpoint_manager.save_checkpoint(
                current_job, 
                -1,  # -1 indicates interrupted
                {'status': 'interrupted', 'timestamp': time.time()}
            ))
    
    elif args.load:
        if not args.job_id:
            logger.error("Job ID required for loading checkpoint")
            return
            
        checkpoint = asyncio.run(checkpoint_manager.load_latest_checkpoint(args.job_id))
        if checkpoint:
            print(json.dumps(checkpoint))
    
    else:
        if not args.job_id:
            logger.error("Job ID required for checkpoint")
            return
            
        # Save the current job ID for potential final checkpoint
        with open(os.path.join(LOCAL_CHECKPOINT_DIR, 'current_job'), 'w') as f:
            f.write(args.job_id)
            
        # Save the checkpoint
        try:
            data = json.loads(args.data)
        except:
            logger.error("Invalid JSON data provided")
            data = {}
            
        asyncio.run(checkpoint_manager.save_checkpoint(args.job_id, args.progress, data))

if __name__ == "__main__":
    main()
</file>

<file path="microservices/python/ml-orchestrator/app/config.py">
from functools import lru_cache
from typing import Dict, Any, Optional
import os
from pydantic import BaseModel, Field
from dotenv import load_dotenv

# Load environment variables from .env file if it exists
load_dotenv()

class Settings(BaseModel):
    """Application settings loaded from environment variables."""
    
    # Service configuration
    service_name: str = Field(default="ml-orchestrator")
    environment: str = Field(default=os.getenv("ENVIRONMENT", "development"))
    log_level: str = Field(default=os.getenv("LOG_LEVEL", "INFO"))
    
    # API configuration
    host: str = Field(default=os.getenv("HOST", "0.0.0.0"))
    port: int = Field(default=int(os.getenv("PORT", "8000")))
    
    # NATS configuration
    nats_url: str = Field(default=os.getenv("NATS_URL", "nats://localhost:4222"))
    nats_user: Optional[str] = Field(default=os.getenv("NATS_USER"))
    nats_password: Optional[str] = Field(default=os.getenv("NATS_PASSWORD"))
    
    # Triton configuration
    triton_url: str = Field(default=os.getenv("TRITON_URL", "localhost:8000"))
    
    # Model configurations
    summarization_model: str = Field(default=os.getenv("SUMMARIZATION_MODEL", "bart_summarization"))
    entity_extraction_model: str = Field(default=os.getenv("ENTITY_EXTRACTION_MODEL", "ner_model"))
    
    # Processing parameters
    default_max_summary_length: int = Field(default=int(os.getenv("DEFAULT_MAX_SUMMARY_LENGTH", "150")))
    default_min_summary_length: int = Field(default=int(os.getenv("DEFAULT_MIN_SUMMARY_LENGTH", "50")))
    default_entity_confidence: float = Field(default=float(os.getenv("DEFAULT_ENTITY_CONFIDENCE", "0.5")))
    
    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"
        case_sensitive = False

@lru_cache()
def get_settings() -> Settings:
    """Create cached settings instance."""
    return Settings()
</file>

<file path="microservices/python/ml-orchestrator/app/main.py">
from contextlib import asynccontextmanager
from typing import Dict, List, Optional, Any, Union
import asyncio
import json
import os
from uuid import uuid4

import numpy as np
from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from loguru import logger
import nats
from nats.js.api import StreamConfig, ConsumerConfig
import tritonclient.http as httpclient
from tritonclient.utils import InferenceServerException

from app.config import Settings, get_settings
from app.models import (
    ProcessRequest, 
    TextSummarizationRequest, 
    EntityExtractionRequest, 
    ProcessResponse,
    HealthResponse
)
from app.triton import TritonClient
from app.nats_client import NatsClient

# Global clients
triton_client: Optional[TritonClient] = None
nats_client: Optional[NatsClient] = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application startup and shutdown events."""
    global triton_client, nats_client
    settings = get_settings()
    
    # Initialize Triton client
    logger.info(f"Connecting to Triton at {settings.triton_url}")
    triton_client = TritonClient(settings.triton_url)
    await triton_client.connect()
    
    # Initialize NATS client
    logger.info(f"Connecting to NATS at {settings.nats_url}")
    nats_client = NatsClient(settings.nats_url)
    await nats_client.connect()
    
    # Initialize NATS streams and consumers
    if nats_client.js:
        try:
            # Create streams if they don't exist
            stream_config = StreamConfig(
                name="NLP_PROCESSING",
                subjects=["ingest.validated.*", "nlp.request.*", "nlp.enriched.*"],
                storage="file",
                retention="limits",
                max_msgs=10_000_000,
                max_bytes=10_000_000_000,
                discard="old",
                max_age=86400_000_000_000,  # 1 day in nanoseconds
            )
            await nats_client.js.add_stream(stream_config)
            logger.info("NLP_PROCESSING stream configured")
            
            # Set up consumers
            for subject in ["ingest.validated.*"]:
                consumer_config = ConsumerConfig(
                    durable_name=f"ml-orchestrator-{subject.replace('*', 'all').replace('.', '-')}",
                    ack_policy="explicit",
                    max_deliver=5,
                    ack_wait=60_000_000_000,  # 60 seconds in nanoseconds
                )
                await nats_client.js.add_consumer("NLP_PROCESSING", consumer_config)
            
            logger.info("NATS consumers configured")
        except Exception as e:
            logger.error(f"Failed to configure NATS streams/consumers: {e}")
    
    # Start background processors
    asyncio.create_task(process_validated_messages())
    
    logger.info("ML Orchestrator service started")
    
    yield
    
    # Cleanup
    if nats_client:
        await nats_client.close()
        logger.info("NATS connection closed")
    
    if triton_client:
        await triton_client.close()
        logger.info("Triton connection closed")
    
    logger.info("ML Orchestrator service shutdown")

app = FastAPI(
    title="Chimera ML Orchestrator Service",
    description="Orchestrates NLP processing tasks using Triton Inference Server",
    version="0.1.0",
    lifespan=lifespan,
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

async def process_validated_messages():
    """Background task to process validated messages from NATS."""
    if not nats_client or not nats_client.js:
        logger.error("NATS client not initialized")
        return
    
    logger.info("Starting background processor for validated messages")
    
    while True:
        try:
            sub = await nats_client.js.pull_subscribe(
                "ingest.validated.*", 
                "ml-orchestrator-ingest-validated-all"
            )
            
            while True:
                try:
                    msgs = await sub.fetch(10, timeout=1)
                    for msg in msgs:
                        try:
                            data = json.loads(msg.data.decode())
                            logger.info(f"Processing message: {data['id']}")
                            
                            # Check content type to determine processing pipeline
                            content_type = data.get("content_type", "")
                            
                            if "research_paper" in content_type or "news_article" in content_type:
                                # For text content, extract text and run summarization + entity extraction
                                if text_content := data.get("payload", {}).get("text", ""):
                                    # Process with Triton
                                    result = await process_text_content(text_content, data["id"])
                                    
                                    # Publish enriched data
                                    enriched_data = {
                                        **data,
                                        "nlp_enrichment": result,
                                    }
                                    
                                    await nats_client.publish(
                                        f"nlp.enriched.{content_type}", 
                                        json.dumps(enriched_data)
                                    )
                                    logger.info(f"Published enriched data for {data['id']}")
                            
                            await msg.ack()
                        except Exception as e:
                            logger.error(f"Error processing message: {e}")
                            await msg.nak(delay=5)
                
                except Exception as e:
                    if "timeout" not in str(e).lower():
                        logger.error(f"Error fetching messages: {e}")
                    await asyncio.sleep(1)
        
        except Exception as e:
            logger.error(f"Subscription error: {e}")
            await asyncio.sleep(5)  # Wait before reconnecting

async def process_text_content(text: str, request_id: str) -> Dict[str, Any]:
    """Process text content through Triton models."""
    if not triton_client:
        raise RuntimeError("Triton client not initialized")
    
    # Run summarization
    summary = await triton_client.run_summarization(text)
    
    # Run entity extraction
    entities = await triton_client.run_entity_extraction(text)
    
    return {
        "summary": summary,
        "entities": entities,
        "processed_at": str(np.datetime64('now')),
    }

@app.get("/health")
async def health_check() -> HealthResponse:
    """Health check endpoint."""
    triton_healthy = triton_client and await triton_client.is_healthy()
    nats_healthy = nats_client and nats_client.is_connected()
    
    return HealthResponse(
        service="ml-orchestrator",
        status="healthy" if (triton_healthy and nats_healthy) else "degraded",
        components={
            "triton": "connected" if triton_healthy else "disconnected",
            "nats": "connected" if nats_healthy else "disconnected",
        },
        version="0.1.0"
    )

@app.post("/process", response_model=ProcessResponse)
async def process_data(
    request: ProcessRequest,
    background_tasks: BackgroundTasks,
    settings: Settings = Depends(get_settings)
) -> ProcessResponse:
    """Generic endpoint to process data through various NLP pipelines."""
    if not triton_client:
        raise HTTPException(500, "Triton client not initialized")
    
    request_id = request.request_id or str(uuid4())
    logger.info(f"Processing request {request_id}")
    
    try:
        result = {}
        
        # Process text summarization if requested
        if request.text_content:
            # If immediate is True, process synchronously
            if request.immediate:
                result["summary"] = await triton_client.run_summarization(request.text_content)
                
                if request.extract_entities:
                    result["entities"] = await triton_client.run_entity_extraction(request.text_content)
            else:
                # Queue for asynchronous processing
                await nats_client.publish(
                    "nlp.request.process",
                    json.dumps({
                        "request_id": request_id,
                        "text_content": request.text_content,
                        "extract_entities": request.extract_entities,
                        "timestamp": str(np.datetime64('now')),
                    })
                )
                result["status"] = "queued"
        
        return ProcessResponse(
            request_id=request_id,
            status="completed" if request.immediate else "queued",
            result=result
        )
    
    except Exception as e:
        logger.error(f"Error processing request: {e}")
        raise HTTPException(500, f"Processing error: {str(e)}")

@app.post("/summarize", response_model=ProcessResponse)
async def summarize_text(request: TextSummarizationRequest) -> ProcessResponse:
    """Endpoint to summarize text."""
    if not triton_client:
        raise HTTPException(500, "Triton client not initialized")
    
    request_id = request.request_id or str(uuid4())
    
    try:
        summary = await triton_client.run_summarization(
            request.text,
            max_length=request.max_length or 150,
            min_length=request.min_length or 50,
        )
        
        return ProcessResponse(
            request_id=request_id,
            status="completed",
            result={"summary": summary}
        )
    
    except Exception as e:
        logger.error(f"Summarization error: {e}")
        raise HTTPException(500, f"Summarization error: {str(e)}")

@app.post("/extract_entities", response_model=ProcessResponse)
async def extract_entities(request: EntityExtractionRequest) -> ProcessResponse:
    """Endpoint to extract entities from text."""
    if not triton_client:
        raise HTTPException(500, "Triton client not initialized")
    
    request_id = request.request_id or str(uuid4())
    
    try:
        entities = await triton_client.run_entity_extraction(
            request.text,
            confidence_threshold=request.confidence_threshold or 0.5,
        )
        
        return ProcessResponse(
            request_id=request_id,
            status="completed",
            result={"entities": entities}
        )
    
    except Exception as e:
        logger.error(f"Entity extraction error: {e}")
        raise HTTPException(500, f"Entity extraction error: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    
    port = int(os.getenv("PORT", "8000"))
    host = os.getenv("HOST", "0.0.0.0")
    
    uvicorn.run("app.main:app", host=host, port=port, reload=True)
</file>

<file path="microservices/python/ml-orchestrator/app/models.py">
from typing import Dict, List, Optional, Union, Any
from pydantic import BaseModel, Field
from uuid import uuid4
from datetime import datetime

class TextSummarizationRequest(BaseModel):
    """Request model for text summarization."""
    request_id: Optional[str] = Field(default_factory=lambda: str(uuid4()))
    text: str = Field(..., description="The text to summarize")
    max_length: Optional[int] = Field(default=None, description="Maximum length of the summary")
    min_length: Optional[int] = Field(default=None, description="Minimum length of the summary")
    
class EntityExtractionRequest(BaseModel):
    """Request model for entity extraction."""
    request_id: Optional[str] = Field(default_factory=lambda: str(uuid4()))
    text: str = Field(..., description="The text to extract entities from")
    confidence_threshold: Optional[float] = Field(default=None, description="Minimum confidence for entity extraction")
    entity_types: Optional[List[str]] = Field(default=None, description="Types of entities to extract")

class ProcessRequest(BaseModel):
    """Generic request model for processing data."""
    request_id: Optional[str] = Field(default_factory=lambda: str(uuid4()))
    text_content: Optional[str] = Field(default=None, description="Text content to process")
    extract_entities: bool = Field(default=True, description="Whether to extract entities")
    immediate: bool = Field(default=True, description="Process immediately (sync) or queue (async)")
    parameters: Optional[Dict[str, Any]] = Field(default=None, description="Additional parameters for processing")

class Entity(BaseModel):
    """Entity extracted from text."""
    text: str = Field(..., description="The entity text")
    type: str = Field(..., description="The entity type (e.g., PERSON, ORG)")
    start: int = Field(..., description="Start position in the text")
    end: int = Field(..., description="End position in the text")
    confidence: float = Field(..., description="Confidence score for the entity")

class ProcessResponse(BaseModel):
    """Response model for processing requests."""
    request_id: str = Field(..., description="ID of the processed request")
    status: str = Field(..., description="Status of the processing (completed, queued, failed)")
    result: Dict[str, Any] = Field(default_factory=dict, description="Processing results")
    error: Optional[str] = Field(default=None, description="Error message if processing failed")
    processed_at: Optional[str] = Field(default_factory=lambda: datetime.now().isoformat(), description="Timestamp of processing")

class HealthResponse(BaseModel):
    """Health check response model."""
    service: str = Field(..., description="Service name")
    status: str = Field(..., description="Service status (healthy, degraded, unhealthy)")
    components: Dict[str, str] = Field(default_factory=dict, description="Status of individual components")
    version: str = Field(..., description="Service version")
</file>

<file path="microservices/python/ml-orchestrator/app/triton.py">
from typing import Dict, List, Any, Optional
import json
import numpy as np
import asyncio
from loguru import logger
import tritonclient.http as httpclient
from tritonclient.utils import InferenceServerException

class TritonClient:
    """Client for interacting with Triton Inference Server."""

    def __init__(self, url: str):
        """Initialize the Triton client.
        
        Args:
            url: URL of the Triton server (e.g., 'localhost:8000')
        """
        self.url = url
        self.client = None
        self.connected = False

    async def connect(self) -> None:
        """Connect to the Triton server."""
        try:
            # Triton HTTP client is synchronous, wrap in executor
            self.client = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: httpclient.InferenceServerClient(url=self.url)
            )
            
            # Check server liveness to verify connection
            is_live = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.client.is_server_live()
            )
            
            if not is_live:
                logger.error("Triton server is not live")
                self.client = None
                self.connected = False
                return

            # Check available models
            models = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.client.get_model_repository_index()
            )
            logger.info(f"Connected to Triton server with {len(models)} models available")
            
            self.connected = True
            
        except Exception as e:
            logger.error(f"Failed to connect to Triton server: {e}")
            self.client = None
            self.connected = False
            raise

    async def close(self) -> None:
        """Close the connection to Triton server."""
        self.client = None
        self.connected = False

    async def is_healthy(self) -> bool:
        """Check if the Triton server is healthy."""
        if not self.client:
            return False
            
        try:
            is_live = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.client.is_server_live()
            )
            is_ready = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.client.is_server_ready()
            )
            return is_live and is_ready
        except Exception:
            return False

    async def run_summarization(
        self, 
        text: str, 
        max_length: int = 150, 
        min_length: int = 50
    ) -> str:
        """Run text summarization through Triton.
        
        Args:
            text: Text to summarize
            max_length: Maximum length of the summary
            min_length: Minimum length of the summary
            
        Returns:
            Summarized text
        """
        if not self.client or not self.connected:
            raise RuntimeError("Triton client not connected")

        try:
            # Prepare inputs
            text_bytes = text.encode('utf-8')
            inputs = [
                httpclient.InferInput('TEXT_INPUT', [1], "BYTES"),
                httpclient.InferInput('MAX_LENGTH', [1], "INT32"),
                httpclient.InferInput('MIN_LENGTH', [1], "INT32"),
            ]
            inputs[0].set_data_from_numpy(np.array([text_bytes], dtype=np.object_))
            inputs[1].set_data_from_numpy(np.array([max_length], dtype=np.int32))
            inputs[2].set_data_from_numpy(np.array([min_length], dtype=np.int32))

            # Define outputs
            outputs = [
                httpclient.InferRequestedOutput('SUMMARY', binary_data=False),
            ]

            # Make inference request
            request_id = f"summarize-{np.random.randint(0, 10000)}"
            logger.debug(f"Sending summarization request {request_id}")
            
            response = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.client.infer(
                    model_name='bart_summarization',
                    inputs=inputs,
                    outputs=outputs,
                    request_id=request_id
                )
            )

            # Process results
            summary_bytes = response.as_numpy('SUMMARY')[0]
            summary = summary_bytes.decode('utf-8') if isinstance(summary_bytes, bytes) else str(summary_bytes)
            
            logger.debug(f"Summarization complete: {len(summary)} chars")
            return summary

        except InferenceServerException as e:
            logger.error(f"Triton inference error: {e}")
            raise RuntimeError(f"Summarization inference failed: {e}")
        
        except Exception as e:
            logger.error(f"Summarization error: {e}")
            raise RuntimeError(f"Summarization failed: {e}")

    async def run_entity_extraction(
        self, 
        text: str,
        confidence_threshold: float = 0.5
    ) -> List[Dict[str, Any]]:
        """Extract entities from text using Triton.
        
        Args:
            text: Text to extract entities from
            confidence_threshold: Minimum confidence for extracted entities
            
        Returns:
            List of extracted entities
        """
        if not self.client or not self.connected:
            raise RuntimeError("Triton client not connected")

        try:
            # Prepare inputs
            text_bytes = text.encode('utf-8')
            inputs = [
                httpclient.InferInput('TEXT_INPUT', [1], "BYTES"),
                httpclient.InferInput('CONFIDENCE_THRESHOLD', [1], "FP32"),
            ]
            inputs[0].set_data_from_numpy(np.array([text_bytes], dtype=np.object_))
            inputs[1].set_data_from_numpy(np.array([confidence_threshold], dtype=np.float32))

            # Define outputs
            outputs = [
                httpclient.InferRequestedOutput('ENTITIES', binary_data=False),
            ]

            # Make inference request
            request_id = f"extract-entities-{np.random.randint(0, 10000)}"
            logger.debug(f"Sending entity extraction request {request_id}")
            
            response = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.client.infer(
                    model_name='ner_model',
                    inputs=inputs,
                    outputs=outputs,
                    request_id=request_id
                )
            )

            # Process results
            entities_json = response.as_numpy('ENTITIES')[0]
            entities_str = entities_json.decode('utf-8') if isinstance(entities_json, bytes) else str(entities_json)
            entities = json.loads(entities_str)
            
            logger.debug(f"Entity extraction complete: {len(entities)} entities found")
            return entities

        except InferenceServerException as e:
            logger.error(f"Triton inference error: {e}")
            raise RuntimeError(f"Entity extraction inference failed: {e}")
        
        except Exception as e:
            logger.error(f"Entity extraction error: {e}")
            raise RuntimeError(f"Entity extraction failed: {e}")

    async def run_model(
        self,
        model_name: str,
        inputs_dict: Dict[str, Any],
        output_names: List[str]
    ) -> Dict[str, Any]:
        """Generic method to run any model on Triton.
        
        Args:
            model_name: Name of the model to run
            inputs_dict: Dictionary of input name to numpy array
            output_names: List of output names to request
            
        Returns:
            Dictionary of output name to numpy array
        """
        if not self.client or not self.connected:
            raise RuntimeError("Triton client not connected")

        try:
            # Prepare inputs
            inputs = []
            for name, (data, shape, dtype) in inputs_dict.items():
                inp = httpclient.InferInput(name, shape, dtype)
                inp.set_data_from_numpy(data)
                inputs.append(inp)

            # Define outputs
            outputs = [
                httpclient.InferRequestedOutput(name, binary_data=False)
                for name in output_names
            ]

            # Make inference request
            request_id = f"model-{np.random.randint(0, 10000)}"
            
            response = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.client.infer(
                    model_name=model_name,
                    inputs=inputs,
                    outputs=outputs,
                    request_id=request_id
                )
            )

            # Process results
            result = {}
            for name in output_names:
                result[name] = response.as_numpy(name)
            
            return result

        except InferenceServerException as e:
            logger.error(f"Triton inference error: {e}")
            raise RuntimeError(f"Model inference failed: {e}")
        
        except Exception as e:
            logger.error(f"Model inference error: {e}")
            raise RuntimeError(f"Model inference failed: {e}")
</file>

<file path="microservices/python/ml-orchestrator/Dockerfile">
FROM python:3.10-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    build-essential \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements file
COPY requirements.txt .

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY app/ ./app/

# Expose the service port
EXPOSE 8002

# Set environment variables
ENV PORT=8002
ENV PYTHONUNBUFFERED=1

# Run the application
CMD ["python", "-m", "app.main"]
</file>

<file path="microservices/python/ml-orchestrator/README.md">
# ML Orchestrator Service

A microservice for orchestrating NLP processing tasks using NVIDIA Triton Inference Server, handling the coordination of machine learning workflows.

## Features

- Automated processing of content through NLP pipelines
- Text summarization using state-of-the-art models
- Entity extraction for knowledge graph population
- Asynchronous processing via NATS message queues
- Integration with Triton Inference Server for scalable model serving
- Checkpoint management for model versioning
- Health monitoring and automatic reconnection

## Components

- **Triton Client**: Interface to Triton Inference Server
- **NATS Client**: Handles message streaming for async processing
- **Checkpoint Manager**: Manages model versions and deployments
- **FastAPI Server**: Provides REST API endpoints

## API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/health` | GET | Health check endpoint |
| `/process` | POST | Process text through the NLP pipeline |
| `/summarize` | POST | Generate a summary for input text |
| `/extract_entities` | POST | Extract entities from input text |
| `/models` | GET | List available models |
| `/models/{model_name}/versions` | GET | List versions for a specific model |

## Message Processing

The service subscribes to the `ingest.validated.*` NATS subject to automatically process validated content:

1. Receives validated content from the ingestion service
2. Performs text extraction based on content type
3. Runs summarization and entity extraction on the text
4. Publishes enriched data to `nlp.enriched.*` subjects
5. Handles retries and error cases automatically

## Models

The service works with the following model types deployed on Triton:

- **Summarization Models**: Generate concise summaries of text content
- **Entity Extraction Models**: Identify and classify named entities
- **Embedding Models**: Generate vector embeddings for semantic search

## Requirements

- Python 3.8+
- NVIDIA Triton Inference Server 22.0+
- NATS Server 2.2+
- Dependencies listed in `requirements.txt`

## Installation

```bash
pip install -r requirements.txt
```

## Configuration

The service is configured using environment variables:

| Variable | Description | Default |
|----------|-------------|---------|
| `TRITON_URL` | URL for Triton server | `localhost:8000` |
| `NATS_URL` | URL for NATS connection | `nats://localhost:4222` |
| `NATS_USER` | NATS username | - |
| `NATS_PASSWORD` | NATS password | - |
| `LOG_LEVEL` | Logging level | `info` |
| `MAX_BATCH_SIZE` | Maximum batch size for inference | `16` |
| `CHECKPOINT_DIR` | Directory for model checkpoints | `/checkpoints` |

## Usage

### Running Locally

```bash
uvicorn app.main:app --host 0.0.0.0 --port 8000
```

### Using Docker

```bash
docker build -t ml-orchestrator-service .
docker run -p 8000:8000 ml-orchestrator-service
```

## Triton Model Configuration

Models in Triton should be configured as follows:

### Summarization Model

```
name: "summarization"
platform: "pytorch_libtorch"
max_batch_size: 16
input [
  {
    name: "TEXT"
    data_type: TYPE_STRING
    dims: [ 1 ]
  }
]
output [
  {
    name: "SUMMARY"
    data_type: TYPE_STRING
    dims: [ 1 ]
  }
]
```

### Entity Extraction Model

```
name: "entity_extraction"
platform: "pytorch_libtorch"
max_batch_size: 16
input [
  {
    name: "TEXT"
    data_type: TYPE_STRING
    dims: [ 1 ]
  }
]
output [
  {
    name: "ENTITIES"
    data_type: TYPE_STRING
    dims: [ -1, 3 ]  # Entity text, type, confidence
  }
]
```

## Example Triton Model Repository Structure

```
model_repository/
├── summarization/
│   ├── 1/
│   │   └── model.pt
│   └── config.pbtxt
└── entity_extraction/
    ├── 1/
    │   └── model.pt
    └── config.pbtxt
```

## Error Handling

- The service implements retry logic for transient failures
- Failed messages are retried with exponential backoff
- Permanent failures are logged and reported via metrics

## Contributing

Contributions are welcome! Please ensure your code follows the project's style guidelines and includes appropriate tests.
</file>

<file path="microservices/python/personalization-engine/app/config.py">
from functools import lru_cache
from typing import Dict, Any, Optional, List
import os
from pydantic import BaseModel, Field
from dotenv import load_dotenv

# Load environment variables from .env file if it exists
load_dotenv()

class Settings(BaseModel):
    """Application settings loaded from environment variables."""
    
    # Service configuration
    service_name: str = Field(default="personalization-engine")
    environment: str = Field(default=os.getenv("ENVIRONMENT", "development"))
    log_level: str = Field(default=os.getenv("LOG_LEVEL", "INFO"))
    
    # API configuration
    host: str = Field(default=os.getenv("HOST", "0.0.0.0"))
    port: int = Field(default=int(os.getenv("PORT", "8002")))
    
    # Vector embedding configuration
    embedding_model: str = Field(default=os.getenv("EMBEDDING_MODEL", "sentence-transformers/all-MiniLM-L6-v2"))
    embedding_dimension: int = Field(default=int(os.getenv("EMBEDDING_DIMENSION", "384")))
    
    # Redis Vector DB configuration
    redis_url: str = Field(default=os.getenv("REDIS_URL", "redis://localhost:6379"))
    redis_index_name: str = Field(default=os.getenv("REDIS_INDEX_NAME", "chimera-vectors"))
    redis_user_index_name: str = Field(default=os.getenv("REDIS_USER_INDEX_NAME", "chimera-users"))
    
    # NATS configuration
    nats_url: str = Field(default=os.getenv("NATS_URL", "nats://localhost:4222"))
    nats_user: Optional[str] = Field(default=os.getenv("NATS_USER"))
    nats_password: Optional[str] = Field(default=os.getenv("NATS_PASSWORD"))
    
    # Content personalization settings
    default_content_types: List[str] = Field(
        default=["research_paper", "news_article", "repository", "tutorial"]
    )
    default_recommendation_limit: int = Field(default=10)
    
    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"
        case_sensitive = False

@lru_cache()
def get_settings() -> Settings:
    """Create cached settings instance."""
    return Settings()
</file>

<file path="microservices/python/personalization-engine/app/main.py">
from contextlib import asynccontextmanager
import asyncio
import json
import os
from typing import Dict, List, Optional, Any, Union
from uuid import uuid4

from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from loguru import logger
import numpy as np

from app.config import Settings, get_settings
from app.models import (
    ContentItem,
    UserProfile,
    PersonalizedRecommendation,
    RecommendationRequest,
    UserProfileUpdate,
    SearchQuery,
    SearchResult,
    HealthResponse,
    ContentVectorization,
    BatchOperation
)
from app.vector_store import VectorStore
from app.nats_client import NatsClient

# Global clients
vector_store: Optional[VectorStore] = None
nats_client: Optional[NatsClient] = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application startup and shutdown events."""
    global vector_store, nats_client
    settings = get_settings()
    
    # Initialize Vector Store
    logger.info(f"Initializing Vector Store")
    vector_store = VectorStore(
        model_name=settings.embedding_model,
        dimension=settings.embedding_dimension,
        redis_url=settings.redis_url,
    )
    await vector_store.initialize()
    
    # Initialize NATS client
    logger.info(f"Connecting to NATS at {settings.nats_url}")
    nats_client = NatsClient(
        nats_url=settings.nats_url,
        user=settings.nats_user,
        password=settings.nats_password,
    )
    await nats_client.connect()
    
    # Initialize NATS streams and subscriptions
    if nats_client.js:
        try:
            # Set up subscriptions for NLP enriched data
            async def process_enriched_data(msg):
                try:
                    data = json.loads(msg.data.decode())
                    logger.info(f"Processing enriched data for vectorization: {data.get('id', 'unknown')}")
                    
                    # Extract content information for vectorization
                    await process_content_for_vectorization(data)
                    
                    await msg.ack()
                except Exception as e:
                    logger.error(f"Error processing enriched message: {e}")
                    await msg.nak(delay=5)
            
            # Subscribe to NLP enriched data
            await nats_client.subscribe(
                "nlp.enriched.*", 
                queue="personalization-engine-processors",
                callback=process_enriched_data
            )
            
            logger.info("NATS subscriptions configured")
        except Exception as e:
            logger.error(f"Failed to configure NATS streams/subscriptions: {e}")
    
    # Start background processors
    asyncio.create_task(health_check_loop())
    
    logger.info("Personalization Engine service started")
    
    yield
    
    # Cleanup
    if nats_client:
        await nats_client.close()
        logger.info("NATS connection closed")
    
    if vector_store:
        await vector_store.close()
        logger.info("Vector Store closed")
    
    logger.info("Personalization Engine service shutdown")

app = FastAPI(
    title="Chimera Personalization Engine",
    description="Manages user profiles and content recommendations using vector embeddings",
    version="0.1.0",
    lifespan=lifespan,
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

async def health_check_loop():
    """Background task to check system health periodically."""
    while True:
        try:
            vector_store_healthy = vector_store and await vector_store.is_healthy()
            nats_healthy = nats_client and nats_client.is_connected()
            
            if not vector_store_healthy:
                logger.warning("Vector Store unhealthy")
                if vector_store:
                    try:
                        await vector_store.initialize()
                    except Exception as e:
                        logger.error(f"Failed to reinitialize Vector Store: {e}")
            
            if not nats_healthy:
                logger.warning("NATS connection unhealthy")
                if nats_client:
                    try:
                        await nats_client.connect()
                    except Exception as e:
                        logger.error(f"Failed to reconnect to NATS: {e}")
                        
        except Exception as e:
            logger.error(f"Error in health check loop: {e}")
            
        await asyncio.sleep(30)  # Check every 30 seconds

async def process_content_for_vectorization(data: Dict[str, Any]):
    """Process content from enriched data for vectorization.
    
    Args:
        data: The enriched data containing content to vectorize
    """
    if not vector_store:
        logger.error("Vector Store not initialized")
        return
    
    try:
        # Extract content information
        content_id = data.get("id", str(uuid4()))
        content_type = data.get("content_type", "unknown")
        
        # Prepare text for vectorization
        texts = []
        
        # Add title if available
        if title := data.get("payload", {}).get("title"):
            texts.append(title)
        
        # Add summary if available
        if summary := data.get("nlp_enrichment", {}).get("summary"):
            texts.append(summary)
        
        # Add full text if available (limited to reduce embedding size)
        if full_text := data.get("payload", {}).get("text", "")[:2000]:
            texts.append(full_text)
        
        # Combine texts with proper separation
        text_to_vectorize = " ".join(texts)
        
        if not text_to_vectorize:
            logger.warning(f"No text available for vectorization for content {content_id}")
            return
        
        # Extract metadata
        metadata = {
            "id": content_id,
            "content_type": content_type,
            "source": data.get("source", "unknown"),
            "timestamp": data.get("timestamp"),
            "title": data.get("payload", {}).get("title", "Untitled"),
            "entities": [e.get("text") for e in data.get("nlp_enrichment", {}).get("entities", [])]
        }
        
        # Generate embedding and store in vector database
        await vector_store.add_item(
            item_id=content_id,
            text=text_to_vectorize,
            metadata=metadata
        )
        
        logger.info(f"Vectorized content {content_id} ({content_type})")
        
        # Publish to content.vectorized subject
        if nats_client:
            await nats_client.publish(
                f"content.vectorized.{content_type}",
                json.dumps({
                    "id": content_id,
                    "content_type": content_type,
                    "vectorized_at": str(np.datetime64('now')),
                })
            )
        
    except Exception as e:
        logger.error(f"Error vectorizing content: {e}")
        raise

@app.get("/health")
async def health_check() -> HealthResponse:
    """Health check endpoint."""
    vector_store_healthy = vector_store and await vector_store.is_healthy()
    nats_healthy = nats_client and nats_client.is_connected()
    
    return HealthResponse(
        service="personalization-engine",
        status="healthy" if (vector_store_healthy and nats_healthy) else "degraded",
        components={
            "vector_store": "connected" if vector_store_healthy else "disconnected",
            "nats": "connected" if nats_healthy else "disconnected",
        },
        version="0.1.0"
    )

@app.post("/users", response_model=UserProfile)
async def create_user_profile(
    profile: UserProfileUpdate,
    settings: Settings = Depends(get_settings)
) -> UserProfile:
    """Create a new user profile."""
    if not vector_store:
        raise HTTPException(500, "Vector Store not initialized")
    
    try:
        # Generate a new user ID if not provided
        user_id = profile.user_id or str(uuid4())
        
        # Generate embedding for interests and preferences
        interests_text = " ".join([
            *profile.interests,
            *profile.expertise_areas,
            profile.role or "",
            profile.background or "",
        ])
        
        # Store user profile
        await vector_store.add_user(
            user_id=user_id,
            interests=interests_text,
            preferences=profile.preferences,
            role=profile.role,
            metadata={
                "expertise_level": profile.expertise_level,
                "background": profile.background,
                "expertise_areas": profile.expertise_areas,
            }
        )
        
        # Return the created profile
        return UserProfile(
            user_id=user_id,
            interests=profile.interests,
            preferences=profile.preferences,
            role=profile.role,
            expertise_level=profile.expertise_level,
            background=profile.background,
            expertise_areas=profile.expertise_areas,
            created_at=str(np.datetime64('now')),
            updated_at=str(np.datetime64('now')),
        )
    
    except Exception as e:
        logger.error(f"Error creating user profile: {e}")
        raise HTTPException(500, f"Failed to create user profile: {str(e)}")

@app.put("/users/{user_id}", response_model=UserProfile)
async def update_user_profile(
    user_id: str,
    profile: UserProfileUpdate,
    settings: Settings = Depends(get_settings)
) -> UserProfile:
    """Update an existing user profile."""
    if not vector_store:
        raise HTTPException(500, "Vector Store not initialized")
    
    try:
        # Check if user exists
        user = await vector_store.get_user(user_id)
        if not user:
            raise HTTPException(404, f"User {user_id} not found")
        
        # Generate embedding for interests and preferences
        interests_text = " ".join([
            *profile.interests,
            *profile.expertise_areas,
            profile.role or "",
            profile.background or "",
        ])
        
        # Update user profile
        await vector_store.update_user(
            user_id=user_id,
            interests=interests_text,
            preferences=profile.preferences,
            role=profile.role,
            metadata={
                "expertise_level": profile.expertise_level,
                "background": profile.background,
                "expertise_areas": profile.expertise_areas,
            }
        )
        
        # Return the updated profile
        return UserProfile(
            user_id=user_id,
            interests=profile.interests,
            preferences=profile.preferences,
            role=profile.role,
            expertise_level=profile.expertise_level,
            background=profile.background,
            expertise_areas=profile.expertise_areas,
            created_at=user.get("created_at", str(np.datetime64('now'))),
            updated_at=str(np.datetime64('now')),
        )
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error updating user profile: {e}")
        raise HTTPException(500, f"Failed to update user profile: {str(e)}")

@app.get("/users/{user_id}", response_model=UserProfile)
async def get_user_profile(
    user_id: str,
    settings: Settings = Depends(get_settings)
) -> UserProfile:
    """Get a user profile by ID."""
    if not vector_store:
        raise HTTPException(500, "Vector Store not initialized")
    
    try:
        # Get user profile
        user = await vector_store.get_user(user_id)
        if not user:
            raise HTTPException(404, f"User {user_id} not found")
        
        # Extract metadata
        metadata = user.get("metadata", {})
        
        # Return the profile
        return UserProfile(
            user_id=user_id,
            interests=user.get("interests", []),
            preferences=user.get("preferences", {}),
            role=user.get("role"),
            expertise_level=metadata.get("expertise_level"),
            background=metadata.get("background"),
            expertise_areas=metadata.get("expertise_areas", []),
            created_at=user.get("created_at", str(np.datetime64('now'))),
            updated_at=user.get("updated_at", str(np.datetime64('now'))),
        )
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error retrieving user profile: {e}")
        raise HTTPException(500, f"Failed to retrieve user profile: {str(e)}")

@app.post("/recommendations", response_model=List[PersonalizedRecommendation])
async def get_recommendations(
    request: RecommendationRequest,
    settings: Settings = Depends(get_settings)
) -> List[PersonalizedRecommendation]:
    """Get personalized content recommendations for a user."""
    if not vector_store:
        raise HTTPException(500, "Vector Store not initialized")
    
    try:
        # Get user profile
        user = await vector_store.get_user(request.user_id)
        if not user:
            raise HTTPException(404, f"User {request.user_id} not found")
        
        # Set query parameters
        content_types = request.content_types or ["research_paper", "news_article", "repository"]
        limit = min(request.limit or 10, 50)  # Cap at 50 items
        
        # Find similar content based on user embedding
        results = await vector_store.find_similar_to_user(
            user_id=request.user_id,
            content_types=content_types,
            limit=limit
        )
        
        # Convert to response model
        recommendations = []
        for item in results:
            metadata = item.get("metadata", {})
            score = item.get("score", 0.0)
            
            recommendations.append(PersonalizedRecommendation(
                content_id=item.get("id"),
                content_type=metadata.get("content_type", "unknown"),
                title=metadata.get("title", "Untitled"),
                source=metadata.get("source", "unknown"),
                relevance_score=float(score),
                timestamp=metadata.get("timestamp"),
                entity_matches=[e for e in metadata.get("entities", []) if e in user.get("interests", [])]
            ))
        
        return recommendations
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error generating recommendations: {e}")
        raise HTTPException(500, f"Failed to generate recommendations: {str(e)}")

@app.post("/search", response_model=SearchResult)
async def semantic_search(
    query: SearchQuery,
    settings: Settings = Depends(get_settings)
) -> SearchResult:
    """Perform semantic search across content."""
    if not vector_store:
        raise HTTPException(500, "Vector Store not initialized")
    
    try:
        # Set query parameters
        content_types = query.content_types or ["research_paper", "news_article", "repository"]
        limit = min(query.limit or 10, 50)  # Cap at 50 items
        
        # Get user profile if provided
        user_embedding = None
        if query.user_id:
            user = await vector_store.get_user(query.user_id)
            if user:
                user_embedding = user.get("embedding")
        
        # Perform hybrid search (text query + optional user embedding)
        results = await vector_store.search(
            query=query.query,
            content_types=content_types,
            limit=limit,
            user_embedding=user_embedding,
            user_weight=query.personalization_weight or 0.3
        )
        
        # Convert to response items
        items = []
        for item in results:
            metadata = item.get("metadata", {})
            score = item.get("score", 0.0)
            
            items.append(ContentItem(
                content_id=item.get("id"),
                content_type=metadata.get("content_type", "unknown"),
                title=metadata.get("title", "Untitled"),
                source=metadata.get("source", "unknown"),
                relevance_score=float(score),
                timestamp=metadata.get("timestamp"),
                entities=metadata.get("entities", [])
            ))
        
        return SearchResult(
            query=query.query,
            items=items,
            total=len(items)
        )
    
    except Exception as e:
        logger.error(f"Error performing search: {e}")
        raise HTTPException(500, f"Search failed: {str(e)}")

@app.post("/vectorize", response_model=ContentVectorization)
async def vectorize_content(
    content: ContentVectorization,
    settings: Settings = Depends(get_settings)
) -> ContentVectorization:
    """Manually vectorize content for recommendation engine."""
    if not vector_store:
        raise HTTPException(500, "Vector Store not initialized")
    
    try:
        # Generate a content ID if not provided
        content_id = content.content_id or str(uuid4())
        
        # Add to vector store
        await vector_store.add_item(
            item_id=content_id,
            text=content.text,
            metadata={
                "id": content_id,
                "content_type": content.content_type,
                "title": content.title,
                "source": content.source,
                "timestamp": str(np.datetime64('now')),
                "entities": content.entities or []
            }
        )
        
        # Update the response
        content.content_id = content_id
        content.processed_at = str(np.datetime64('now'))
        
        return content
    
    except Exception as e:
        logger.error(f"Error vectorizing content: {e}")
        raise HTTPException(500, f"Vectorization failed: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    
    port = int(os.getenv("PORT", "8002"))
    host = os.getenv("HOST", "0.0.0.0")
    
    uvicorn.run("app.main:app", host=host, port=port, reload=True)
</file>

<file path="microservices/python/personalization-engine/app/models.py">
from typing import Dict, List, Optional, Any, Union
from pydantic import BaseModel, Field
from uuid import uuid4
from datetime import datetime

class ContentItem(BaseModel):
    """Content item for recommendations and search results."""
    content_id: str = Field(..., description="Unique identifier for the content")
    content_type: str = Field(..., description="Type of content (e.g., research_paper, news_article)")
    title: str = Field(..., description="Title of the content")
    source: str = Field(..., description="Source of the content")
    relevance_score: float = Field(..., description="Relevance score for the content (0.0-1.0)")
    timestamp: Optional[str] = Field(None, description="Timestamp when the content was created/published")
    entities: Optional[List[str]] = Field(None, description="Entities mentioned in the content")

class PersonalizedRecommendation(ContentItem):
    """Personalized content recommendation for a user."""
    entity_matches: Optional[List[str]] = Field(default_factory=list, description="Entities that match user interests")

class UserProfileUpdate(BaseModel):
    """Request model for updating a user profile."""
    user_id: Optional[str] = Field(None, description="Unique identifier for the user (generated if not provided)")
    role: Optional[str] = Field(None, description="User's role (e.g., engineer, executive, investor, learner)")
    interests: List[str] = Field(default_factory=list, description="User's interests and topics of interest")
    preferences: Dict[str, Any] = Field(default_factory=dict, description="User's content preferences")
    expertise_level: Optional[str] = Field(None, description="User's expertise level (beginner, intermediate, expert)")
    background: Optional[str] = Field(None, description="User's background or industry")
    expertise_areas: List[str] = Field(default_factory=list, description="User's areas of expertise")

class UserProfile(UserProfileUpdate):
    """Complete user profile with generated fields."""
    user_id: str = Field(..., description="Unique identifier for the user")
    created_at: str = Field(..., description="Timestamp when the profile was created")
    updated_at: str = Field(..., description="Timestamp when the profile was last updated")

class RecommendationRequest(BaseModel):
    """Request model for getting personalized recommendations."""
    user_id: str = Field(..., description="User ID to get recommendations for")
    content_types: Optional[List[str]] = Field(None, description="Types of content to recommend")
    limit: Optional[int] = Field(None, description="Maximum number of recommendations to return")

class SearchQuery(BaseModel):
    """Request model for semantic search."""
    query: str = Field(..., description="Search query text")
    content_types: Optional[List[str]] = Field(None, description="Types of content to search")
    limit: Optional[int] = Field(None, description="Maximum number of results to return")
    user_id: Optional[str] = Field(None, description="User ID for personalized search results")
    personalization_weight: Optional[float] = Field(None, description="Weight of personalization (0.0-1.0)")

class SearchResult(BaseModel):
    """Response model for search results."""
    query: str = Field(..., description="Original search query")
    items: List[ContentItem] = Field(..., description="Search result items")
    total: int = Field(..., description="Total number of results")

class ContentVectorization(BaseModel):
    """Request/response model for content vectorization."""
    content_id: Optional[str] = Field(None, description="Content ID (generated if not provided)")
    content_type: str = Field(..., description="Type of content")
    title: str = Field(..., description="Title of the content")
    text: str = Field(..., description="Text to vectorize")
    source: Optional[str] = Field(None, description="Source of the content")
    entities: Optional[List[str]] = Field(None, description="Entities in the content")
    processed_at: Optional[str] = Field(None, description="Timestamp when the content was processed")

class EntityBatch(BaseModel):
    """Batch of entities."""
    entities: List[str] = Field(..., description="List of entities")

class BatchOperation(BaseModel):
    """Base model for batch operations."""
    ids: List[str] = Field(..., description="List of IDs to operate on")

class HealthResponse(BaseModel):
    """Health check response model."""
    service: str = Field(..., description="Service name")
    status: str = Field(..., description="Service status (healthy, degraded, unhealthy)")
    components: Dict[str, str] = Field(default_factory=dict, description="Status of individual components")
    version: str = Field(..., description="Service version")
</file>

<file path="microservices/python/personalization-engine/docs/vector_store_backup_strategy.md">
# Vector Store Backup & Restore Strategy

## Overview

This document outlines the backup and restore strategy for the AI-Driven Personalization Engine's Vector Store component, which manages both Redis metadata and FAISS indices. The implementation provides a robust, consistent backup mechanism with configurable retention policies and verification capabilities.

## Architecture

### Components

1. **Redis Storage**

   - Stores metadata and vector data in JSON format
   - Uses RDB snapshots for point-in-time backups
   - Keys prefixed with `chimera-vectors:` and `chimera-users:`

2. **FAISS Indices**

   - In-memory vector indices for fast similarity search
   - Two separate indices:
     - Content index: For content embeddings
     - User index: For user profile embeddings
   - Serialized to disk during backup

3. **Backup Storage**
   - Primary: Local filesystem (`/tmp/vector_store_backup` by default)
   - Secondary: S3 bucket (optional) with server-side encryption
   - Structured backup format with metadata

### Backup Method

1. **Backup Process**

   ```
   /tmp/vector_store_backup/
   └── {backup_id}/
       ├── metadata.json       # Backup metadata and verification info
       ├── content_index.faiss # Serialized content vector index
       └── user_index.faiss    # Serialized user vector index
   ```

2. **Consistency Guarantees**
   - Atomic Redis RDB snapshot
   - Consistent FAISS index state capture
   - Metadata validation during restore
   - Size and integrity verification

### Backup Schedule & Retention

1. **Frequency**

   - Daily backups: Every 24 hours
   - Weekly backups: Every Monday
   - Monthly backups: First day of each month

2. **Retention Policy**

   - Daily backups: 7 days
   - Weekly backups: 4 weeks
   - Monthly backups: 6 months

3. **Storage & Security**
   - Local storage for fast recovery
   - S3 with AES-256 encryption for durability
   - Automatic cleanup of expired backups

## Recovery Process

### Recovery Time Objective (RTO)

- Target: < 5 minutes for local restore
- Target: < 15 minutes for S3 restore

### Recovery Point Objective (RPO)

- Target: < 24 hours (based on daily backup frequency)
- Configurable based on business requirements

### Restore Procedure

1. Download backup from S3 (if needed)
2. Verify backup metadata and integrity
3. Load FAISS indices from serialized files
4. Verify index sizes match metadata
5. Reconnect to Redis (loads RDB automatically)
6. Verify Redis key counts
7. Resume normal operation

## Monitoring & Verification

### Backup Monitoring

- Success/failure metrics
- Backup size and duration
- S3 upload status
- Storage usage tracking

### Backup Verification

- Automatic integrity checks
- Index size validation
- Redis key count verification
- Manual verification command available

### Health Checks

- Backup freshness monitoring
- Storage space monitoring
- S3 connectivity checks
- Redis connectivity validation

## Implementation Details

### Key Methods

1. `backup()`

   ```python
   async def backup(
       self,
       backup_id: Optional[str] = None,
       upload_to_s3: bool = True
   ) -> str:
   ```

   - Creates timestamped backup
   - Triggers Redis RDB snapshot
   - Exports FAISS indices
   - Generates metadata
   - Optionally uploads to S3

2. `restore()`

   ```python
   async def restore(
       self,
       backup_id: str,
       download_from_s3: bool = True
   ) -> None:
   ```

   - Downloads backup if needed
   - Verifies metadata
   - Loads indices
   - Reconnects to Redis
   - Verifies data integrity

3. `verify_backup()`

   ```python
   async def verify_backup(
       self,
       backup_id: str
   ) -> Dict[str, Any]:
   ```

   - Checks backup integrity
   - Validates index sizes
   - Returns verification report

4. `cleanup_old_backups()`
   ```python
   async def cleanup_old_backups(
       self,
       retain_days: int = 7,
       retain_weekly: int = 4,
       retain_monthly: int = 6
   ) -> None:
   ```
   - Implements retention policy
   - Removes expired backups
   - Cleans up both local and S3 storage

### Configuration

```python
class VectorStore:
    def __init__(
        self,
        backup_dir: str = "/tmp/vector_store_backup",
        s3_bucket: Optional[str] = None,
        s3_prefix: str = "vector_store_backups"
    ):
```

### Error Handling

- Comprehensive error capture
- Automatic cleanup on failure
- Detailed error logging
- Retry mechanisms for S3 operations

## Usage Examples

1. **Create Backup**

   ```python
   backup_id = await vector_store.backup(upload_to_s3=True)
   ```

2. **Restore from Backup**

   ```python
   await vector_store.restore(
       backup_id="20240215_120000",
       download_from_s3=True
   )
   ```

3. **Verify Backup**

   ```python
   results = await vector_store.verify_backup("20240215_120000")
   ```

4. **List Available Backups**

   ```python
   backups = await vector_store.list_backups()
   ```

5. **Clean Up Old Backups**
   ```python
   await vector_store.cleanup_old_backups(
       retain_days=7,
       retain_weekly=4,
       retain_monthly=6
   )
   ```

## Operational Considerations

### Performance Impact

- Backup operations are non-blocking
- Redis RDB snapshot uses copy-on-write
- FAISS index serialization is memory-intensive
- S3 operations are async

### Resource Requirements

- Disk space: ~2x size of Redis + FAISS data
- Memory: Temporary spike during FAISS serialization
- Network: S3 upload/download bandwidth

### Security

- S3 server-side encryption (AES-256)
- IAM role-based access control
- Secure local file permissions
- No sensitive data in metadata

### Monitoring

- Prometheus metrics for backup operations
- Alerting on backup failures
- Storage usage monitoring
- Backup freshness tracking

## Future Enhancements

1. **Incremental Backups**

   - Implement delta backups for Redis
   - Track FAISS index changes

2. **Compression**

   - Add backup compression support
   - Optimize S3 transfer sizes

3. **Multi-Region Support**

   - Cross-region backup replication
   - Geographic redundancy

4. **Automated Testing**
   - Regular restore drills
   - Backup integrity validation
   - Performance benchmarking

## Conclusion

This backup strategy provides a robust foundation for data resilience in the Vector Store component. The implementation balances performance, reliability, and operational simplicity while meeting the specified RPO and RTO targets.
</file>

<file path="microservices/python/personalization-engine/tests/conftest.py">
import os
import sys

# Add the parent directory to Python path so that app module can be imported
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
</file>

<file path="microservices/python/personalization-engine/tests/test_vector_store.py">
import json
import os
from datetime import datetime, timezone
from unittest.mock import AsyncMock, Mock, patch

import faiss
import numpy as np
import pytest
import redis
from app.vector_store import VectorStore
from sentence_transformers import SentenceTransformer

# Test data
TEST_EMBEDDING_DIM = 384
TEST_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
TEST_REDIS_URL = "redis://localhost:6379"
TEST_INDEX_NAME = "test-vectors"
TEST_USER_INDEX_NAME = "test-users"
TEST_BACKUP_DIR = "/tmp/test_vector_store_backup"

@pytest.fixture
def mock_sentence_transformer():
    """Mock SentenceTransformer to avoid loading the actual model."""
    with patch('sentence_transformers.SentenceTransformer') as mock:
        # Configure mock to return consistent embeddings for testing
        mock_model = Mock()
        mock_model.encode.return_value = np.ones(TEST_EMBEDDING_DIM, dtype=np.float32)
        mock.return_value = mock_model
        yield mock

@pytest.fixture
def mock_redis():
    """Mock Redis client."""
    with patch('redis.from_url') as mock:
        mock_redis = AsyncMock()
        mock_redis.ping.return_value = True
        mock_redis.keys.return_value = []
        mock_redis.get.return_value = None
        mock_redis.pipeline.return_value = AsyncMock()
        mock.return_value = mock_redis
        yield mock_redis

@pytest.fixture
async def vector_store(mock_sentence_transformer, mock_redis):
    """Create a VectorStore instance with mocked dependencies."""
    store = VectorStore(
        model_name=TEST_MODEL_NAME,
        dimension=TEST_EMBEDDING_DIM,
        redis_url=TEST_REDIS_URL,
        index_name=TEST_INDEX_NAME,
        user_index_name=TEST_USER_INDEX_NAME,
        backup_dir=TEST_BACKUP_DIR
    )
    await store.initialize()
    return store

@pytest.mark.asyncio
async def test_initialization(vector_store, mock_redis):
    """Test VectorStore initialization."""
    assert vector_store.initialized
    assert vector_store.model is not None
    assert vector_store.redis is not None
    assert vector_store.content_index is not None
    assert vector_store.user_index is not None
    assert isinstance(vector_store.content_index, faiss.IndexFlatIP)
    assert isinstance(vector_store.user_index, faiss.IndexFlatIP)

@pytest.mark.asyncio
async def test_generate_embedding(vector_store):
    """Test embedding generation."""
    test_text = "This is a test text"
    embedding = await vector_store._generate_embedding(test_text)
    
    assert isinstance(embedding, np.ndarray)
    assert embedding.shape == (TEST_EMBEDDING_DIM,)
    assert embedding.dtype == np.float32

@pytest.mark.asyncio
async def test_add_item(vector_store, mock_redis):
    """Test adding an item to the vector store."""
    item_id = "test_item_1"
    text = "Test item content"
    metadata = {"type": "test", "category": "sample"}
    
    # Configure mock Redis to store the item
    mock_redis.set.return_value = True
    
    await vector_store.add_item(item_id, text, metadata)
    
    # Verify Redis interaction
    mock_redis.set.assert_called_once()
    call_args = mock_redis.set.call_args[0]
    assert call_args[0] == f"{TEST_INDEX_NAME}:{item_id}"
    
    # Verify stored data structure
    stored_data = json.loads(call_args[1])
    assert stored_data["id"] == item_id
    assert stored_data["metadata"] == metadata
    assert "embedding" in stored_data
    assert "timestamp" in stored_data
    
    # Verify FAISS index update
    assert vector_store.content_index.ntotal == 1

@pytest.mark.asyncio
async def test_add_user(vector_store, mock_redis):
    """Test adding a user to the vector store."""
    user_id = "test_user_1"
    interests = "AI, Machine Learning, Python"
    preferences = {"theme": "dark", "language": "en"}
    role = "developer"
    metadata = {"country": "US"}
    
    # Configure mock Redis
    mock_redis.set.return_value = True
    
    await vector_store.add_user(user_id, interests, preferences, role, metadata)
    
    # Verify Redis interaction
    mock_redis.set.assert_called_once()
    call_args = mock_redis.set.call_args[0]
    assert call_args[0] == f"{TEST_USER_INDEX_NAME}:{user_id}"
    
    # Verify stored data structure
    stored_data = json.loads(call_args[1])
    assert stored_data["id"] == user_id
    assert stored_data["interests"] == interests
    assert stored_data["preferences"] == preferences
    assert stored_data["role"] == role
    assert stored_data["metadata"] == metadata
    assert "embedding" in stored_data
    assert "timestamp" in stored_data
    
    # Verify FAISS index update
    assert vector_store.user_index.ntotal == 1

@pytest.mark.asyncio
async def test_backup_restore(vector_store, mock_redis, tmp_path):
    """Test backup and restore functionality."""
    # Setup test data
    backup_dir = str(tmp_path / "backups")
    vector_store.backup_dir = backup_dir
    os.makedirs(backup_dir, exist_ok=True)
    
    # Add test data
    await vector_store.add_item("test_item", "Test content", {"type": "test"})
    await vector_store.add_user("test_user", "Test interests", {"pref": "test"})
    
    # Create backup
    backup_id = await vector_store.backup(upload_to_s3=False)
    assert os.path.exists(os.path.join(backup_dir, backup_id))
    
    # Clear indices
    vector_store.content_index = faiss.IndexFlatIP(TEST_EMBEDDING_DIM)
    vector_store.user_index = faiss.IndexFlatIP(TEST_EMBEDDING_DIM)
    
    # Restore from backup
    await vector_store.restore(backup_id, download_from_s3=False)
    
    # Verify restoration
    assert vector_store.content_index.ntotal == 1
    assert vector_store.user_index.ntotal == 1

@pytest.mark.asyncio
async def test_find_similar_to_user(vector_store, mock_redis):
    """Test finding similar content for a user."""
    # Setup test data
    user_id = "test_user"
    user_data = {
        "id": user_id,
        "embedding": np.ones(TEST_EMBEDDING_DIM).tolist(),
        "interests": "test interests"
    }
    mock_redis.get.return_value = json.dumps(user_data)
    
    # Add some test content
    await vector_store.add_item("item1", "Test content 1", {"type": "article"})
    await vector_store.add_item("item2", "Test content 2", {"type": "video"})
    
    # Find similar content
    results = await vector_store.find_similar_to_user(user_id, content_types=["article"], limit=2)
    
    assert isinstance(results, list)
    assert len(results) <= 2
    for result in results:
        assert "id" in result
        assert "similarity" in result
        assert "metadata" in result

@pytest.mark.asyncio
async def test_health_check(vector_store, mock_redis):
    """Test health check functionality."""
    # Test when healthy
    mock_redis.ping.return_value = True
    assert await vector_store.is_healthy()
    
    # Test when Redis is down
    mock_redis.ping.side_effect = redis.ConnectionError()
    assert not await vector_store.is_healthy()

@pytest.mark.asyncio
async def test_error_handling(vector_store):
    """Test error handling in various scenarios."""
    # Test uninitialized state
    vector_store.initialized = False
    with pytest.raises(RuntimeError):
        await vector_store.add_item("test", "content", {})
    
    # Test model not initialized
    vector_store.initialized = True
    vector_store.model = None
    with pytest.raises(RuntimeError):
        await vector_store._generate_embedding("test")
    
    # Test invalid backup restore
    with pytest.raises(FileNotFoundError):
        await vector_store.restore("nonexistent_backup", download_from_s3=False)

@pytest.mark.asyncio
async def test_cleanup(vector_store, mock_redis):
    """Test cleanup and resource management."""
    # Test close method
    await vector_store.close()
    mock_redis.close.assert_called_once()
    
    # Verify metrics are updated
    assert vector_store.content_index.ntotal == 0
    assert vector_store.user_index.ntotal == 0
</file>

<file path="microservices/python/personalization-engine/Dockerfile">
FROM nvidia/cuda:12.1.0-runtime-ubuntu22.04 as base

# Set working directory
WORKDIR /app

# Install Python and system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3.10 \
    python3-pip \
    python3-dev \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# Set Python to use UTF-8 encoding
ENV LANG=C.UTF-8 \
    LC_ALL=C.UTF-8 \
    PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1

# Install Python dependencies
COPY requirements.txt .
RUN pip3 install --no-cache-dir -r requirements.txt

# Copy application code
COPY app /app/app

# Create non-root user for security
RUN useradd -m -s /bin/bash appuser
RUN chown -R appuser:appuser /app
USER appuser

# Configure environment variables
ENV PORT=8002 \
    HOST=0.0.0.0 \
    PYTHONPATH=/app \
    LOG_LEVEL=INFO

# Expose port
EXPOSE 8002

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8002/health || exit 1

# Start the application
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8002"]
</file>

<file path="microservices/python/personalization-engine/README.md">
# Personalization Engine

A microservice that manages user profiles and content recommendations using vector embeddings for semantic search and personalized content delivery.

## Features

- User profile management with interest tracking
- Vector-based content recommendations
- Semantic search capabilities
- Automatic content vectorization from NLP-enriched data
- Real-time personalization based on user activity
- Integration with NATS for event streaming
- Redis-backed vector database for fast similarity search

## Components

- **Vector Store**: Redis-backed vector database for embeddings
- **NATS Client**: Handles message streaming for content updates
- **User Profile Manager**: Tracks and updates user preferences
- **Recommendation Engine**: Generates personalized content recommendations
- **FastAPI Server**: Provides REST API endpoints

## API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/health` | GET | Health check endpoint |
| `/users` | POST | Create a new user profile |
| `/users/{user_id}` | GET | Get user profile by ID |
| `/users/{user_id}` | PUT | Update user profile |
| `/users/{user_id}` | DELETE | Delete user profile |
| `/recommendations/{user_id}` | GET | Get personalized recommendations |
| `/search` | POST | Perform semantic search |
| `/vectorize` | POST | Manually vectorize content |
| `/users/{user_id}/activity` | POST | Update user activity |

## Message Processing

The service subscribes to the `nlp.enriched.*` NATS subject to automatically process and vectorize new content:

1. Receives NLP-enriched content from the ML Orchestrator
2. Extracts text, summaries, and metadata
3. Generates vector embeddings for the content
4. Stores embeddings in the vector database with metadata
5. Updates recommendations based on new content

## Vector Store

The vector store component:

- Uses Redis as the backend database
- Supports approximate nearest neighbor (ANN) search
- Enables efficient storage and retrieval of high-dimensional vectors
- Handles metadata storage alongside vectors
- Provides filtering capabilities for recommendation generation

## User Profiles

User profiles include:

- Basic user information
- Interest vectors (derived from content interactions)
- Explicit preferences (topics, sources, etc.)
- Interaction history
- Recommendation history to prevent duplicates

## Requirements

- Python a3.8+
- Redis Stack 6.2+ (with RediSearch and RedisJSON modules)
- NATS Server 2.2+
- Dependencies listed in `requirements.txt`

## Installation

```bash
pip install -r requirements.txt
```

## Configuration

The service is configured using environment variables:

| Variable | Description | Default |
|----------|-------------|---------|
| `REDIS_URL` | URL for Redis connection | `redis://localhost:6379` |
| `NATS_URL` | URL for NATS connection | `nats://localhost:4222` |
| `NATS_USER` | NATS username | - |
| `NATS_PASSWORD` | NATS password | - |
| `EMBEDDING_MODEL` | Name of embedding model | `text-embedding-3-small` |
| `EMBEDDING_DIMENSION` | Dimension of embeddings | `1536` |
| `LOG_LEVEL` | Logging level | `info` |

## Usage

### Running Locally

```bash
uvicorn app.main:app --host 0.0.0.0 --port 8000
```

### Using Docker

```bash
docker build -t personalization-engine .
docker run -p 8000:8000 personalization-engine
```

## Recommendation Algorithm

The recommendation engine uses a multi-step algorithm:

1. **User Interest Representation**:
   - Computes the user's interest vector from content interactions
   - Weighs recent interactions more heavily
   - Incorporates explicit preferences

2. **Candidate Generation**:
   - Uses vector similarity search to find content similar to user interests
   - Includes diversity mechanisms to avoid recommendation bubbles
   - Filters out previously seen content

3. **Ranking**:
   - Ranks candidates based on similarity, recency, and popularity
   - Applies personalized weighting based on user behavior
   - Supports customizable ranking strategies

## Semantic Search

The semantic search feature:

- Converts natural language queries to vectors
- Finds semantically similar content regardless of exact keyword matches
- Supports filtering by metadata (date, source, type, etc.)
- Returns ranked results with relevance scores

## Example Usage

### Get Recommendations

```bash
curl -X GET "http://localhost:8000/recommendations/user123?limit=10&content_type=research_paper"
```

### Semantic Search

```bash
curl -X POST "http://localhost:8000/search" \
     -H "Content-Type: application/json" \
     -d '{"query": "advances in natural language processing", "limit": 5}'
```

## Performance Considerations

- Vector operations are computationally intensive
- The service is designed for horizontal scaling
- Redis persistence should be configured for production use
- Consider using a dedicated Redis instance for vector storage

## Contributing

Contributions are welcome! Please ensure your code follows the project's style guidelines and includes appropriate tests.
</file>

<file path="microservices/python/personalization-engine/requirements-test.txt">
pytest==8.0.0
pytest-asyncio==0.23.5
pytest-cov==4.1.0
numpy==1.26.3
faiss-cpu==1.7.4
redis==5.0.1
sentence-transformers==2.2.2
</file>

<file path="microservices/rust/ingestion-service/src/config.rs">
use std::env;
use tracing::warn;

/// Application configuration loaded from environment variables
#[derive(Debug, Clone)]
pub struct AppConfig {
    /// Port to listen on
    pub port: u16,
    
    /// NATS server URL
    pub nats_url: String,
    
    /// Environment name (development, staging, production)
    pub environment: String,
}

impl AppConfig {
    /// Load configuration from environment variables with defaults
    pub fn from_env() -> Self {
        let port = env::var("PORT")
            .ok()
            .and_then(|s| s.parse::<u16>().ok())
            .unwrap_or_else(|| {
                warn!("PORT environment variable not set or invalid, using default port 3000");
                3000
            });
            
        let nats_url = env::var("NATS_URL")
            .unwrap_or_else(|_| {
                warn!("NATS_URL environment variable not set, using default nats://localhost:4222");
                "nats://localhost:4222".to_string()
            });
            
        let environment = env::var("ENVIRONMENT")
            .unwrap_or_else(|_| {
                warn!("ENVIRONMENT environment variable not set, using default development");
                "development".to_string()
            });
            
        Self {
            port,
            nats_url,
            environment,
        }
    }
}
</file>

<file path="microservices/rust/ingestion-service/src/error.rs">
use axum::{
    http::StatusCode,
    response::{IntoResponse, Response},
    Json,
};
use serde_json::json;
use thiserror::Error;

/// Custom error types for the ingestion service
#[derive(Error, Debug)]
pub enum AppError {
    #[error("Failed to connect to NATS: {0}")]
    NatsConnectionError(String),
    
    #[error("Failed to publish message to NATS: {0}")]
    NatsPublishError(String),
    
    #[error("Invalid input data: {0}")]
    ValidationError(String),
    
    #[error("Internal server error: {0}")]
    InternalError(String),
}

/// Convert application errors into appropriate HTTP responses
impl IntoResponse for AppError {
    fn into_response(self) -> Response {
        let (status, error_message) = match self {
            AppError::NatsConnectionError(msg) => (
                StatusCode::SERVICE_UNAVAILABLE,
                msg,
            ),
            AppError::NatsPublishError(msg) => (
                StatusCode::INTERNAL_SERVER_ERROR,
                msg,
            ),
            AppError::ValidationError(msg) => (
                StatusCode::BAD_REQUEST,
                msg,
            ),
            AppError::InternalError(msg) => (
                StatusCode::INTERNAL_SERVER_ERROR,
                msg,
            ),
        };

        let body = Json(json!({
            "error": {
                "message": error_message,
                "code": status.as_u16()
            }
        }));

        (status, body).into_response()
    }
}

/// Helper type for Result with AppError as error type
pub type Result<T> = std::result::Result<T, AppError>;
</file>

<file path="microservices/rust/ingestion-service/src/main.rs">
mod models;
mod error;
mod nats;
mod routes;
mod config;

use std::sync::Arc;
use axum::{
    routing::{post, get},
    Router,
    extract::Extension,
    http::{HeaderValue, Method},
};
use tower_http::{
    trace::TraceLayer,
    cors::{CorsLayer, Any},
};
use tracing::{info, Level};
use tracing_subscriber::{layer::SubscriberExt, util::SubscriberInitExt};

use crate::config::AppConfig;
use crate::nats::NatsClient;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Initialize tracing
    tracing_subscriber::registry()
        .with(tracing_subscriber::EnvFilter::new(
            std::env::var("RUST_LOG").unwrap_or_else(|_| "info,tower_http=debug".into()),
        ))
        .with(tracing_subscriber::fmt::layer())
        .init();

    info!("Initializing Chimera Ingestion Service");
    
    // Load configuration
    let config = AppConfig::from_env();
    info!("Loaded configuration: {:#?}", config);

    // Initialize NATS connection
    let nats_client = NatsClient::new(&config.nats_url).await?;
    let nats_client = Arc::new(nats_client);

    // Build our application with a route
    let app = Router::new()
        .route("/health", get(routes::health_check))
        .route("/ingest", post(routes::ingest_data))
        .route("/ingest/batch", post(routes::ingest_batch))
        // Add middleware
        .layer(
            CorsLayer::new()
                .allow_origin(Any)
                .allow_methods([Method::GET, Method::POST])
                .allow_headers(Any)
        )
        .layer(TraceLayer::new_for_http())
        .layer(Extension(nats_client));

    // Run our app
    let addr = format!("0.0.0.0:{}", config.port);
    let listener = tokio::net::TcpListener::bind(&addr).await?;
    info!("Ingestion service listening on {}", addr);
    
    axum::serve(listener, app).await?;
    
    Ok(())
}
</file>

<file path="microservices/rust/ingestion-service/src/models.rs">
use serde::{Deserialize, Serialize};
use chrono::{DateTime, Utc};
use uuid::Uuid;

/// Represents raw data ingested into the system from various sources
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct RawData {
    /// Unique identifier for the data item
    #[serde(default = "Uuid::new_v4")]
    pub id: Uuid,
    
    /// Source of the data (e.g., "arxiv", "github", "news-api")
    pub source: String,
    
    /// Type of content (e.g., "research_paper", "code_repository", "news_article")
    pub content_type: String,
    
    /// The actual data payload, represented as arbitrary JSON
    pub payload: serde_json::Value,
    
    /// Timestamp when the data was ingested, defaults to current time
    #[serde(default = "Utc::now")]
    pub timestamp: DateTime<Utc>,
    
    /// Optional metadata about the data
    #[serde(default)]
    pub metadata: serde_json::Value,
}

/// Batch of raw data items to be ingested
#[derive(Debug, Serialize, Deserialize)]
pub struct BatchRawData {
    /// Collection of data items to ingest
    pub items: Vec<RawData>,
}

/// Response for successful ingestion
#[derive(Debug, Serialize, Deserialize)]
pub struct IngestResponse {
    /// Status of the operation
    pub status: String,
    
    /// ID of the ingested data item
    pub id: Uuid,
    
    /// Timestamp when the data was ingested
    pub timestamp: DateTime<Utc>,
}

/// Response for batch ingestion
#[derive(Debug, Serialize, Deserialize)]
pub struct BatchIngestResponse {
    /// Status of the operation
    pub status: String,
    
    /// Number of items successfully ingested
    pub count: usize,
    
    /// IDs of the ingested data items
    pub ids: Vec<Uuid>,
    
    /// Timestamp when the batch was processed
    pub timestamp: DateTime<Utc>,
}

/// Health check response
#[derive(Debug, Serialize, Deserialize)]
pub struct HealthResponse {
    /// Service name
    pub service: String,
    
    /// Service status
    pub status: String,
    
    /// Service version
    pub version: String,
    
    /// Timestamp of the health check
    pub timestamp: DateTime<Utc>,
}
</file>

<file path="microservices/rust/ingestion-service/src/nats.rs">
use async_nats::Client;
use serde::Serialize;
use tracing::{info, error, instrument};
use crate::error::{AppError, Result};

/// Client wrapper for NATS interactions
pub struct NatsClient {
    client: Client,
}

impl NatsClient {
    /// Create a new NATS client
    pub async fn new(url: &str) -> Result<Self> {
        info!("Connecting to NATS server at {}", url);
        
        let client = async_nats::connect(url)
            .await
            .map_err(|e| {
                error!("Failed to connect to NATS: {}", e);
                AppError::NatsConnectionError(e.to_string())
            })?;
        
        info!("Successfully connected to NATS");
        
        Ok(Self { client })
    }

    /// Publish a message to a NATS subject
    #[instrument(skip(self, payload), fields(subject = %subject))]
    pub async fn publish<T: Serialize>(&self, subject: &str, payload: &T) -> Result<()> {
        let payload = serde_json::to_vec(payload).map_err(|e| {
            error!("JSON serialization error: {}", e);
            AppError::InternalError(format!("JSON serialization error: {}", e))
        })?;
        
        info!("Publishing message to subject: {}", subject);
        
        self.client.publish(subject, payload.into())
            .await
            .map_err(|e| {
                error!("Failed to publish to NATS: {}", e);
                AppError::NatsPublishError(e.to_string())
            })?;
        
        info!("Successfully published message to {}", subject);
        
        Ok(())
    }
}
</file>

<file path="microservices/rust/ingestion-service/src/routes.rs">
use axum::{
    extract::{Json, Extension},
    http::StatusCode,
};
use chrono::Utc;
use tracing::{info, warn, error, instrument};
use std::sync::Arc;

use crate::models::{RawData, BatchRawData, IngestResponse, BatchIngestResponse, HealthResponse};
use crate::nats::NatsClient;
use crate::error::{Result, AppError};

/// Health check endpoint
#[instrument(skip_all)]
pub async fn health_check() -> Json<HealthResponse> {
    let response = HealthResponse {
        service: "ingestion-service".to_string(),
        status: "operational".to_string(),
        version: env!("CARGO_PKG_VERSION").to_string(),
        timestamp: Utc::now(),
    };
    
    Json(response)
}

/// Ingest a single data item
#[instrument(skip(nats_client, payload), fields(source = %payload.source, content_type = %payload.content_type))]
pub async fn ingest_data(
    Extension(nats_client): Extension<Arc<NatsClient>>,
    Json(payload): Json<RawData>,
) -> Result<(StatusCode, Json<IngestResponse>)> {
    info!("Processing ingestion request: id={}", payload.id);
    
    // Validate input
    if payload.source.is_empty() {
        warn!("Empty source field in ingestion request");
        return Err(AppError::ValidationError("Source field cannot be empty".to_string()));
    }
    
    if payload.content_type.is_empty() {
        warn!("Empty content_type field in ingestion request");
        return Err(AppError::ValidationError("Content type field cannot be empty".to_string()));
    }
    
    if payload.payload.is_null() {
        warn!("Empty payload in ingestion request");
        return Err(AppError::ValidationError("Payload cannot be null".to_string()));
    }
    
    // Determine the appropriate NATS subject based on content type
    let subject = format!("ingest.raw.{}", payload.content_type);
    
    // Publish to NATS
    nats_client.publish(&subject, &payload).await?;
    
    // Create response
    let response = IngestResponse {
        status: "success".to_string(),
        id: payload.id,
        timestamp: Utc::now(),
    };
    
    info!("Successfully ingested data with id: {}", payload.id);
    
    Ok((StatusCode::CREATED, Json(response)))
}

/// Batch ingest multiple data items
#[instrument(skip(nats_client, payload), fields(item_count = %payload.items.len()))]
pub async fn ingest_batch(
    Extension(nats_client): Extension<Arc<NatsClient>>,
    Json(payload): Json<BatchRawData>,
) -> Result<(StatusCode, Json<BatchIngestResponse>)> {
    info!("Processing batch ingestion request with {} items", payload.items.len());
    
    if payload.items.is_empty() {
        warn!("Empty batch in ingestion request");
        return Err(AppError::ValidationError("Batch contains no items".to_string()));
    }
    
    let mut successful_ids = Vec::with_capacity(payload.items.len());
    
    // Process each item
    for item in payload.items.iter() {
        // Validate item
        if item.source.is_empty() || item.content_type.is_empty() || item.payload.is_null() {
            error!("Invalid item in batch, id: {}", item.id);
            continue;
        }
        
        // Determine subject
        let subject = format!("ingest.raw.{}", item.content_type);
        
        // Publish to NATS
        match nats_client.publish(&subject, item).await {
            Ok(_) => {
                successful_ids.push(item.id);
                info!("Successfully published item {}", item.id);
            },
            Err(e) => {
                error!("Failed to publish item {}: {}", item.id, e);
                // Continue processing other items even if one fails
            }
        }
    }
    
    // Create response
    let response = BatchIngestResponse {
        status: "success".to_string(),
        count: successful_ids.len(),
        ids: successful_ids,
        timestamp: Utc::now(),
    };
    
    info!("Batch ingestion completed: {}/{} items successful", 
          response.count, payload.items.len());
    
    Ok((StatusCode::CREATED, Json(response)))
}
</file>

<file path="microservices/rust/ingestion-service/Cargo.toml">
[package]
name = "ingestion-service"
version = "0.1.0"
edition = "2021"

[dependencies]
axum = "0.7.2"
tokio = { version = "1.35.1", features = ["full"] }
serde = { version = "1.0.193", features = ["derive"] }
serde_json = "1.0.108"
async-nats = "0.33.0"
tower = "0.4.13"
tower-http = { version = "0.5.0", features = ["trace", "cors"] }
tracing = "0.1.40"
tracing-subscriber = { version = "0.3.18", features = ["env-filter"] }
thiserror = "1.0.56"
chrono = { version = "0.4.31", features = ["serde"] }
uuid = { version = "1.6.1", features = ["v4", "serde"] }
</file>

<file path="microservices/rust/ingestion-service/Dockerfile">
FROM rust:1.76-slim as builder

WORKDIR /app

# Create blank project
RUN USER=root cargo new --bin ingestion-service

# Copy manifest
COPY Cargo.toml ./

# Update the Cargo.toml in the dummy project
RUN sed -i 's/name = "ingestion-service"/name = "app"/' ingestion-service/Cargo.toml
COPY src ./src

# Build for release
RUN cargo build --release

# Runtime stage
FROM debian:bookworm-slim

WORKDIR /app

# Install dependencies
RUN apt-get update && \
    apt-get install -y --no-install-recommends ca-certificates && \
    rm -rf /var/lib/apt/lists/*

# Copy the binary from builder
COPY --from=builder /app/target/release/ingestion-service .

# Expose the service port
EXPOSE 8001

# Set environment variables
ENV PORT=8001
ENV RUST_LOG=info

# Run the binary
CMD ["./ingestion-service"]
</file>

<file path="microservices/rust/ingestion-service/README.md">
# Ingestion Service

A high-performance Rust microservice for data ingestion and validation, designed to efficiently process incoming data and publish it to NATS streams for further processing.

## Features

- Fast and efficient data ingestion using Rust and Axum
- Single-item and batch ingestion endpoints
- Validation and preprocessing of incoming data
- Publication to NATS streams for downstream processing
- Structured error handling with detailed responses
- Health check endpoint for monitoring
- CORS support for web clients

## Components

- **Axum Web Framework**: Provides HTTP server and routing
- **NATS Client**: Handles message publishing to NATS streams
- **Custom Error Handling**: Structured error types and responses
- **Data Models**: Type-safe request and response models

## API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/health` | GET | Health check endpoint |
| `/ingest` | POST | Single-item ingestion endpoint |
| `/ingest/batch` | POST | Batch ingestion endpoint |

## Request and Response Format

### Single-Item Ingestion

**Request:**
```json
{
  "id": "optional-custom-id",
  "source": "source-name",
  "content_type": "research_paper",
  "payload": {
    "title": "Example Research Paper",
    "text": "This is the content of the research paper...",
    "metadata": {
      "author": "John Doe",
      "published_date": "2023-03-15"
    }
  }
}
```

**Response (Success):**
```json
{
  "id": "generated-or-custom-id",
  "status": "success",
  "timestamp": "2023-03-16T10:15:30Z"
}
```

### Batch Ingestion

**Request:**
```json
{
  "items": [
    {
      "id": "item-1",
      "source": "source-name",
      "content_type": "research_paper",
      "payload": { ... }
    },
    {
      "id": "item-2",
      "source": "source-name",
      "content_type": "news_article",
      "payload": { ... }
    }
  ]
}
```

**Response (Success):**
```json
{
  "success_count": 2,
  "error_count": 0,
  "results": [
    {
      "id": "item-1",
      "status": "success",
      "timestamp": "2023-03-16T10:15:30Z"
    },
    {
      "id": "item-2",
      "status": "success",
      "timestamp": "2023-03-16T10:15:30Z"
    }
  ]
}
```

## NATS Message Format

The service publishes messages to NATS with the following format:

```json
{
  "id": "item-id",
  "source": "source-name",
  "content_type": "research_paper",
  "timestamp": "2023-03-16T10:15:30Z",
  "payload": {
    "title": "Example Research Paper",
    "text": "This is the content of the research paper...",
    "metadata": {
      "author": "John Doe",
      "published_date": "2023-03-15"
    }
  }
}
```

Messages are published to subjects following the pattern: `ingest.validated.{content_type}`

## Requirements

- Rust 1.60+ (2021 edition)
- NATS Server 2.2+

## Dependencies

Major dependencies include:
- `axum`: Web framework
- `tokio`: Async runtime
- `serde`: Serialization/deserialization
- `async-nats`: NATS client
- `tower-http`: Middleware for HTTP
- `tracing`: Logging and diagnostics

## Installation

1. Clone the repository
2. Build the service:

```bash
cargo build --release
```

## Configuration

The service is configured using environment variables:

| Variable | Description | Default |
|----------|-------------|---------|
| `NATS_URL` | URL for NATS connection | `nats://localhost:4222` |
| `PORT` | HTTP server port | `3000` |
| `RUST_LOG` | Logging level | `info,tower_http=debug` |

## Usage

### Running Locally

```bash
cargo run --release
```

### Using Docker

```bash
docker build -t ingestion-service .
docker run -p 3000:3000 ingestion-service
```

## Performance Considerations

- The service is designed for high throughput with asynchronous processing
- Batch ingestion should be preferred for high-volume data processing
- Connection pooling is used for NATS to reduce overhead
- Error handling is designed to be graceful under load

## Error Handling

The service provides structured error responses:

```json
{
  "error": "validation_error",
  "message": "Invalid payload format",
  "details": "field 'text' is required"
}
```

Common error types include:
- `validation_error`: Invalid request format or data
- `internal_error`: Server-side processing error
- `nats_error`: Error communicating with NATS

## Testing

Run the test suite with:

```bash
cargo test
```

## Contributing

Contributions are welcome! Please ensure your code follows the project's style guidelines and includes appropriate tests.
</file>

<file path="microservices/ARCHITECTURE.md">
# SolnAI Microservices Architecture

This document details the architectural design of the SolnAI microservices system, including service interactions, data flow, and system design considerations.

## 1. System Overview

SolnAI uses a microservices architecture to provide scalable, maintainable AI-powered data processing and personalization. The system is designed to ingest, process, analyze, and deliver content with AI capabilities.

### 1.1 High-level Architecture

```
┌───────────────┐     ┌───────────────┐     ┌───────────────┐     ┌───────────────┐
│               │     │               │     │               │     │               │
│    Client     │────▶│   Ingestion   │────▶│      ML       │────▶│   Knowledge   │
│  Applications │     │    Service    │     │  Orchestrator │────▶│     Graph     │
│               │     │               │     │               │     │               │
└───────────────┘     └───────────────┘     └───────────────┘     └───────────────┘
       │                                            │                     │
       │                                            │                     │
       │                                            ▼                     │
       │                                   ┌───────────────┐              │
       │                                   │               │              │
       │                                   │      GPU      │              │
       │                                   │   Monitoring  │              │
       └──────────────────────────────────│     Agent     │              │
                                          │               │              │
                                          └───────────────┘              │
                                                                         │
                                                                         ▼
                                                                ┌───────────────┐
                                                                │               │
                                                                │Personalization│
                                                                │    Engine     │
                                                                │               │
                                                                └───────────────┘
                                                                         ▲
                                                                         │
                                                                         │
                                                                         │
                                                                ┌───────────────┐
                                                                │               │
                                                                │    Client     │
                                                                │  Applications │
                                                                │               │
                                                                └───────────────┘
```

### 1.2 Core Design Principles

- **Event-Driven Architecture**: Asynchronous communication via NATS
- **Microservices Independence**: Loose coupling, high cohesion
- **Data Resilience**: Multi-tiered backup strategies
- **Scalability**: Horizontal scaling with stateful considerations
- **Observability**: Comprehensive monitoring and tracing

## 2. Technical Stack

### 2.1 Core Technologies

- **Languages**: Python 3.11+, Rust 1.75+
- **Message Queue**: NATS 2.10 with JetStream
- **Databases**:
  - Redis 7.2+ (Vector Store)
  - Neo4j 5.11+ (Knowledge Graph)
  - PostgreSQL 15+ (Metadata)
- **ML Infrastructure**: NVIDIA Triton 2.39+
- **Container Orchestration**: Kubernetes 1.28+

### 2.2 Infrastructure Components

- **Cloud Services**:
  - S3-compatible object storage
  - GPU-enabled compute nodes (NVIDIA H100)
  - Load balancers and ingress controllers
- **Monitoring Stack**:
  - Prometheus + Grafana
  - OpenTelemetry for tracing
  - Loki for log aggregation

## 3. Service Architecture

### 3.1 Ingestion Service (Rust)

#### Implementation Details

```rust
// Core message structure
pub struct ContentMessage {
    id: String,
    content_type: ContentType,
    payload: Vec<u8>,
    metadata: HashMap<String, String>,
    timestamp: DateTime<Utc>,
}

// NATS subject patterns
const SUBJECT_VALIDATED: &str = "ingest.validated.>";
const SUBJECT_ERROR: &str = "ingest.error.>";
```

#### Performance Characteristics

- Throughput: 10K messages/sec per instance
- Max message size: 1MB
- Batch size: 100 messages
- Processing latency: < 50ms

### 3.2 ML Orchestrator (Python)

#### Implementation Details

```python
class ModelRegistry:
    def __init__(self):
        self.models: Dict[str, ModelMetadata] = {}
        self.triton_client = TritonClient()

    async def load_model(self, model_id: str, version: str):
        # Model loading logic
        pass
```

#### Model Deployment Flow

1. Model artifact validation
2. Triton model repository update
3. Version transition (Blue/Green)
4. Health check verification

### 3.3 Knowledge Graph Service (Python)

#### Schema Definition

```cypher
// Core node types
CREATE CONSTRAINT ON (p:Person) ASSERT p.id IS UNIQUE;
CREATE CONSTRAINT ON (o:Organization) ASSERT o.id IS UNIQUE;
CREATE CONSTRAINT ON (t:Topic) ASSERT t.id IS UNIQUE;

// Core relationship types
(:Person)-[:WORKS_AT]->(:Organization)
(:Organization)-[:SPECIALIZES_IN]->(:Topic)
(:Person)-[:CONTRIBUTES_TO]->(:Topic)
```

#### Query Patterns

- Entity resolution: O(log n)
- Relationship traversal: Max depth 3
- Cache hit ratio target: 85%

### 3.4 Personalization Engine (Python)

#### Vector Store Implementation

```python
class VectorStore:
    def __init__(self):
        self.redis_client = Redis(decode_responses=True)
        self.faiss_index = faiss.IndexFlatL2(384)  # BERT embedding dim

    async def add_vectors(self, vectors: np.ndarray, metadata: List[dict]):
        # Atomic vector addition
        pass

    async def similarity_search(self, query_vector: np.ndarray, k: int = 10):
        # Approximate nearest neighbor search
        pass
```

#### Backup Strategy Implementation

```python
class VectorStoreBackup:
    def __init__(self):
        self.s3_client = boto3.client('s3')
        self.metrics = PrometheusMetrics()

    async def create_backup(self):
        # 1. Create atomic RDB snapshot
        # 2. Serialize FAISS index
        # 3. Upload to S3 with encryption
        pass
```

## 4. Data Flow & Communication

### 4.1 Message Patterns

| Pattern | Schema                                           | Example                                                    |
| ------- | ------------------------------------------------ | ---------------------------------------------------------- |
| Command | `{action: string, payload: object}`              | `{"action": "process_content", "payload": {...}}`          |
| Event   | `{type: string, data: object, metadata: object}` | `{"type": "content_processed", "data": {...}}`             |
| Query   | `{query_type: string, parameters: object}`       | `{"query_type": "similarity_search", "parameters": {...}}` |

### 4.2 NATS Configuration

```yaml
jetstream:
  max_memory_store: 1G
  max_file_store: 8G
  store_dir: /data
  domain: chimera

tls:
  cert_file: /etc/nats/certs/server.crt
  key_file: /etc/nats/certs/server.key
  ca_file: /etc/nats/certs/ca.crt
  verify: true
  timeout: 5
```

## 5. Resilience Implementation

### 5.1 Circuit Breaker Pattern

```python
class CircuitBreaker:
    def __init__(self):
        self.failure_count = 0
        self.state = CircuitState.CLOSED
        self.last_failure = None

    async def execute(self, func: Callable):
        if self.state == CircuitState.OPEN:
            if self._should_retry():
                return await self._try_half_open(func)
            raise CircuitOpenError()

        try:
            result = await func()
            self._on_success()
            return result
        except Exception as e:
            self._on_failure()
            raise
```

### 5.2 Backup Verification

```python
class BackupVerifier:
    async def verify_backup(self, backup_id: str) -> VerificationResult:
        # 1. Check metadata integrity
        metadata = await self._load_metadata(backup_id)
        if not self._verify_checksums(metadata):
            return VerificationResult.CHECKSUM_MISMATCH

        # 2. Verify FAISS index
        if not await self._verify_faiss_index(backup_id):
            return VerificationResult.INDEX_CORRUPTED

        # 3. Verify Redis snapshot
        if not await self._verify_redis_snapshot(backup_id):
            return VerificationResult.SNAPSHOT_CORRUPTED

        return VerificationResult.SUCCESS
```

## 6. Monitoring Implementation

### 6.1 Custom Metrics

```python
# Prometheus metric definitions
VECTOR_STORE_METRICS = {
    'backup_duration': Histogram(
        'vector_store_backup_duration_seconds',
        'Time taken to complete backup',
        buckets=[1, 5, 10, 30, 60, 120, 300]
    ),
    'vector_count': Gauge(
        'vector_store_vector_count',
        'Total number of vectors in store'
    ),
    'operation_errors': Counter(
        'vector_store_operation_errors_total',
        'Total number of operation errors',
        ['operation_type']
    )
}
```

### 6.2 Health Checks

```python
class HealthCheck:
    async def check_health(self) -> HealthStatus:
        redis_health = await self._check_redis()
        faiss_health = await self._check_faiss()
        backup_health = await self._check_backup_status()

        return HealthStatus(
            status=all([redis_health, faiss_health, backup_health]),
            components={
                'redis': redis_health,
                'faiss': faiss_health,
                'backup': backup_health
            }
        )
```

## 7. Security Implementation

### 7.1 Authentication Flow

```python
class AuthenticationManager:
    async def authenticate_request(self, request: Request) -> AuthResult:
        token = self._extract_token(request)
        if not token:
            return AuthResult(valid=False, error='Missing token')

        try:
            payload = await self._verify_token(token)
            return AuthResult(valid=True, user_id=payload['sub'])
        except JWTError:
            return AuthResult(valid=False, error='Invalid token')
```

### 7.2 Authorization Policies

```yaml
policies:
  - name: vector-store-admin
    resources: ["vector-store/*"]
    actions: ["read", "write", "backup", "restore"]
    effect: allow

  - name: vector-store-reader
    resources: ["vector-store/vectors"]
    actions: ["read"]
    effect: allow
```

## 8. Deployment Configuration

### 8.1 Resource Requirements

| Service         | CPU     | Memory | Storage | GPU     |
| --------------- | ------- | ------ | ------- | ------- |
| Ingestion       | 2 cores | 4GB    | 20GB    | N/A     |
| ML Orchestrator | 4 cores | 16GB   | 50GB    | 1x H100 |
| Knowledge Graph | 8 cores | 32GB   | 100GB   | N/A     |
| Personalization | 4 cores | 16GB   | 50GB    | N/A     |

### 8.2 Scaling Thresholds

| Metric          | Warning | Critical | Auto-scale |
| --------------- | ------- | -------- | ---------- |
| CPU Usage       | 70%     | 85%      | Yes        |
| Memory Usage    | 75%     | 90%      | Yes        |
| Request Latency | 200ms   | 500ms    | Yes        |
| Error Rate      | 1%      | 5%       | No         |

## 9. Future Extensions

### 9.1 Planned Enhancements

- Incremental vector store backups
- Multi-region deployment support
- Real-time model updating
- Enhanced privacy controls

### 9.2 Research Areas

- Vector compression techniques
- Hybrid search algorithms
- Automated model optimization
- Enhanced knowledge graph reasoning
</file>

<file path="microservices/README.md">
# SolnAI Microservices

This directory contains the microservices that power the SolnAI platform, built using Python and Rust.

## Architecture Overview

SolnAI's microservices architecture consists of specialized services that work together to provide an end-to-end AI solution. The system includes data ingestion, processing, storage, and retrieval capabilities with a focus on AI/ML operations.

```
┌─────────────────┐     ┌────────────────┐     ┌────────────────────┐
│                 │     │                │     │                    │
│ Ingestion       │────>│ ML             │────>│ Knowledge Graph    │
│ Service (Rust)  │     │ Orchestrator   │     │ Service (Python)   │
│                 │     │ (Python)       │     │                    │
└─────────────────┘     └────────────────┘     └────────────────────┘
                                │                        │
                                │                        │
                                ▼                        ▼
                        ┌────────────────┐     ┌────────────────────┐
                        │                │     │                    │
                        │ GPU Monitoring │     │ Personalization    │
                        │ Agent (Python) │     │ Engine (Python)    │
                        │                │     │                    │
                        └────────────────┘     └────────────────────┘
```

## Microservices

### Python Microservices

#### GPU Monitoring Agent

**Purpose**: Monitors NVIDIA GPU metrics and exposes them for Prometheus scraping.

**Features**:
- Collects GPU utilization, memory usage, temperature, and power metrics
- Exposes metrics via a Prometheus-compatible endpoint
- Auto-discovers available GPUs via NVML
- Includes health check endpoint

**Endpoints**:
- `/metrics`: Prometheus metrics endpoint
- `/health`: Health check endpoint

**Tech Stack**:
- Flask for HTTP server
- NVIDIA Management Library (NVML) via pynvml
- Prometheus client for metrics

#### Knowledge Graph Service

**Purpose**: Manages a knowledge graph using Neo4j to store entities and relationships extracted from content.

**Features**:
- Entity and relationship management
- Graph querying capabilities
- Integration with NLP enrichment pipeline
- Batch operations for entities and relationships

**Components**:
- Neo4j client for database operations
- NATS client for message processing
- Entity and relationship modeling

**Endpoints**:
- `/health`: Health check endpoint
- `/entities`: Entity creation endpoint
- `/relationships`: Relationship creation endpoint
- `/query`: Graph query endpoint
- `/entities/batch`: Batch entity operations

**Tech Stack**:
- FastAPI for HTTP server
- Neo4j for graph database
- NATS for messaging

#### ML Orchestrator

**Purpose**: Orchestrates NLP processing tasks using Triton Inference Server.

**Features**:
- Text summarization
- Entity extraction
- Background message processing
- Integration with NATS message streams

**Components**:
- Triton client for model inference
- NATS client for message processing
- Checkpoint management for model versions

**Endpoints**:
- `/health`: Health check endpoint
- `/process`: General processing endpoint
- `/summarize`: Text summarization endpoint
- `/extract_entities`: Entity extraction endpoint

**Tech Stack**:
- FastAPI for HTTP server
- NVIDIA Triton Inference Server client
- NATS for messaging

#### Personalization Engine

**Purpose**: Manages user profiles and content recommendations using vector embeddings.

**Features**:
- User profile management
- Content vectorization
- Personalized content recommendations
- Semantic search capabilities

**Components**:
- Vector store for embedding storage and retrieval
- NATS client for processing enriched content
- User profile and content modeling

**Endpoints**:
- `/health`: Health check endpoint
- `/users`: User profile management endpoints
- `/recommendations`: Content recommendation endpoint
- `/search`: Semantic search endpoint
- `/vectorize`: Content vectorization endpoint

**Tech Stack**:
- FastAPI for HTTP server
- Redis for vector database
- NATS for messaging

### Rust Microservices

#### Ingestion Service

**Purpose**: High-performance service for ingesting data from various sources.

**Features**:
- Fast data ingestion with Rust
- Validation and preprocessing of incoming data
- Publication to NATS streams for further processing
- Batch ingestion support

**Components**:
- Axum web framework for API endpoints
- NATS client for message publishing
- Error handling and validation

**Endpoints**:
- `/health`: Health check endpoint
- `/ingest`: Single item ingestion endpoint
- `/ingest/batch`: Batch ingestion endpoint

**Tech Stack**:
- Axum for HTTP server
- async-nats for NATS client
- Tokio for async runtime

## Communication

The microservices communicate primarily through NATS (a high-performance messaging system):

1. **Data Flow**:
   - Ingestion Service → ML Orchestrator → Knowledge Graph/Personalization Engine
   - Messages follow subjects like `ingest.validated.*`, `nlp.enriched.*`

2. **Message Formats**:
   - JSON-based messages with standardized fields
   - Includes metadata, payload, and processing results

## Deployment

All services are containerized using Docker and can be deployed in Kubernetes:

- Services include health checks for monitoring
- Configuration via environment variables
- Resource-specific containers (e.g., GPU monitoring agent requires NVIDIA runtime)

Docker Compose and Kubernetes manifests are provided for deployment.

## System Requirements

- Python 3.9+ for Python microservices
- Rust 2021 edition for Rust microservices
- NATS server for messaging
- Neo4j database for knowledge graph
- Redis for vector storage
- NVIDIA GPUs with appropriate drivers for ML workloads

## Development Guide

### Environment Setup

Each service has its own dependencies and environment requirements. Please refer to the respective service directories for specific setup instructions.

### Building and Running Services

#### Python Services

1. Navigate to the service directory
2. Install dependencies: `pip install -r requirements.txt`
3. Run the service: `python -m app.main` or using the provided Dockerfile

#### Rust Services

1. Navigate to the service directory
2. Build the service: `cargo build --release`
3. Run the service: `cargo run --release` or using the provided Dockerfile

## Configuration

Each service is configurable via environment variables, with sensible defaults provided in many cases. The configuration parameters are defined in respective config files (e.g., `config.py` or `config.rs`).

## Security Considerations

- Services use authentication for external systems (Neo4j, NATS)
- CORS is configured for API endpoints
- No direct exposure of internal services is recommended

## Monitoring & Observability

- Prometheus metrics exposed by GPU monitoring agent
- Health check endpoints for all services
- Logging with configurable levels
- Background health check loops in critical services
</file>

<file path="scripts/end_to_end_workflow.py">
#!/usr/bin/env python3
"""
Chimera Platform End-to-End Workflow Demonstration
-------------------------------------------------
This script demonstrates a complete workflow through the Chimera platform:
1. Data ingestion via the Ingestion Service
2. ML processing via ML Orchestrator
3. Knowledge Graph enrichment
4. Personalized recommendations via Personalization Engine

Usage:
    python end_to_end_workflow.py [--config CONFIG_FILE]
"""

import argparse
import asyncio
import json
import logging
import os
import sys
import time
import uuid
from datetime import datetime
from typing import Dict, List, Any, Optional

import httpx
import coloredlogs

# Configure logging
logger = logging.getLogger("chimera-workflow")
coloredlogs.install(
    level="INFO",
    logger=logger,
    fmt="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)

# Default configuration
DEFAULT_CONFIG = {
    "ingestion_url": "http://localhost:8001",
    "ml_orchestrator_url": "http://localhost:8002",
    "knowledge_graph_url": "http://localhost:8003",
    "personalization_url": "http://localhost:8002",  # Same port as ML Orchestrator but different service
    "timeout": 60,
    "poll_interval": 2
}

class ChimeraWorkflow:
    """Orchestrates end-to-end workflows through Chimera platform."""
    
    def __init__(self, config: Dict[str, Any]):
        """Initialize the workflow with configuration."""
        self.config = config
        self.http_client = httpx.AsyncClient(timeout=config["timeout"])
        self.request_id = str(uuid.uuid4())
        self.user_id = f"demo-user-{uuid.uuid4().hex[:8]}"
    
    async def close(self):
        """Close resources."""
        await self.http_client.aclose()
    
    async def ingest_sample_content(self) -> List[str]:
        """Ingest sample content through the Ingestion Service.
        
        Returns:
            List of content IDs that were ingested
        """
        logger.info("⚡ Step 1: Ingesting sample content")
        
        # Sample research papers about AI technologies
        sample_content = [
            {
                "id": f"paper-{uuid.uuid4().hex[:8]}",
                "source": "research-database",
                "content_type": "research_paper",
                "timestamp": datetime.utcnow().isoformat(),
                "payload": {
                    "title": "Advanced Techniques in Neural Network Deployment for Production Systems",
                    "text": """
                    This research paper discusses state-of-the-art approaches for deploying neural networks 
                    in production environments. We explore techniques for model optimization, including 
                    quantization, pruning, and knowledge distillation. Additionally, we discuss deployment 
                    strategies across different hardware platforms, with a focus on NVIDIA GPU acceleration 
                    technologies. The paper presents benchmark results comparing inference performance
                    across various model architectures and optimization strategies.
                    """,
                    "authors": ["Dr. Alexandra Smith", "Dr. Michael Chen"],
                    "publication_date": "2025-02-15",
                    "url": "https://example.com/research/neural-networks-production"
                }
            },
            {
                "id": f"paper-{uuid.uuid4().hex[:8]}",
                "source": "research-database",
                "content_type": "research_paper",
                "timestamp": datetime.utcnow().isoformat(),
                "payload": {
                    "title": "Knowledge Graph Applications in Enterprise AI Systems",
                    "text": """
                    This paper explores the application of knowledge graphs in enterprise AI systems.
                    We discuss the integration of semantic networks with machine learning pipelines
                    to enhance reasoning capabilities and provide context-aware intelligence.
                    The research presents a novel architecture that combines Neo4j graph databases
                    with vector embeddings to create a hybrid semantic-neural system capable of
                    advanced inference across diverse data sources.
                    """,
                    "authors": ["Dr. Sarah Johnson", "Dr. David Williams"],
                    "publication_date": "2025-03-01",
                    "url": "https://example.com/research/knowledge-graph-applications"
                }
            },
            {
                "id": f"news-{uuid.uuid4().hex[:8]}",
                "source": "tech-news",
                "content_type": "news_article",
                "timestamp": datetime.utcnow().isoformat(),
                "payload": {
                    "title": "NVIDIA Announces Next-Generation GPU Architecture",
                    "text": """
                    NVIDIA today announced their next-generation GPU architecture, codenamed "Cascade,"
                    which promises significant improvements in AI training and inference performance.
                    The new architecture features enhanced Tensor Cores optimized for transformer models,
                    along with specialized hardware for graph neural networks. Industry analysts suggest
                    this development could accelerate AI model development cycles by up to 40% while
                    reducing energy consumption compared to previous generations.
                    """,
                    "author": "Tech Insider Team",
                    "publication_date": "2025-03-25",
                    "url": "https://example.com/news/nvidia-next-gen-gpu"
                }
            }
        ]
        
        content_ids = []
        for content in sample_content:
            try:
                logger.info(f"  📥 Ingesting: {content['payload']['title']}")
                response = await self.http_client.post(
                    f"{self.config['ingestion_url']}/ingest",
                    json=content
                )
                response.raise_for_status()
                result = response.json()
                content_ids.append(content["id"])
                logger.info(f"  ✅ Successfully ingested content with ID: {content['id']}")
            except httpx.HTTPError as e:
                logger.error(f"  ❌ Failed to ingest content: {str(e)}")
        
        return content_ids
    
    async def monitor_ml_processing(self, content_ids: List[str]) -> bool:
        """Monitor ML processing of ingested content.
        
        Args:
            content_ids: List of content IDs to monitor
            
        Returns:
            True if all content was processed successfully
        """
        logger.info("⚡ Step 2: Monitoring ML processing")
        
        # Wait for all content to be processed
        max_attempts = 30
        all_processed = False
        
        for attempt in range(max_attempts):
            processed_count = 0
            
            for content_id in content_ids:
                try:
                    response = await self.http_client.get(
                        f"{self.config['ml_orchestrator_url']}/status/{content_id}"
                    )
                    
                    if response.status_code == 200:
                        status = response.json()
                        if status.get("status") == "completed":
                            processed_count += 1
                            logger.info(f"  ✅ Content {content_id} processed successfully")
                        elif status.get("status") == "failed":
                            logger.error(f"  ❌ Processing failed for content {content_id}: {status.get('error')}")
                        else:
                            logger.info(f"  ⏳ Content {content_id} is still processing: {status.get('status')}")
                    elif response.status_code == 404:
                        # May be 404 if already processed and cleared from cache
                        processed_count += 1
                        logger.info(f"  ✅ Content {content_id} likely processed (not in processing queue)")
                except httpx.HTTPError as e:
                    logger.error(f"  ❌ Error checking status for {content_id}: {str(e)}")
            
            if processed_count == len(content_ids):
                all_processed = True
                break
            
            logger.info(f"  ⏳ Waiting for processing: {processed_count}/{len(content_ids)} complete")
            await asyncio.sleep(self.config["poll_interval"])
        
        if all_processed:
            logger.info("  ✅ All content processed successfully by ML Orchestrator")
        else:
            logger.warning("  ⚠️ Timed out waiting for all content to be processed")
        
        return all_processed
    
    async def verify_knowledge_graph(self, content_ids: List[str]) -> Dict[str, List[str]]:
        """Verify that content has been added to Knowledge Graph.
        
        Args:
            content_ids: List of content IDs to verify
            
        Returns:
            Dictionary mapping content IDs to lists of entity IDs
        """
        logger.info("⚡ Step 3: Verifying Knowledge Graph enrichment")
        
        content_entities = {}
        
        # Give Knowledge Graph time to process
        await asyncio.sleep(5)
        
        # Query entities for each content
        for content_id in content_ids:
            try:
                response = await self.http_client.get(
                    f"{self.config['knowledge_graph_url']}/entities",
                    params={"filter": f"source_id:{content_id}"}
                )
                response.raise_for_status()
                entities = response.json()
                
                if entities:
                    entity_ids = [entity["id"] for entity in entities]
                    content_entities[content_id] = entity_ids
                    logger.info(f"  ✅ Found {len(entities)} entities for content {content_id}")
                    
                    # Print a few entity examples
                    for i, entity in enumerate(entities[:3]):
                        logger.info(f"    📊 Entity: {entity['name']} (Type: {entity['type']})")
                else:
                    logger.warning(f"  ⚠️ No entities found for content {content_id}")
                    content_entities[content_id] = []
            except httpx.HTTPError as e:
                logger.error(f"  ❌ Error querying entities for {content_id}: {str(e)}")
                content_entities[content_id] = []
        
        # Get relationships between entities
        try:
            entity_ids = [eid for ids in content_entities.values() for eid in ids]
            if entity_ids:
                entity_filter = f"id:{entity_ids[0]}" + "".join([f" OR id:{eid}" for eid in entity_ids[1:]])
                response = await self.http_client.get(
                    f"{self.config['knowledge_graph_url']}/relationships",
                    params={"filter": entity_filter}
                )
                response.raise_for_status()
                relationships = response.json()
                
                if relationships:
                    logger.info(f"  ✅ Found {len(relationships)} relationships between entities")
                    # Print a few relationship examples
                    for i, rel in enumerate(relationships[:3]):
                        logger.info(f"    🔄 Relationship: {rel['start_entity']} --[{rel['type']}]--> {rel['end_entity']}")
                else:
                    logger.warning("  ⚠️ No relationships found between entities")
            else:
                logger.warning("  ⚠️ No entity IDs to query relationships")
        except httpx.HTTPError as e:
            logger.error(f"  ❌ Error querying relationships: {str(e)}")
        
        return content_entities
    
    async def create_user_profile(self) -> str:
        """Create a test user profile for personalization.
        
        Returns:
            User ID of the created profile
        """
        logger.info("⚡ Step 4: Creating user profile for personalization")
        
        user_profile = {
            "user_id": self.user_id,
            "role": "researcher",
            "interests": ["Neural Networks", "GPU Acceleration", "Knowledge Graphs", "Enterprise AI"],
            "preferences": {
                "content_types": ["research_paper", "news_article"],
                "delivery_frequency": "daily"
            },
            "expertise_level": "expert",
            "background": "Computer Science",
            "expertise_areas": ["Deep Learning", "Distributed Systems"]
        }
        
        try:
            response = await self.http_client.post(
                f"{self.config['personalization_url']}/users",
                json=user_profile
            )
            response.raise_for_status()
            profile = response.json()
            logger.info(f"  ✅ Created user profile with ID: {profile['user_id']}")
            logger.info(f"    👤 Role: {profile['role']}")
            logger.info(f"    🧠 Interests: {', '.join(profile['interests'])}")
            return profile["user_id"]
        except httpx.HTTPError as e:
            logger.error(f"  ❌ Failed to create user profile: {str(e)}")
            return self.user_id  # Return the generated ID even if creation failed
    
    async def get_personalized_recommendations(self, user_id: str) -> List[Dict[str, Any]]:
        """Get personalized recommendations for a user.
        
        Args:
            user_id: User ID to get recommendations for
            
        Returns:
            List of recommendation objects
        """
        logger.info("⚡ Step 5: Getting personalized recommendations")
        
        # Wait for content to be vectorized
        await asyncio.sleep(5)
        
        try:
            response = await self.http_client.post(
                f"{self.config['personalization_url']}/recommendations",
                json={
                    "user_id": user_id,
                    "limit": 10
                }
            )
            response.raise_for_status()
            recommendations = response.json()
            
            if recommendations:
                logger.info(f"  ✅ Retrieved {len(recommendations)} personalized recommendations")
                for i, rec in enumerate(recommendations[:5], 1):
                    logger.info(f"    📚 {i}. {rec['title']} ({rec['content_type']})")
                    logger.info(f"       Relevance: {rec['relevance_score']:.2f}")
                    if rec.get("entity_matches"):
                        logger.info(f"       Matching interests: {', '.join(rec['entity_matches'])}")
            else:
                logger.warning("  ⚠️ No recommendations found")
            
            return recommendations
        except httpx.HTTPError as e:
            logger.error(f"  ❌ Failed to get recommendations: {str(e)}")
            return []
    
    async def semantic_search(self, query: str, user_id: str) -> Dict[str, Any]:
        """Perform a semantic search with user context.
        
        Args:
            query: Search query
            user_id: User ID for personalization
            
        Returns:
            Search results
        """
        logger.info(f"⚡ Step 6: Performing semantic search for: '{query}'")
        
        try:
            response = await self.http_client.post(
                f"{self.config['personalization_url']}/search",
                json={
                    "query": query,
                    "user_id": user_id,
                    "personalization_weight": 0.3
                }
            )
            response.raise_for_status()
            results = response.json()
            
            if results["items"]:
                logger.info(f"  ✅ Found {results['total']} search results")
                for i, item in enumerate(results["items"][:5], 1):
                    logger.info(f"    🔍 {i}. {item['title']} ({item['content_type']})")
                    logger.info(f"       Relevance: {item['relevance_score']:.2f}")
            else:
                logger.warning("  ⚠️ No search results found")
            
            return results
        except httpx.HTTPError as e:
            logger.error(f"  ❌ Failed to perform search: {str(e)}")
            return {"items": [], "total": 0, "query": query}
    
    async def run_complete_workflow(self):
        """Run the complete end-to-end workflow."""
        logger.info("🚀 Starting Chimera Platform End-to-End Workflow")
        logger.info("===============================================")
        
        try:
            # 1. Ingest sample content
            content_ids = await self.ingest_sample_content()
            if not content_ids:
                logger.error("❌ Workflow failed: No content was ingested")
                return False
            
            # 2. Monitor ML processing
            ml_success = await self.monitor_ml_processing(content_ids)
            
            # 3. Verify Knowledge Graph enrichment
            content_entities = await self.verify_knowledge_graph(content_ids)
            
            # 4. Create user profile
            user_id = await self.create_user_profile()
            
            # 5. Get personalized recommendations
            recommendations = await self.get_personalized_recommendations(user_id)
            
            # 6. Perform semantic search
            search_results = await self.semantic_search("neural networks GPU optimization", user_id)
            
            # Workflow summary
            logger.info("===============================================")
            logger.info("📊 Workflow Summary:")
            logger.info(f"  📥 Content Ingested: {len(content_ids)}")
            logger.info(f"  🔄 ML Processing: {'Successful' if ml_success else 'Incomplete'}")
            
            entity_count = sum(len(entities) for entities in content_entities.values())
            logger.info(f"  📊 Knowledge Graph Entities: {entity_count}")
            
            logger.info(f"  👤 User Profile: {user_id}")
            logger.info(f"  📚 Recommendations: {len(recommendations)}")
            logger.info(f"  🔍 Search Results: {search_results['total']}")
            
            logger.info("===============================================")
            logger.info("✅ End-to-End Workflow Complete")
            
            return True
        except Exception as e:
            logger.error(f"❌ Unexpected error in workflow: {str(e)}")
            return False
        finally:
            await self.close()

async def main():
    """Main entry point for the workflow script."""
    parser = argparse.ArgumentParser(description="Chimera Platform End-to-End Workflow")
    parser.add_argument("--config", type=str, help="Path to configuration file")
    args = parser.parse_args()
    
    # Load configuration
    config = DEFAULT_CONFIG.copy()
    if args.config:
        try:
            with open(args.config, "r") as f:
                config.update(json.load(f))
        except (IOError, json.JSONDecodeError) as e:
            logger.error(f"Error loading configuration file: {str(e)}")
            sys.exit(1)
    
    # Run workflow
    workflow = ChimeraWorkflow(config)
    success = await workflow.run_complete_workflow()
    
    if not success:
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="scripts/nats_stream_setup.py">
#!/usr/bin/env python3
import asyncio
import nats
from nats.js.api import StreamConfig, RetentionPolicy, DiscardPolicy, StorageType
import os
import sys
import argparse
from loguru import logger

async def setup_streams(nats_servers, tls=False):
    # Set up TLS options if enabled
    tls_config = None
    if tls:
        tls_config = {
            "ca_file": os.environ.get("NATS_CA_FILE", "/etc/nats/certs/ca.crt"),
            "cert_file": os.environ.get("NATS_CERT_FILE", "/etc/nats/certs/client.crt"),
            "key_file": os.environ.get("NATS_KEY_FILE", "/etc/nats/certs/client.key")
        }
        logger.info(f"Using TLS with CA: {tls_config['ca_file']}")
    
    # Connect to NATS with TLS if configured
    nc = await nats.connect(
        servers=nats_servers.split(","),
        tls=tls_config
    )
    
    logger.info(f"Connected to NATS server at {nc.connected_url.netloc}")
    
    # Get JetStream context
    js = nc.jetstream()
    
    # Configure streams
    streams = [
        # Real-time inference stream with sharding for latency-critical workloads
        StreamConfig(
            name="REALTIME_INFERENCE",
            subjects=["chimera.inference.realtime.*"],
            retention=RetentionPolicy.LIMITS,
            max_age=86400000000000,  # 24 hours in nanoseconds
            storage=StorageType.FILE,
            num_replicas=3,
            discard=DiscardPolicy.OLD,
            max_bytes=10_000_000_000,  # 10GB
            duplicate_window=120000000000  # 2 minutes
        ),
        
        # Batch inference stream
        StreamConfig(
            name="BATCH_INFERENCE",
            subjects=["chimera.inference.batch.*"],
            retention=RetentionPolicy.LIMITS,
            max_age=604800000000000,  # 7 days in nanoseconds
            storage=StorageType.FILE,
            num_replicas=3,
            discard=DiscardPolicy.OLD,
            max_bytes=50_000_000_000,  # 50GB
        ),
        
        # Audit stream for compliance
        StreamConfig(
            name="AUDIT_LOGS",
            subjects=["chimera.audit.*"],
            retention=RetentionPolicy.LIMITS,
            max_age=7776000000000000,  # 90 days in nanoseconds
            storage=StorageType.FILE,
            num_replicas=3,
            discard=DiscardPolicy.NEW,  # Prevent dropping audit logs
            max_bytes=100_000_000_000,  # 100GB
        )
    ]
    
    # Create or update each stream
    for config in streams:
        try:
            # Try to add the stream
            stream = await js.add_stream(config=config)
            logger.info(f"Created stream {config.name}")
        except nats.js.errors.BadRequestError:
            # Stream exists, update it
            stream = await js.update_stream(config=config)
            logger.info(f"Updated stream {config.name}")
    
    await nc.close()
    logger.info("Stream setup complete")

def main():
    parser = argparse.ArgumentParser(description='Setup NATS JetStream streams for Chimera')
    parser.add_argument('--nats-servers', dest='nats_servers', 
                        default="nats://nats-0.nats:4222,nats://nats-1.nats:4222,nats://nats-2.nats:4222",
                        help='Comma-separated list of NATS server URLs')
    parser.add_argument('--tls', dest='tls', action='store_true',
                        help='Enable TLS for NATS connection')
    
    args = parser.parse_args()
    
    logger.info(f"Setting up streams with servers: {args.nats_servers}")
    asyncio.run(setup_streams(args.nats_servers, args.tls))

if __name__ == "__main__":
    main()
</file>

<file path="scripts/run_integration_tests.sh">
#!/bin/bash
# Chimera Platform Integration Test Runner
# Usage: ./run_integration_tests.sh [--mode=full|service|interservice|e2e] [--docker] [--env=ENV_FILE]

set -e

# Default values
MODE="full"
USE_DOCKER=0
ENV_FILE=""
PROJECT_ROOT=$(cd "$(dirname "$0")/.." && pwd)
TESTS_DIR="$PROJECT_ROOT/tests/integration"
DOCKER_COMPOSE_FILE="$TESTS_DIR/docker-compose.test.yml"

# Process arguments
for arg in "$@"; do
  case $arg in
    --mode=*)
      MODE="${arg#*=}"
      ;;
    --docker)
      USE_DOCKER=1
      ;;
    --env=*)
      ENV_FILE="${arg#*=}"
      ;;
    --help)
      echo "Chimera Platform Integration Test Runner"
      echo ""
      echo "Usage: ./run_integration_tests.sh [OPTIONS]"
      echo ""
      echo "Options:"
      echo "  --mode=MODE    Test mode to run (full, service, interservice, e2e)"
      echo "                 full: Run all tests"
      echo "                 service: Test individual services"
      echo "                 interservice: Test communication between services"
      echo "                 e2e: End-to-end workflow tests"
      echo "  --docker       Run tests in Docker containers"
      echo "  --env=FILE     Environment file to use"
      echo "  --help         Show this help message"
      exit 0
      ;;
    *)
      echo "Unknown option: $arg"
      echo "Use --help for usage information"
      exit 1
      ;;
  esac
done

# Validation
if [[ "$MODE" != "full" && "$MODE" != "service" && "$MODE" != "interservice" && "$MODE" != "e2e" ]]; then
  echo "Error: Invalid mode '$MODE'"
  echo "Valid modes: full, service, interservice, e2e"
  exit 1
fi

if [[ -n "$ENV_FILE" && ! -f "$ENV_FILE" ]]; then
  echo "Error: Environment file '$ENV_FILE' not found"
  exit 1
fi

# Determine test files to run based on mode
case $MODE in
  full)
    TEST_FILES="$TESTS_DIR"
    ;;
  service)
    TEST_FILES="$TESTS_DIR/test_service_*.py"
    ;;
  interservice)
    TEST_FILES="$TESTS_DIR/test_inter_service_comm.py"
    ;;
  e2e)
    TEST_FILES="$TESTS_DIR/test_e2e_flows.py"
    ;;
esac

# Print test configuration
echo "🚀 Running Chimera Platform Integration Tests"
echo "============================================"
echo "Mode:      $MODE"
echo "Docker:    $([ $USE_DOCKER -eq 1 ] && echo 'Yes' || echo 'No')"
if [[ -n "$ENV_FILE" ]]; then
  echo "Env File:  $ENV_FILE"
fi
echo "Tests:     $TEST_FILES"
echo "============================================"

# Set up environment if needed
if [[ -n "$ENV_FILE" ]]; then
  echo "📝 Loading environment variables from $ENV_FILE"
  export $(grep -v '^#' "$ENV_FILE" | xargs)
fi

# Function to run tests directly
run_local_tests() {
  echo "🧪 Running tests locally..."
  
  # Check if Python virtual environment exists, create if needed
  if [[ ! -d "$PROJECT_ROOT/venv" ]]; then
    echo "📦 Creating Python virtual environment..."
    python3 -m venv "$PROJECT_ROOT/venv"
    "$PROJECT_ROOT/venv/bin/pip" install -r "$TESTS_DIR/requirements.txt"
  fi
  
  # Activate virtual environment and run tests
  source "$PROJECT_ROOT/venv/bin/activate"
  python -m pytest $TEST_FILES -v --no-header
  
  return $?
}

# Function to run tests in Docker
run_docker_tests() {
  echo "🐳 Running tests in Docker..."
  
  # Make sure Docker is running
  if ! docker info &>/dev/null; then
    echo "❌ Error: Docker is not running or not accessible"
    exit 1
  fi
  
  # Build the test Docker image
  echo "🔨 Building test Docker image..."
  docker build -t chimera-integration-test -f "$TESTS_DIR/Dockerfile.test" "$TESTS_DIR"
  
  # Run the tests with Docker Compose
  echo "🚀 Starting test environment..."
  if [[ -n "$ENV_FILE" ]]; then
    docker-compose -f "$DOCKER_COMPOSE_FILE" --env-file "$ENV_FILE" up -d
  else
    docker-compose -f "$DOCKER_COMPOSE_FILE" up -d
  fi
  
  echo "⏳ Waiting for services to be ready..."
  sleep 10
  
  # Run the actual tests
  if [[ "$MODE" == "full" ]]; then
    docker-compose -f "$DOCKER_COMPOSE_FILE" exec test-runner pytest -xvs
  else
    docker-compose -f "$DOCKER_COMPOSE_FILE" exec test-runner pytest -xvs $TEST_FILES
  fi
  
  TEST_EXIT_CODE=$?
  
  # Clean up
  echo "🧹 Cleaning up Docker containers..."
  docker-compose -f "$DOCKER_COMPOSE_FILE" down
  
  return $TEST_EXIT_CODE
}

# Run tests based on configuration
if [[ $USE_DOCKER -eq 1 ]]; then
  run_docker_tests
else
  run_local_tests
fi

# Get exit code from test run
TEST_EXIT_CODE=$?

# Final output
echo "============================================"
if [[ $TEST_EXIT_CODE -eq 0 ]]; then
  echo "✅ All tests passed!"
else
  echo "❌ Tests failed with exit code $TEST_EXIT_CODE"
fi

exit $TEST_EXIT_CODE
</file>

<file path="tests/integration/mock_services/triton/mock_triton_server.py">
#!/usr/bin/env python3
"""
Mock Triton Inference Server for Integration Testing
Simulates Triton Inference Server responses for testing ML Orchestrator
"""
from http.server import HTTPServer, BaseHTTPRequestHandler
import json
import os
import time
from urllib.parse import urlparse, parse_qs
import threading

# Default port for the mock server
PORT = int(os.getenv("PORT", "8000"))

class MockTritonHandler(BaseHTTPRequestHandler):
    """Mock Triton Inference Server HTTP handler."""
    
    def _set_headers(self, status_code=200, content_type="application/json"):
        """Set response headers."""
        self.send_response(status_code)
        self.send_header("Content-Type", content_type)
        self.end_headers()
    
    def do_GET(self):
        """Handle GET requests."""
        parsed_path = urlparse(self.path)
        path = parsed_path.path
        
        # Health endpoint
        if path == "/v2/health/ready" or path == "/v2/health/live":
            self._set_headers(200)
            self.wfile.write(json.dumps({"status": "ready"}).encode())
            return
            
        # Server metadata
        elif path == "/v2":
            self._set_headers(200)
            self.wfile.write(json.dumps({
                "name": "mock_triton_server",
                "version": "2.30.0",
                "extensions": ["classification", "sequence", "model_repository"]
            }).encode())
            return
            
        # Model metadata
        elif path.startswith("/v2/models"):
            parts = path.split("/")
            if len(parts) >= 4:
                model_name = parts[3]
                
                # Model ready
                if path.endswith("/ready"):
                    self._set_headers(200)
                    self.wfile.write(json.dumps({"ready": True}).encode())
                    return
                    
                # Model metadata
                else:
                    self._set_headers(200)
                    self.wfile.write(json.dumps({
                        "name": model_name,
                        "versions": ["1"],
                        "platform": "pytorch",
                        "inputs": [
                            {"name": "input", "datatype": "FP32", "shape": [-1, 1024]}
                        ],
                        "outputs": [
                            {"name": "output", "datatype": "FP32", "shape": [-1, 1024]}
                        ]
                    }).encode())
                    return
        
        # Default - not found
        self._set_headers(404)
        self.wfile.write(json.dumps({"error": "Not found"}).encode())
    
    def do_POST(self):
        """Handle POST requests."""
        parsed_path = urlparse(self.path)
        path = parsed_path.path
        
        # Read request body
        content_length = int(self.headers.get("Content-Length", 0))
        request_body = self.rfile.read(content_length)
        
        try:
            request_json = json.loads(request_body)
        except json.JSONDecodeError:
            self._set_headers(400)
            self.wfile.write(json.dumps({"error": "Invalid JSON"}).encode())
            return
            
        # Model inference
        if path.startswith("/v2/models") and path.endswith("/infer"):
            parts = path.split("/")
            if len(parts) >= 4:
                model_name = parts[3]
                
                # Generate mock response based on model
                response = self._generate_mock_inference(model_name, request_json)
                
                self._set_headers(200)
                self.wfile.write(json.dumps(response).encode())
                return
        
        # Default - not found or not implemented
        self._set_headers(404)
        self.wfile.write(json.dumps({"error": "Not found or not implemented"}).encode())
    
    def _generate_mock_inference(self, model_name, request):
        """Generate mock inference response."""
        # Add a small delay to simulate processing
        time.sleep(0.1)
        
        if model_name == "nlp_enrichment":
            # NLP enrichment model
            return {
                "model_name": model_name,
                "model_version": "1",
                "outputs": [
                    {
                        "name": "output",
                        "datatype": "BYTES",
                        "shape": [1],
                        "data": [{
                            "entities": [
                                {"text": "AI", "label": "TECHNOLOGY", "score": 0.95},
                                {"text": "Neural Networks", "label": "TECHNOLOGY", "score": 0.92}
                            ],
                            "summary": "This is a summary of the input text.",
                            "sentiment": {"positive": 0.8, "negative": 0.1, "neutral": 0.1},
                            "keywords": ["AI", "technology", "research"],
                            "categories": ["Technology", "Science", "Research"]
                        }]
                    }
                ]
            }
        elif model_name == "embedding_model":
            # Vector embedding model
            vector_size = 384
            return {
                "model_name": model_name,
                "model_version": "1",
                "outputs": [
                    {
                        "name": "output",
                        "datatype": "FP32",
                        "shape": [1, vector_size],
                        "data": [0.1] * vector_size  # Mock embedding vector
                    }
                ]
            }
        elif model_name == "classification_model":
            # Classification model
            return {
                "model_name": model_name,
                "model_version": "1",
                "outputs": [
                    {
                        "name": "output",
                        "datatype": "BYTES",
                        "shape": [1],
                        "data": [{
                            "labels": ["technology", "science", "business"],
                            "scores": [0.8, 0.15, 0.05]
                        }]
                    }
                ]
            }
        
        # Default response
        return {
            "model_name": model_name,
            "model_version": "1",
            "outputs": [
                {
                    "name": "output",
                    "datatype": "BYTES",
                    "shape": [1],
                    "data": ["Mock inference result"]
                }
            ]
        }

def run_server(port):
    """Run the HTTP server."""
    server_address = ('', port)
    httpd = HTTPServer(server_address, MockTritonHandler)
    print(f"Starting mock Triton server on port {port}")
    httpd.serve_forever()

if __name__ == "__main__":
    run_server(PORT)
</file>

<file path="tests/integration/conftest.py">
import os
import pytest
import httpx
import asyncio
import json
from typing import Dict, List, Any, Optional, Callable
import nats
from nats.js.api import StreamConfig
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

# Service URLs from environment variables
INGESTION_URL = os.getenv("INGESTION_URL", "http://localhost:18001")
ML_ORCHESTRATOR_URL = os.getenv("ML_ORCHESTRATOR_URL", "http://localhost:18002")
KNOWLEDGE_GRAPH_URL = os.getenv("KNOWLEDGE_GRAPH_URL", "http://localhost:18003")
PERSONALIZATION_URL = os.getenv("PERSONALIZATION_URL", "http://localhost:18004")

# NATS configuration
NATS_URL = os.getenv("NATS_URL", "nats://nats_test:password@localhost:14222")

# Other service configurations
REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:16379")
NEO4J_URI = os.getenv("NEO4J_URI", "bolt://localhost:17687")
NEO4J_USER = os.getenv("NEO4J_USER", "neo4j")
NEO4J_PASSWORD = os.getenv("NEO4J_PASSWORD", "testpassword")

@pytest.fixture(scope="session")
def event_loop():
    """Create an instance of the default event loop for the test session."""
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()

@pytest.fixture(scope="session")
async def nats_client():
    """Connect to NATS and create a JetStream client."""
    client = await nats.connect(NATS_URL)
    
    # Create JetStream context
    js = client.jetstream()
    
    # Ensure test stream exists
    try:
        await js.stream_info("chimera_test")
    except nats.js.errors.NotFoundError:
        await js.add_stream(
            StreamConfig(
                name="chimera_test",
                subjects=[
                    "test.*",
                    "ingestion.raw.*",
                    "nlp.enriched.*",
                    "content.vectorized.*",
                    "user.profile.*",
                    "recommendation.request.*",
                ],
                retention="limits",
                max_age=3600,  # 1 hour retention for tests
                storage="memory",
                discard="old",
            )
        )
    
    yield client
    
    # Clean up
    await client.drain()

@pytest.fixture(scope="session")
async def http_client():
    """Create an async HTTP client for API testing."""
    async with httpx.AsyncClient(timeout=30.0) as client:
        yield client

@pytest.fixture
async def wait_for_services(http_client):
    """Wait for all services to be healthy before testing."""
    services = {
        "ingestion": f"{INGESTION_URL}/health",
        "ml_orchestrator": f"{ML_ORCHESTRATOR_URL}/health",
        "knowledge_graph": f"{KNOWLEDGE_GRAPH_URL}/health",
        "personalization": f"{PERSONALIZATION_URL}/health",
    }
    
    @retry(
        stop=stop_after_attempt(10),
        wait=wait_exponential(multiplier=1, min=1, max=10),
        retry=retry_if_exception_type((httpx.RequestError, httpx.HTTPStatusError)),
    )
    async def check_service(name, url):
        response = await http_client.get(url)
        response.raise_for_status()
        assert response.status_code == 200
        data = response.json()
        assert data["status"] in ["healthy", "ok"]
        return True
    
    results = []
    for name, url in services.items():
        try:
            result = await check_service(name, url)
            results.append((name, result))
        except Exception as e:
            results.append((name, False))
            raise RuntimeError(f"Service {name} not healthy: {str(e)}")
    
    return dict(results)

@pytest.fixture
async def clear_test_data():
    """Clear test data from all persistent storage."""
    # Function that will be executed after tests
    async def _clear():
        # Code to clear test data would go here
        # This could include:
        # - Deleting test records from Neo4j
        # - Clearing test keys from Redis
        # - Any other cleanup necessary
        pass
    
    # This will be executed before tests
    yield
    
    # This will be executed after tests
    await _clear()

# Helpers for test data generation
def generate_test_payload(data_type: str, **kwargs) -> Dict[str, Any]:
    """Generate test data payloads for different services."""
    if data_type == "raw_content":
        return {
            "id": kwargs.get("id", "test-content-1"),
            "source": kwargs.get("source", "test-source"),
            "content_type": kwargs.get("content_type", "research_paper"),
            "timestamp": kwargs.get("timestamp", "2025-03-31T23:25:36-04:00"),
            "payload": {
                "title": kwargs.get("title", "Test Research Paper"),
                "text": kwargs.get("text", "This is a test research paper about AI technology."),
                "authors": kwargs.get("authors", ["Test Author"]),
                "publication_date": kwargs.get("publication_date", "2025-03-31"),
                "url": kwargs.get("url", "https://example.com/test-paper"),
            }
        }
    elif data_type == "user_profile":
        return {
            "user_id": kwargs.get("user_id", "test-user-1"),
            "role": kwargs.get("role", "researcher"),
            "interests": kwargs.get("interests", ["AI", "Machine Learning", "Neural Networks"]),
            "preferences": kwargs.get("preferences", {"content_types": ["research_paper"]}),
            "expertise_level": kwargs.get("expertise_level", "expert"),
            "background": kwargs.get("background", "Computer Science"),
            "expertise_areas": kwargs.get("expertise_areas", ["Deep Learning", "Computer Vision"])
        }
    elif data_type == "entity":
        return {
            "name": kwargs.get("name", "Test Entity"),
            "type": kwargs.get("type", "Technology"),
            "properties": kwargs.get("properties", {"relevance": "high"}),
            "source": kwargs.get("source", "test")
        }
    
    raise ValueError(f"Unknown data type: {data_type}")
</file>

<file path="tests/integration/docker-compose.test.yml">
version: '3.8'

services:
  # Infrastructure services
  nats:
    image: nats:2.10.7-alpine
    command: "--jetstream --auth nats_test:password"
    ports:
      - "14222:4222"
    healthcheck:
      test: ["CMD", "nats-server", "check", "connection", "--host", "localhost", "--port", "4222"]
      interval: 5s
      timeout: 3s
      retries: 5

  redis:
    image: redislabs/redisearch:latest
    ports:
      - "16379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5

  neo4j:
    image: neo4j:5.13.0
    ports:
      - "17474:7474" # HTTP
      - "17687:7687" # Bolt
    environment:
      - NEO4J_AUTH=neo4j/testpassword
      - NEO4J_dbms_memory_heap_initial__size=512m
      - NEO4J_dbms_memory_heap_max__size=1G
    healthcheck:
      test: ["CMD", "wget", "http://localhost:7474", "-O", "/dev/null"]
      interval: 10s
      timeout: 5s
      retries: 5
  
  # Mock Triton server for testing
  mock-triton:
    image: python:3.10-slim
    volumes:
      - ./mock_services/triton:/app
    command: ["python", "/app/mock_triton_server.py"]
    ports:
      - "18000:8000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v2/health/ready"]
      interval: 5s
      timeout: 3s
      retries: 5

  # Application services
  ingestion-service:
    build:
      context: ../../microservices/rust/ingestion-service
      dockerfile: Dockerfile
    ports:
      - "18001:8001"
    environment:
      - PORT=8001
      - NATS_URL=nats://nats_test:password@nats:4222
      - LOG_LEVEL=debug
    depends_on:
      nats:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 10s
      timeout: 5s
      retries: 3

  ml-orchestrator:
    build:
      context: ../../microservices/python/ml-orchestrator
      dockerfile: Dockerfile
    ports:
      - "18002:8002"
    environment:
      - PORT=8002
      - NATS_URL=nats://nats_test:password@nats:4222
      - TRITON_URL=http://mock-triton:8000
      - LOG_LEVEL=debug
    depends_on:
      nats:
        condition: service_healthy
      mock-triton:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8002/health"]
      interval: 10s
      timeout: 5s
      retries: 3

  knowledge-graph:
    build:
      context: ../../microservices/python/knowledge-graph
      dockerfile: Dockerfile
    ports:
      - "18003:8003"
    environment:
      - PORT=8003
      - NATS_URL=nats://nats_test:password@nats:4222
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=testpassword
      - LOG_LEVEL=debug
    depends_on:
      nats:
        condition: service_healthy
      neo4j:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8003/health"]
      interval: 10s
      timeout: 5s
      retries: 3

  personalization-engine:
    build:
      context: ../../microservices/python/personalization-engine
      dockerfile: Dockerfile
    ports:
      - "18004:8002"
    environment:
      - PORT=8002
      - NATS_URL=nats://nats_test:password@nats:4222
      - REDIS_URL=redis://redis:6379
      - LOG_LEVEL=debug
    depends_on:
      nats:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8002/health"]
      interval: 10s
      timeout: 5s
      retries: 3

  # Test runner container
  test-runner:
    build:
      context: .
      dockerfile: Dockerfile.test
    volumes:
      - ./:/tests
      - ./results:/results
    environment:
      - NATS_URL=nats://nats_test:password@nats:4222
      - REDIS_URL=redis://redis:6379
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=testpassword
      - INGESTION_URL=http://ingestion-service:8001
      - ML_ORCHESTRATOR_URL=http://ml-orchestrator:8002
      - KNOWLEDGE_GRAPH_URL=http://knowledge-graph:8003
      - PERSONALIZATION_URL=http://personalization-engine:8002
    depends_on:
      ingestion-service:
        condition: service_healthy
      ml-orchestrator:
        condition: service_healthy
      knowledge-graph:
        condition: service_healthy
      personalization-engine:
        condition: service_healthy
    command: ["pytest", "-xvs", "--alluredir=/results"]
</file>

<file path="tests/integration/Dockerfile.test">
FROM python:3.10-slim

WORKDIR /tests

# Install system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    curl \
    gnupg \
    wget \
    git \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements
COPY requirements.txt .

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Create results directory
RUN mkdir -p /results

# Set Python to use UTF-8 encoding
ENV PYTHONIOENCODING=utf-8 \
    PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1

# Set working directory to tests
WORKDIR /tests

# Default command - can be overridden
CMD ["pytest", "-xvs"]
</file>

<file path="tests/integration/requirements.txt">
pytest==7.4.3
pytest-asyncio==0.21.1
pytest-cov==4.1.0
httpx==0.25.2
tenacity==8.2.3
docker-compose==1.29.2
nats-py==2.6.0
redis==5.0.1
neo4j==5.14.1
faiss-cpu==1.7.4
sentence-transformers==2.2.2
requests==2.31.0
pydantic==2.5.3
fastapi==0.108.0
allure-pytest==2.13.2
prometheus-client==0.19.0
opentelemetry-sdk==1.21.0
opentelemetry-exporter-otlp==1.21.0
</file>

<file path="tests/integration/run_services.sh">
#!/bin/bash
set -e

# Colors for terminal output
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m' # No Color

echo -e "${GREEN}Starting core infrastructure services...${NC}"
docker-compose -f docker-compose.test.yml up -d nats redis neo4j mock-triton

# Wait for services to be ready
echo -e "${YELLOW}Waiting for infrastructure services to be ready...${NC}"
sleep 10

# Set environment variables for local testing
export NATS_URL="nats://nats_test:password@localhost:14222"
export REDIS_URL="redis://localhost:16379"
export NEO4J_URI="bolt://localhost:17687"
export NEO4J_USER="neo4j"
export NEO4J_PASSWORD="testpassword"
export INGESTION_URL="http://localhost:18001"
export ML_ORCHESTRATOR_URL="http://localhost:18002"
export KNOWLEDGE_GRAPH_URL="http://localhost:18003"
export PERSONALIZATION_URL="http://localhost:18004"
export TRITON_URL="http://localhost:18000"

echo -e "${GREEN}Environment variables set:${NC}"
echo "NATS_URL=$NATS_URL"
echo "REDIS_URL=$REDIS_URL"
echo "NEO4J_URI=$NEO4J_URI"
echo "INGESTION_URL=$INGESTION_URL"
echo "ML_ORCHESTRATOR_URL=$ML_ORCHESTRATOR_URL"
echo "KNOWLEDGE_GRAPH_URL=$KNOWLEDGE_GRAPH_URL"
echo "PERSONALIZATION_URL=$PERSONALIZATION_URL"
echo "TRITON_URL=$TRITON_URL"

# Run the tests
echo -e "${GREEN}Running integration tests...${NC}"
pytest -xvs "$@"

# Cleanup
echo -e "${GREEN}Stopping services...${NC}"
docker-compose -f docker-compose.test.yml down
</file>

<file path="tests/integration/test_e2e_flows.py">
import pytest
import asyncio
import json
import uuid
import time
from typing import Dict, List, Any
import httpx
import allure

# Import helper functions from conftest
from conftest import (
    INGESTION_URL,
    ML_ORCHESTRATOR_URL,
    KNOWLEDGE_GRAPH_URL,
    PERSONALIZATION_URL,
    generate_test_payload
)

@allure.epic("Chimera Platform")
@allure.feature("End-to-End Data Flow")
class TestE2EDataFlow:
    """Test complete end-to-end data flows across all microservices."""
    
    @pytest.mark.asyncio
    @allure.story("Complete Content Pipeline")
    @allure.severity(allure.severity_level.CRITICAL)
    async def test_complete_data_pipeline(self, http_client, nats_client, wait_for_services, clear_test_data):
        """Test the complete data pipeline from ingestion to personalization."""
        # Validate all services are healthy
        assert wait_for_services
        
        # Step 1: Create a test user profile
        with allure.step("Create user profile"):
            user_id = f"test-user-{uuid.uuid4()}"
            user_payload = generate_test_payload("user_profile", user_id=user_id)
            
            response = await http_client.post(
                f"{PERSONALIZATION_URL}/users",
                json=user_payload
            )
            assert response.status_code == 200
            user_data = response.json()
            assert user_data["user_id"] == user_id
            assert len(user_data["interests"]) > 0
        
        # Step 2: Ingest test content
        with allure.step("Ingest test content"):
            content_id = f"test-content-{uuid.uuid4()}"
            content_payload = generate_test_payload(
                "raw_content", 
                id=content_id,
                title="Neural Networks in Production",
                text="This research paper discusses advanced neural network architectures for production systems. " +
                     "Deep learning models can be optimized for inference in various ways. " +
                     "The paper explores techniques for model compression and quantization."
            )
            
            response = await http_client.post(
                f"{INGESTION_URL}/ingest",
                json=content_payload
            )
            assert response.status_code == 202
            ingest_result = response.json()
            assert ingest_result["id"] == content_id
            assert ingest_result["status"] == "processing"
        
        # Step 3: Wait for the content to be processed through the pipeline
        with allure.step("Monitor NLP enrichment"):
            # Setup subscription to monitor processing
            nlp_result = None
            enrichment_complete = asyncio.Event()
            
            async def nlp_monitor(msg):
                nonlocal nlp_result
                data = json.loads(msg.data.decode())
                if data.get("id") == content_id:
                    nlp_result = data
                    enrichment_complete.set()
                await msg.ack()
            
            # Subscribe to NLP enrichment notifications
            sub = await nats_client.subscribe("nlp.enriched.*", cb=nlp_monitor)
            
            # Wait for enrichment to complete (with timeout)
            try:
                await asyncio.wait_for(enrichment_complete.wait(), timeout=30.0)
            except asyncio.TimeoutError:
                pytest.fail("Timed out waiting for NLP enrichment")
            finally:
                await sub.unsubscribe()
            
            assert nlp_result is not None
            assert nlp_result["id"] == content_id
            assert "nlp_enrichment" in nlp_result
        
        # Step 4: Validate knowledge graph processing
        with allure.step("Verify knowledge graph processing"):
            # Give the knowledge graph service time to process
            await asyncio.sleep(3)
            
            # Query the knowledge graph for the content entity
            response = await http_client.get(
                f"{KNOWLEDGE_GRAPH_URL}/entities",
                params={"filter": f"source_id:{content_id}"}
            )
            assert response.status_code == 200
            entities = response.json()
            assert len(entities) > 0
            
            # Check for expected entities
            entity_names = [e["name"] for e in entities]
            assert any(name.lower() == "neural networks" for name in entity_names)
        
        # Step 5: Wait for vectorization and check for recommendations
        with allure.step("Verify personalization engine processing"):
            # Wait for content to be vectorized
            max_attempts = 10
            for attempt in range(max_attempts):
                # Request recommendations for the user
                response = await http_client.post(
                    f"{PERSONALIZATION_URL}/recommendations",
                    json={"user_id": user_id, "limit": 5}
                )
                
                if response.status_code == 200:
                    recommendations = response.json()
                    if any(r["content_id"] == content_id for r in recommendations):
                        break
                
                # Wait before retrying
                await asyncio.sleep(3)
                
                # Last attempt
                if attempt == max_attempts - 1:
                    pytest.fail("Content not available in recommendations after maximum retries")
            
            # Final validation of recommendations
            response = await http_client.post(
                f"{PERSONALIZATION_URL}/recommendations",
                json={"user_id": user_id, "limit": 5}
            )
            assert response.status_code == 200
            recommendations = response.json()
            assert len(recommendations) > 0
            
            # Verify our test content is in recommendations
            content_recs = [r for r in recommendations if r["content_id"] == content_id]
            assert len(content_recs) > 0
            assert content_recs[0]["relevance_score"] > 0
        
        # Step 6: Test semantic search functionality
        with allure.step("Test semantic search"):
            # Search for content with keywords
            response = await http_client.post(
                f"{PERSONALIZATION_URL}/search",
                json={
                    "query": "neural networks optimization",
                    "user_id": user_id,
                    "personalization_weight": 0.3
                }
            )
            assert response.status_code == 200
            search_results = response.json()
            assert search_results["total"] > 0
            
            # Verify our test content is in search results
            content_results = [r for r in search_results["items"] if r["content_id"] == content_id]
            assert len(content_results) > 0
    
    @pytest.mark.asyncio
    @allure.story("User Profile Personalization")
    @allure.severity(allure.severity_level.HIGH)
    async def test_user_profile_updates(self, http_client, wait_for_services, clear_test_data):
        """Test updates to user profiles and how they affect recommendations."""
        # Create initial user profile
        user_id = f"test-user-{uuid.uuid4()}"
        initial_interests = ["Machine Learning", "Data Science"]
        updated_interests = ["Neural Networks", "Computer Vision", "Deep Learning"]
        
        # Step 1: Create initial profile
        with allure.step("Create initial user profile"):
            user_payload = generate_test_payload(
                "user_profile", 
                user_id=user_id,
                interests=initial_interests,
                expertise_level="beginner"
            )
            
            response = await http_client.post(
                f"{PERSONALIZATION_URL}/users",
                json=user_payload
            )
            assert response.status_code == 200
            user_data = response.json()
            assert user_data["user_id"] == user_id
            assert set(user_data["interests"]) == set(initial_interests)
        
        # Step 2: Add vectorized content through direct API
        with allure.step("Add test vectorized content"):
            # Add multiple content items with different topics
            content_items = [
                {
                    "content_id": f"cv-content-{uuid.uuid4()}",
                    "content_type": "research_paper",
                    "title": "Advanced Computer Vision Techniques",
                    "text": "This paper explores the latest developments in computer vision and image recognition.",
                    "source": "test_academic",
                    "entities": ["Computer Vision", "Image Recognition", "AI"]
                },
                {
                    "content_id": f"ml-content-{uuid.uuid4()}",
                    "content_type": "research_paper",
                    "title": "Machine Learning Fundamentals",
                    "text": "An introduction to machine learning algorithms and techniques for beginners.",
                    "source": "test_academic",
                    "entities": ["Machine Learning", "Algorithms", "Data Science"]
                },
                {
                    "content_id": f"dl-content-{uuid.uuid4()}",
                    "content_type": "research_paper",
                    "title": "Deep Learning Architectures",
                    "text": "A comprehensive review of modern neural network architectures and training methods.",
                    "source": "test_academic",
                    "entities": ["Deep Learning", "Neural Networks", "AI"]
                }
            ]
            
            for item in content_items:
                response = await http_client.post(
                    f"{PERSONALIZATION_URL}/vectorize",
                    json=item
                )
                assert response.status_code == 200
        
        # Step 3: Get initial recommendations
        with allure.step("Get initial recommendations"):
            response = await http_client.post(
                f"{PERSONALIZATION_URL}/recommendations",
                json={"user_id": user_id, "limit": 10}
            )
            assert response.status_code == 200
            initial_recs = response.json()
            
            # Initial profile should prioritize ML content
            ml_first = False
            for rec in initial_recs:
                if "Machine Learning" in rec.get("title", ""):
                    ml_first = True
                    break
            assert ml_first, "Machine Learning content should be prioritized for initial profile"
        
        # Step 4: Update user profile with new interests
        with allure.step("Update user profile with new interests"):
            user_payload = generate_test_payload(
                "user_profile", 
                user_id=user_id,
                interests=updated_interests,
                expertise_level="intermediate"
            )
            
            response = await http_client.put(
                f"{PERSONALIZATION_URL}/users/{user_id}",
                json=user_payload
            )
            assert response.status_code == 200
            user_data = response.json()
            assert set(user_data["interests"]) == set(updated_interests)
        
        # Step 5: Get updated recommendations
        with allure.step("Get updated recommendations"):
            # Wait briefly for embeddings to update
            await asyncio.sleep(2)
            
            response = await http_client.post(
                f"{PERSONALIZATION_URL}/recommendations",
                json={"user_id": user_id, "limit": 10}
            )
            assert response.status_code == 200
            updated_recs = response.json()
            
            # Updated profile should prioritize neural networks/deep learning content
            dl_first = False
            for rec in updated_recs[:3]:  # Check top 3
                if "Deep Learning" in rec.get("title", "") or "Neural Networks" in rec.get("title", ""):
                    dl_first = True
                    break
            assert dl_first, "Neural Networks/Deep Learning content should be prioritized after profile update"

@allure.epic("Chimera Platform")
@allure.feature("Microservice Communication")
class TestInterServiceCommunication:
    """Test specific inter-service communication patterns."""
    
    @pytest.mark.asyncio
    @allure.story("Ingestion to ML Orchestration")
    async def test_ingestion_to_ml_orchestration(self, http_client, nats_client, wait_for_services):
        """Test data flow from Ingestion Service to ML Orchestrator."""
        # Configure message monitoring
        message_received = asyncio.Event()
        test_data = None
        
        async def message_handler(msg):
            nonlocal test_data
            data = json.loads(msg.data.decode())
            test_data = data
            message_received.set()
            await msg.ack()
        
        # Subscribe to the relevant subject
        sub = await nats_client.subscribe("ingestion.raw.*", cb=message_handler)
        
        try:
            # Generate test content
            content_id = f"test-ml-{uuid.uuid4()}"
            content_payload = generate_test_payload("raw_content", id=content_id)
            
            # Send to ingestion service
            response = await http_client.post(
                f"{INGESTION_URL}/ingest",
                json=content_payload
            )
            assert response.status_code == 202
            
            # Wait for message to be processed
            try:
                await asyncio.wait_for(message_received.wait(), timeout=15.0)
            except asyncio.TimeoutError:
                pytest.fail("Message not received within timeout period")
            
            # Verify message content
            assert test_data is not None
            assert test_data["id"] == content_id
            
            # Check ML Orchestrator processed it
            await asyncio.sleep(3)  # Give some time for processing
            
            # Query ML Orchestrator status endpoint
            response = await http_client.get(
                f"{ML_ORCHESTRATOR_URL}/status/{content_id}"
            )
            assert response.status_code in [200, 202, 404]  # May be 404 if processed and cleared
            
        finally:
            await sub.unsubscribe()
    
    @pytest.mark.asyncio
    @allure.story("ML Orchestrator to Knowledge Graph")
    async def test_ml_to_knowledge_graph(self, http_client, nats_client, wait_for_services):
        """Test data flow from ML Orchestrator to Knowledge Graph."""
        # Create a mock NLP-enriched message
        content_id = f"test-kg-{uuid.uuid4()}"
        enriched_data = {
            "id": content_id,
            "source": "test-source",
            "content_type": "research_paper",
            "timestamp": str(time.time()),
            "nlp_enrichment": {
                "entities": [
                    {"text": "AI Platform", "label": "PRODUCT", "score": 0.95},
                    {"text": "NVIDIA", "label": "ORGANIZATION", "score": 0.92},
                    {"text": "GPU Acceleration", "label": "TECHNOLOGY", "score": 0.89}
                ],
                "summary": "Test summary about AI platforms using NVIDIA GPUs.",
                "keywords": ["AI", "NVIDIA", "GPU", "acceleration"],
                "sentiment": {"positive": 0.7, "negative": 0.1, "neutral": 0.2}
            },
            "payload": {
                "title": "GPU-Accelerated AI Platforms",
                "text": "This is test content about NVIDIA's GPU-accelerated AI platforms."
            }
        }
        
        # Publish directly to NLP enriched subject
        await nats_client.publish(
            f"nlp.enriched.{enriched_data['content_type']}",
            json.dumps(enriched_data)
        )
        
        # Wait for Knowledge Graph to process
        await asyncio.sleep(5)
        
        # Verify entities were created in Knowledge Graph
        response = await http_client.get(
            f"{KNOWLEDGE_GRAPH_URL}/entities",
            params={"filter": f"source_id:{content_id}"}
        )
        assert response.status_code == 200
        entities = response.json()
        
        # Verify expected entities exist
        entity_names = [e["name"].upper() for e in entities]
        assert "NVIDIA" in entity_names or "GPU ACCELERATION" in entity_names
        
        # Check relationships were created
        response = await http_client.get(
            f"{KNOWLEDGE_GRAPH_URL}/relationships",
            params={"filter": f"source_id:{content_id}"}
        )
        assert response.status_code == 200
        relationships = response.json()
        assert len(relationships) > 0
    
    @pytest.mark.asyncio
    @allure.story("Knowledge Graph to Personalization Engine")
    async def test_knowledge_graph_to_personalization(self, http_client, nats_client, wait_for_services):
        """Test data flow from Knowledge Graph to Personalization Engine."""
        content_id = f"test-pers-{uuid.uuid4()}"
        
        # Create an entity in the Knowledge Graph
        entity_payload = {
            "name": "Test Entity",
            "type": "TECHNOLOGY",
            "properties": {
                "description": "A test technology entity",
                "source_id": content_id
            }
        }
        
        response = await http_client.post(
            f"{KNOWLEDGE_GRAPH_URL}/entities",
            json=entity_payload
        )
        assert response.status_code in [200, 201]
        
        # Directly publish enriched content that would trigger vectorization
        enriched_data = {
            "id": content_id,
            "source": "test-source",
            "content_type": "research_paper",
            "timestamp": str(time.time()),
            "nlp_enrichment": {
                "entities": [
                    {"text": "Test Entity", "label": "TECHNOLOGY", "score": 0.95}
                ],
                "summary": "Test summary about test entities.",
            },
            "payload": {
                "title": "Test Entity Research",
                "text": "This is a test document about Test Entity technology."
            }
        }
        
        await nats_client.publish(
            f"nlp.enriched.{enriched_data['content_type']}",
            json.dumps(enriched_data)
        )
        
        # Wait for processing
        await asyncio.sleep(5)
        
        # Check if vectorized in Personalization Engine
        # Create a user to test search
        user_id = f"test-user-{uuid.uuid4()}"
        user_payload = generate_test_payload(
            "user_profile", 
            user_id=user_id,
            interests=["Test Entity", "Technology"]
        )
        
        response = await http_client.post(
            f"{PERSONALIZATION_URL}/users",
            json=user_payload
        )
        assert response.status_code == 200
        
        # Search for the entity
        response = await http_client.post(
            f"{PERSONALIZATION_URL}/search",
            json={"query": "Test Entity", "user_id": user_id}
        )
        assert response.status_code == 200
        search_results = response.json()
        
        # Verify found in results
        found = False
        for item in search_results.get("items", []):
            if item.get("content_id") == content_id:
                found = True
                break
        
        assert found, "Content should be found in personalization engine"
</file>

<file path="tests/integration/test_inter_service_comm.py">
#!/usr/bin/env python3
"""
Inter-Service Communication Tests
--------------------------------
Tests to validate the communication patterns between microservices:
- Direct HTTP calls
- Message-based communication via NATS
- Data flow through Redis
- Graph operations via Neo4j
"""

import asyncio
import json
import os
import uuid
import pytest
import httpx
import nats
from nats.js.api import ConsumerConfig, DeliverPolicy
import redis.asyncio as redis
from neo4j import AsyncGraphDatabase

# Test configurations
SERVICES = {
    "ingestion": os.environ.get("INGESTION_URL", "http://localhost:8001"),
    "ml_orchestrator": os.environ.get("ML_ORCHESTRATOR_URL", "http://localhost:8002"),
    "knowledge_graph": os.environ.get("KNOWLEDGE_GRAPH_URL", "http://localhost:8003"),
    "personalization": os.environ.get("PERSONALIZATION_URL", "http://localhost:8004"),
}

NATS_URL = os.environ.get("NATS_URL", "nats://localhost:4222")
REDIS_URL = os.environ.get("REDIS_URL", "redis://localhost:6379/0")
NEO4J_URI = os.environ.get("NEO4J_URI", "neo4j://localhost:7687")
NEO4J_USER = os.environ.get("NEO4J_USER", "neo4j")
NEO4J_PASSWORD = os.environ.get("NEO4J_PASSWORD", "password")

# Test data
TEST_DATA = {
    "content": {
        "id": f"test-{uuid.uuid4().hex[:8]}",
        "source": "integration-test",
        "content_type": "test_document",
        "timestamp": "2025-03-30T12:00:00Z",
        "payload": {
            "title": "Inter-Service Communication Test",
            "text": "This is a test document to validate communication between services.",
            "metadata": {
                "test_id": uuid.uuid4().hex,
                "priority": "high"
            }
        }
    },
    "user": {
        "user_id": f"test-user-{uuid.uuid4().hex[:8]}",
        "role": "tester",
        "interests": ["Testing", "Microservices", "NATS", "Redis"],
        "preferences": {
            "content_types": ["test_document"],
            "delivery_frequency": "realtime"
        }
    }
}

# Fixtures
@pytest.fixture(scope="module")
async def http_client():
    """Create an HTTP client for service communication."""
    async with httpx.AsyncClient(timeout=10.0) as client:
        yield client

@pytest.fixture(scope="module")
async def nats_client():
    """Create a NATS client for message-based communication."""
    nc = await nats.connect(NATS_URL)
    try:
        # Access JetStream
        js = nc.jetstream()
        
        # Ensure test streams exist
        try:
            await js.add_stream(name="TEST_STREAM", subjects=["test.*"])
        except nats.errors.Error:
            # Stream might already exist
            pass
            
        yield nc
    finally:
        await nc.close()

@pytest.fixture(scope="module")
async def redis_client():
    """Create a Redis client for shared data access."""
    client = redis.from_url(REDIS_URL)
    try:
        yield client
    finally:
        await client.close()

@pytest.fixture(scope="module")
async def neo4j_client():
    """Create a Neo4j client for graph operations."""
    driver = AsyncGraphDatabase.driver(
        NEO4J_URI, 
        auth=(NEO4J_USER, NEO4J_PASSWORD)
    )
    try:
        yield driver
    finally:
        await driver.close()

@pytest.fixture(scope="module")
async def test_content_id(http_client):
    """Ingest test content and return its ID."""
    content_id = TEST_DATA["content"]["id"]
    
    # Check if ingestion service is available
    try:
        response = await http_client.get(f"{SERVICES['ingestion']}/health")
        if response.status_code != 200:
            pytest.skip("Ingestion service not available")
    except httpx.RequestError:
        pytest.skip("Ingestion service not available")
    
    # Ingest test content
    response = await http_client.post(
        f"{SERVICES['ingestion']}/ingest",
        json=TEST_DATA["content"]
    )
    assert response.status_code == 200, "Failed to ingest test content"
    
    # Wait a moment for ingestion to process
    await asyncio.sleep(2)
    
    return content_id

@pytest.fixture(scope="module")
async def test_user_id(http_client):
    """Create a test user and return the ID."""
    user_id = TEST_DATA["user"]["user_id"]
    
    # Check if personalization service is available
    try:
        response = await http_client.get(f"{SERVICES['personalization']}/health")
        if response.status_code != 200:
            pytest.skip("Personalization service not available")
    except httpx.RequestError:
        pytest.skip("Personalization service not available")
    
    # Create test user
    response = await http_client.post(
        f"{SERVICES['personalization']}/users",
        json=TEST_DATA["user"]
    )
    
    # If user already exists, that's fine too
    if response.status_code not in (200, 201, 409):
        pytest.fail(f"Failed to create test user: {response.text}")
    
    return user_id

# Tests for HTTP-based communication between services
@pytest.mark.asyncio
async def test_service_health_checks(http_client):
    """Test that all services are healthy and can communicate via HTTP."""
    for service_name, url in SERVICES.items():
        try:
            response = await http_client.get(f"{url}/health")
            assert response.status_code == 200, f"{service_name} health check failed"
            
            data = response.json()
            assert data.get("status") == "healthy", f"{service_name} reports unhealthy status"
            
            # Check component health if provided
            components = data.get("components", {})
            for component, status in components.items():
                assert status.get("status") in ("healthy", "up", "available"), \
                    f"{service_name}'s component {component} is not healthy"
                
            print(f"✅ Service {service_name} is healthy")
        except httpx.RequestError as e:
            pytest.fail(f"Could not connect to {service_name} at {url}: {str(e)}")

@pytest.mark.asyncio
async def test_ml_orchestrator_triton_communication(http_client):
    """Test ML Orchestrator can communicate with Triton Inference Server."""
    try:
        response = await http_client.get(f"{SERVICES['ml_orchestrator']}/models")
        assert response.status_code == 200, "Failed to get models from ML Orchestrator"
        
        models = response.json()
        assert isinstance(models, list), "Models response should be a list"
        assert len(models) > 0, "No models returned from ML Orchestrator"
        
        print(f"✅ ML Orchestrator successfully communicates with Triton")
        print(f"📊 Available models: {', '.join([m.get('name') for m in models])}")
    except httpx.RequestError:
        pytest.skip("ML Orchestrator service not available")

@pytest.mark.asyncio
async def test_knowledge_graph_neo4j_communication(http_client):
    """Test Knowledge Graph service can communicate with Neo4j."""
    try:
        response = await http_client.get(f"{SERVICES['knowledge_graph']}/graph/stats")
        assert response.status_code == 200, "Failed to get graph stats"
        
        stats = response.json()
        assert "nodes" in stats, "Graph stats should contain node count"
        assert "relationships" in stats, "Graph stats should contain relationship count"
        
        print(f"✅ Knowledge Graph service successfully communicates with Neo4j")
        print(f"📊 Graph stats: {stats['nodes']} nodes, {stats['relationships']} relationships")
    except httpx.RequestError:
        pytest.skip("Knowledge Graph service not available")

@pytest.mark.asyncio
async def test_personalization_vector_store_communication(http_client):
    """Test Personalization Engine can communicate with Vector Store."""
    try:
        response = await http_client.get(f"{SERVICES['personalization']}/vector-store/stats")
        assert response.status_code == 200, "Failed to get vector store stats"
        
        stats = response.json()
        assert "vectors" in stats, "Vector store stats should contain vector count"
        assert "dimensions" in stats, "Vector store stats should contain dimensions"
        
        print(f"✅ Personalization Engine successfully communicates with Vector Store")
        print(f"📊 Vector store stats: {stats['vectors']} vectors of {stats['dimensions']} dimensions")
    except httpx.RequestError:
        pytest.skip("Personalization service not available")

# Tests for NATS-based communication between services
@pytest.mark.asyncio
async def test_nats_messaging(nats_client, test_content_id):
    """Test NATS messaging between services."""
    # Create unique subject for this test
    test_subject = f"test.inter_service.{uuid.uuid4().hex[:8]}"
    
    # Create a JetStream context
    js = nats_client.jetstream()
    
    # Subscribe to test messages
    received_messages = []
    
    async def message_handler(msg):
        data = json.loads(msg.data.decode())
        received_messages.append(data)
        await msg.ack()
    
    # Create an ephemeral consumer
    psub = await js.pull_subscribe(
        subject=test_subject,
        durable="test_consumer",
        config=ConsumerConfig(deliver_policy=DeliverPolicy.NEW)
    )
    
    # Publish a test message
    test_payload = {
        "content_id": test_content_id,
        "timestamp": "2025-03-30T12:01:00Z",
        "action": "process",
        "metadata": {
            "test_id": uuid.uuid4().hex,
            "priority": "high"
        }
    }
    
    await js.publish(test_subject, json.dumps(test_payload).encode())
    print(f"✅ Published test message to {test_subject}")
    
    # Fetch messages
    await asyncio.sleep(1)
    messages = await psub.fetch(1, timeout=3)
    
    for msg in messages:
        await message_handler(msg)
    
    # Clean up
    await js.delete_consumer("TEST_STREAM", "test_consumer")
    
    # Verify we received the message
    assert len(received_messages) > 0, "No messages received via NATS"
    assert received_messages[0]["content_id"] == test_content_id, "Message content ID mismatch"
    assert received_messages[0]["action"] == "process", "Message action mismatch"
    
    print(f"✅ Successfully received message via NATS")

@pytest.mark.asyncio
async def test_redis_data_sharing(redis_client, test_content_id):
    """Test data sharing between services via Redis."""
    test_key = f"test:content:{test_content_id}"
    test_data = {
        "id": test_content_id,
        "status": "processing",
        "last_updated": "2025-03-30T12:02:00Z",
        "metadata": {
            "test_id": uuid.uuid4().hex,
            "service": "integration_test"
        }
    }
    
    # Store test data in Redis
    await redis_client.set(test_key, json.dumps(test_data))
    
    # Set a short TTL for cleanup
    await redis_client.expire(test_key, 60)
    
    # Verify data was stored
    stored_data = await redis_client.get(test_key)
    assert stored_data is not None, "Failed to store data in Redis"
    
    stored_json = json.loads(stored_data)
    assert stored_json["id"] == test_content_id, "Redis data ID mismatch"
    
    print(f"✅ Successfully shared data via Redis")
    
    # Test Redis PubSub
    test_channel = f"test:events:{uuid.uuid4().hex[:8]}"
    test_event = {
        "event_type": "content_processed",
        "content_id": test_content_id,
        "timestamp": "2025-03-30T12:03:00Z"
    }
    
    # Use a separate Redis client for PubSub
    redis_pubsub = redis.from_url(REDIS_URL)
    
    try:
        # Setup receiver
        received_events = []
        
        async def message_handler(message):
            if message["type"] == "message":
                data = json.loads(message["data"])
                received_events.append(data)
        
        # Subscribe to channel
        pubsub = redis_pubsub.pubsub()
        await pubsub.subscribe(test_channel)
        
        # Start listening in the background
        future = asyncio.create_task(pubsub.run(message_handler))
        
        # Give the subscription time to initialize
        await asyncio.sleep(1)
        
        # Publish event
        await redis_client.publish(test_channel, json.dumps(test_event))
        
        # Wait for the event to be received
        await asyncio.sleep(2)
        
        # Cancel the pubsub task
        future.cancel()
        try:
            await future
        except asyncio.CancelledError:
            pass
        
        # Verify event was received
        assert len(received_events) > 0, "No events received via Redis PubSub"
        assert received_events[0]["content_id"] == test_content_id, "Event content ID mismatch"
        assert received_events[0]["event_type"] == "content_processed", "Event type mismatch"
        
        print(f"✅ Successfully published and received events via Redis PubSub")
    finally:
        await redis_pubsub.close()

@pytest.mark.asyncio
async def test_neo4j_graph_operations(neo4j_client, test_content_id):
    """Test graph operations via Neo4j."""
    # Generate unique labels for this test run to avoid conflicts
    content_label = f"TestContent_{uuid.uuid4().hex[:8]}"
    entity_label = f"TestEntity_{uuid.uuid4().hex[:8]}"
    relationship_type = "TEST_CONTAINS"
    
    # Create test nodes and relationships
    session = neo4j_client.session()
    
    try:
        # Create content node
        result = await session.run(
            f"""
            CREATE (c:{content_label} {{id: $content_id, type: 'test_document'}})
            RETURN c
            """,
            {"content_id": test_content_id}
        )
        content_node = await result.single()
        assert content_node is not None, "Failed to create content node"
        
        # Create entity nodes and relationships
        test_entities = [
            {"name": "Test Entity 1", "type": "concept"},
            {"name": "Test Entity 2", "type": "technology"},
            {"name": "Test Entity 3", "type": "method"}
        ]
        
        for entity in test_entities:
            result = await session.run(
                f"""
                CREATE (e:{entity_label} {{name: $name, type: $type}})
                WITH e
                MATCH (c:{content_label} {{id: $content_id}})
                CREATE (c)-[r:{relationship_type}]->(e)
                RETURN e, r
                """,
                {"name": entity["name"], "type": entity["type"], "content_id": test_content_id}
            )
            record = await result.single()
            assert record is not None, f"Failed to create entity {entity['name']}"
        
        # Query the graph to verify
        result = await session.run(
            f"""
            MATCH (c:{content_label} {{id: $content_id}})-[r:{relationship_type}]->(e:{entity_label})
            RETURN c, collect(e) as entities, count(e) as entity_count
            """,
            {"content_id": test_content_id}
        )
        record = await result.single()
        assert record is not None, "Failed to query content with entities"
        assert record["entity_count"] == len(test_entities), "Entity count mismatch"
        
        print(f"✅ Successfully performed graph operations via Neo4j")
        print(f"📊 Created {record['entity_count']} entity nodes connected to content node")
    finally:
        # Clean up test nodes
        await session.run(
            f"""
            MATCH (c:{content_label} {{id: $content_id}})
            OPTIONAL MATCH (c)-[r]->(e:{entity_label})
            DETACH DELETE c, e
            """,
            {"content_id": test_content_id}
        )
        
        await session.close()

@pytest.mark.asyncio
async def test_full_data_flow(http_client, nats_client, redis_client, neo4j_client, test_content_id, test_user_id):
    """Test complete data flow across all services."""
    # Check that content was ingested properly via HTTP
    try:
        response = await http_client.get(f"{SERVICES['ingestion']}/content/{test_content_id}")
        assert response.status_code == 200, "Failed to retrieve content from Ingestion Service"
        content = response.json()
        assert content["id"] == test_content_id, "Content ID mismatch"
        print(f"✅ Successfully verified content ingestion via HTTP")
    except httpx.RequestError:
        pytest.skip("Ingestion service not available")
    
    # Check that content was processed by ML Orchestrator
    max_attempts = 10
    ml_processed = False
    
    for attempt in range(max_attempts):
        try:
            response = await http_client.get(f"{SERVICES['ml_orchestrator']}/status/{test_content_id}")
            if response.status_code == 200:
                status = response.json()
                if status.get("status") == "completed":
                    ml_processed = True
                    break
            
            # Wait before retrying
            await asyncio.sleep(2)
        except httpx.RequestError:
            # Service might be temporarily unavailable
            await asyncio.sleep(2)
    
    if ml_processed:
        print(f"✅ Successfully verified ML processing of content")
    else:
        print(f"⚠️ ML processing verification timed out - this might be expected in test environment")
    
    # Check that entities were added to Knowledge Graph
    try:
        response = await http_client.get(
            f"{SERVICES['knowledge_graph']}/entities",
            params={"filter": f"source_id:{test_content_id}"}
        )
        
        if response.status_code == 200:
            entities = response.json()
            if entities:
                print(f"✅ Successfully verified Knowledge Graph entities ({len(entities)} entities)")
                for i, entity in enumerate(entities[:3]):
                    print(f"    📊 Entity {i+1}: {entity.get('name')} (Type: {entity.get('type')})")
            else:
                print(f"⚠️ No entities found for content - this might be expected in test environment")
        else:
            print(f"⚠️ Knowledge Graph query returned status {response.status_code}")
    except httpx.RequestError:
        print(f"⚠️ Knowledge Graph service not available - this might be expected in test environment")
    
    # Check for personalized recommendations
    try:
        response = await http_client.post(
            f"{SERVICES['personalization']}/recommendations",
            json={"user_id": test_user_id, "limit": 5}
        )
        
        if response.status_code == 200:
            recommendations = response.json()
            if recommendations:
                print(f"✅ Successfully retrieved personalized recommendations ({len(recommendations)} items)")
                for i, rec in enumerate(recommendations[:3]):
                    print(f"    📚 Recommendation {i+1}: {rec.get('title')} (Score: {rec.get('relevance_score', 0):.2f})")
            else:
                print(f"⚠️ No recommendations found - this might be expected in test environment")
        else:
            print(f"⚠️ Personalization query returned status {response.status_code}")
    except httpx.RequestError:
        print(f"⚠️ Personalization service not available - this might be expected in test environment")

if __name__ == "__main__":
    pytest.main(["-xvs", __file__])
</file>

<file path=".gitignore">
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Virtual Environment
venv/
env/
ENV/
.env
.venv

# IDE
.idea/
.vscode/
*.swp
*.swo
.DS_Store

# Project specific
/tmp/vector_store_backup/
*.faiss
redis_data.json
metadata.json

# Logs
*.log
.log/
logs/

# Coverage reports
htmlcov/
.tox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
.hypothesis/

# Jupyter Notebook
.ipynb_checkpoints

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# pytest
.pytest_cache/

# Prometheus metrics
prometheus/
metrics/

# AWS
.aws/
*.aws.json

# Environment variables
.env
.env.local
.env.*.local

# Local development
local_settings.py
db.sqlite3
db.sqlite3-journal
</file>

<file path=".windsurfrules">
# NVIDIA AI Intelligence Platform
## Development Workspace Specification Rule v1.0

### Core Principles
* **Reproducibility:** Consistent environments across machines
* **Consistency:** Standardized tooling and practices
* **Security:** Secure credential handling and dependency vetting
* **Efficiency:** Fast feedback loops and infrastructure integration
* **Collaboration:** Standardized workflows and knowledge sharing

### Environment Requirements
* **Primary:** Dev Containers (`.devcontainer/devcontainer.json`)
* **Cloud:** brev.dev for GPU access
* **Base:** Ubuntu LTS with Docker

### Required Tools
* Git, Docker, kubectl, Helm
* Language toolchains:
  * Rust (stable, 2021 edition)
  * Python (3.11+) with Poetry/PDM
  * Node.js (LTS) with npm/yarn
* VS Code with extensions for Rust, Python, TypeScript, Docker, K8s

### Language Standards
* **Rust:**
  * `cargo fmt`, `cargo clippy`, `cargo test`
  * Proper error handling with `Result`
* **Python:**
  * `black`, `ruff`/`flake8`, `mypy`, `pytest`
  * Type hints mandatory
* **TypeScript:**
  * `prettier`, `eslint`, strict typing
  * Jest/Vitest for testing

### Version Control
* GitFlow model with protected branches
* Conventional Commits
* PRs require description, passing checks, approvals

### Dependencies
* Commit lockfiles
* Scan for vulnerabilities
* Vet new dependencies for security and license

### Secrets
* No hardcoding in source
* Use `.env` files locally
* Secure management in shared environments

### Infrastructure
* K8s access via RBAC
* Triton via HTTP/gRPC API
* Shared dev/staging for databases and queues

### Quality & Documentation
* Automated linting/formatting
* Unit and integration tests required
* Documentation for code, APIs, components

### Security
* Least privilege principle
* Keep tools updated
* No production data in development

### Compliance
* Mandatory for all contributors
* Non-compliance blocks PRs
* Quarterly specification review
</file>

<file path="info.md">
Excellent. I will now begin crafting a comprehensive set of deliverables to launch the next-generation AI-native intelligence platform, incorporating data privacy compliance (GDPR, CCPA), role-specific personas with journey flows (investors, ML engineers, etc.), and early-stage extensibility through a plugin architecture. 

The deliverables will include:
- A conceptual system blueprint with module mapping and real-time intelligence flow
- An ingestion and intelligence pipeline architecture diagram
- A backend stack comparison matrix with ecosystem fit and tradeoffs
- A role-based UX dashboard framework with user flows and predictive insight integration
- A phased MVP launch roadmap prioritizing compounding value

All responses will be structured, long-form, and enhanced with visuals and production-ready code patterns wherever appropriate. I’ll update you shortly with the first wave of outputs.

# Next-Gen NVIDIA-Powered AI Intelligence Platform: Architecture & Launch Strategy

## Conceptual System Blueprint

The **AI Intelligence Platform** will be a modular, event-driven system optimized for NVIDIA GPU infrastructure. It ingests diverse data on the global AI ecosystem (research papers, news, code, funding, etc.), then processes and transforms it into **validated, personalized, predictive insights** for end users. Key architectural principles include microservices (to isolate functionality), an event-driven pipeline (for real-time streaming of data), and GPU-accelerated AI services (for sub-minute inference latency). Privacy-by-design and compliance (GDPR, CCPA) are embedded at every layer – for example, user data is minimal and encrypted, with clear consent and data deletion workflows. The system avoids reinventing the wheel by leveraging commodity tech where possible (e.g. using existing model servers, databases, and message queues), focusing development on the platform’s unique intelligence and personalization features.

 ([image]()) **Figure:** *Conceptual architecture blueprint of the AI intelligence platform, showing data sources flowing into an event-driven ingestion pipeline, GPU-accelerated AI processing (NVIDIA Triton serving multiple models), knowledge storage (content DB and knowledge graph), and user-facing services (personalized dashboards, real-time alerts, and plugin integrations).* The platform’s modules interact as follows: **external data sources** feed into an **ingestion service** (with a plugin model for extensibility), which publishes events to a **message queue** for downstream processing. **Processing microservices** subscribe to these events to validate and enrich the data (adding summaries, tags, knowledge-graph links, analytics). All heavy AI/ML tasks call an **NVIDIA Triton Inference Server** (hosting GPU-optimized models) or external APIs as needed. Processed knowledge is stored in a **content repository** and a **knowledge graph database**. A **backend API gateway/orchestrator** then serves the aggregated insights to **role-specific dashboards** and triggers **real-time notifications**. This design supports **personalization** (via a user profile & preferences service that filters and ranks content per user) and **extensibility** (via plugin interfaces that allow third-party data sources or analysis modules to be added without disrupting the core system).

**Key System Components:**

- **External Data Sources:** Multi-modal AI domain data streams – e.g. research papers (ArXiv API or Semantic Scholar), news articles and blogs (RSS feeds, curated sites), open-source projects (GitHub trending, PyPI libraries), funding and market data (Crunchbase, press releases), and possibly social media signals. These sources are treated as **modular plugins**, so new ones can be added easily even after MVP launch.

- **Ingestion Service (Plugin-Based):** A dedicated microservice (or set of microservices) handles connecting to sources, fetching or receiving new data in real-time. This could involve RSS feed polling, web scraping (if needed), or using source-specific APIs/SDKs. The ingestion layer normalizes incoming data into a common format and timestamps events for downstream processing. It is **NVIDIA-native** in that it runs on GPU-capable servers (though ingestion itself is mostly I/O-bound) and integrates with GPU-accelerated decoding or parsing if applicable (e.g. using RAPIDS for any heavy data prep on GPU). To ensure compliance, this layer also applies **privacy filters** (e.g. removing personal identifiers if any slip in) and honors opt-out lists (e.g. if certain sources or individuals request not to be tracked).

- **Event Bus / Message Queue:** An **event-driven architecture** connects ingestion to processing. For example, Apache Kafka or NATS serves as a **high-throughput queue** that decouples the producers (ingestors) from consumers (processing services). This design ensures *resilience* and *scalability*: if processing is slow or temporarily down, ingestion can continue queuing new items, and multiple processing workers can scale out to consume the stream. Event messages carry the content payload (article text, metadata, etc.) or references to where it’s stored. An event-driven pipeline also enables **real-time processing**, since new data propagates through the system as soon as it arrives ([Event-driven architecture with Apache Kafka | Statsig](https://www.statsig.com/perspectives/event-driven-architecture-kafka#:~:text=Event,your%20microservices%20can%20communicate)). The event bus can partition streams by topic (e.g. “research-papers”, “news”, “repo-updates”) so that specialized processors handle each.

- **Processing & Intelligence Modules:** A suite of microservices subscribes to the event bus and performs successive transformation steps. The major modules in the **intelligence pipeline** include:
  - **Validation & Filtering:** Verifies data quality and relevance. This could remove duplicates, filter out non-AI-related content, and flag or reject items that don’t meet reliability thresholds (e.g. spam or known misinformation sources). Basic NLP classification (using a lightweight model or keyword rules) can confirm the item is about AI/ML before it goes further.
  - **Summarization & NLP Enrichment:** Condenses the content and adds metadata. An abstractive **summarization** model generates a concise summary of articles or papers for quick reading. We may apply *multi-level summarization* techniques – for instance, first use an extractive step to identify key sentences, then have an abstractive model paraphrase those into a fluent summary ([Techniques for automatic summarization of documents using language models | AWS Machine Learning Blog](https://aws.amazon.com/blogs/machine-learning/techniques-for-automatic-summarization-of-documents-using-language-models/#:~:text=There%20are%20several%20techniques%20to,an%20understanding%20of%20complex%20narratives)). This ensures even lengthy documents (which might exceed single-pass model token limits) are handled by summarizing in stages. Alongside summarization, this module performs **NLP tagging**: extracting keywords, topics, and named entities. For example, it uses **named entity recognition (NER)** to find persons, organizations, technologies, etc., mentioned in the text ([Extract knowledge from text: End-to-end information extraction pipeline with spaCy and Neo4j | by Tomaz Bratanic | TDS Archive | Medium](https://medium.com/towards-data-science/extract-knowledge-from-text-end-to-end-information-extraction-pipeline-with-spacy-and-neo4j-502b2b1e0754#:~:text=The%20next%20step%20is%20identifying,known%20as%20named%20entity%20recognition)). Entities are then standardized via **entity linking** to a canonical knowledge base – e.g. mapping “Google” vs “Google LLC” to a single entity ID ([Extract knowledge from text: End-to-end information extraction pipeline with spaCy and Neo4j | by Tomaz Bratanic | TDS Archive | Medium](https://medium.com/towards-data-science/extract-knowledge-from-text-end-to-end-information-extraction-pipeline-with-spacy-and-neo4j-502b2b1e0754#:~:text=Identifying%20relevant%20entities%20in%20the,linking%2C%20extracted%20entities%20from%20the)). This yields consistent references for the knowledge graph. The enriched metadata includes tags like “domain: NLP” or “topic: autonomous vehicles” to help classify content for users.
  - **Knowledge Graph Extraction:** Based on the entities and context, this module updates a **knowledge graph** that represents relationships in the global AI ecosystem. Using either rule-based patterns or prompt-based large language model extraction, it identifies relationships such as *“Company X acquires Company Y”*, *“Researcher A (from Organization B) publishes Paper Z on Topic Q”*, or *“Startup M receives $N funding from Investor P”*. Relation extraction models (or LLMs with few-shot prompts) detect these connections ([Extract knowledge from text: End-to-end information extraction pipeline with spaCy and Neo4j | by Tomaz Bratanic | TDS Archive | Medium](https://medium.com/towards-data-science/extract-knowledge-from-text-end-to-end-information-extraction-pipeline-with-spacy-and-neo4j-502b2b1e0754#:~:text=Lastly%2C%20the%20IE%20pipeline%20then,we%20have%20the%20following%20text)). The results are added to a graph database (e.g. Neo4j or a property graph store) with nodes like **People, Organizations, Technologies, Datasets, Papers, Products** and edges capturing relations (FOUNDED_BY, INVESTED_IN, PUBLISHED, SOTA_ON, etc.). Over time, this **knowledge graph** becomes a rich network that the platform can query for deeper insights and recommendations. It’s updated in near-real-time as new facts arrive, but also periodically validated for correctness (possibly via human curation or cross-source corroboration) to ensure quality of knowledge.
  - **Analytics & Insight Generation:** Beyond raw extraction, the pipeline includes analytics modules that derive higher-level insights. For instance, a **trend detection** service monitors the stream of content and counts mentions of topics or entities over time – enabling it to spot surges (trending research areas or hot startups). It might use time-series models or simple statistical thresholds to flag “This week, interest in *quantum ML* is up 300%”. Another module might do **comparative analysis**: e.g., when a new model benchmark result is ingested, automatically compare it to prior state-of-the-art and produce a short insight like “New model X exceeds previous accuracy by 2% on Y dataset, but uses 10x parameters.” Such analysis can be powered by templates filled with data or by an LLM prompt that’s fed the relevant facts to produce an analytical sentence. A **sentiment analysis** model may also run on relevant content (like news or social media about companies or products) to gauge market sentiment. These analytic outputs are stored as additional metadata or notifications (for example, tagging a company entity with “positive momentum” or a paper with “breakthrough” if multiple experts mention it).
  - **Privacy & Compliance Check (Continuous):** As part of processing, especially if any personal data appears (e.g. a user’s name in a social media source), the system can automatically anonymize or drop such data, and it logs data lineage for compliance audits. User profile data used in personalization is kept separate from content processing, and any joining of the two (to deliver personalized content) is done transiently in memory when serving, to avoid mixing personal data with content storage. Compliance modules ensure that if a user requests their data or profile be deleted (GDPR “right to be forgotten”), it can scrub all personal identifiers from logs and databases quickly.

- **AI/ML Model Serving (NVIDIA Triton):** At the heart of the platform’s intelligence are various AI models powering summarization, NLP, and predictions. To maximize GPU utilization and support **modular model management**, we deploy **NVIDIA Triton Inference Server** in the Kubernetes cluster ([Serving ML Model Pipelines on NVIDIA Triton Inference Server with Ensemble Models | NVIDIA Technical Blog](https://developer.nvidia.com/blog/serving-ml-model-pipelines-on-nvidia-triton-inference-server-with-ensemble-models/#:~:text=NVIDIA%20Triton%20Inference%20Server%20is,an%20input%20to%20the%20other)). Triton provides a **standardized, scalable serving layer** for multiple models (it can host TensorFlow, PyTorch, ONNX models, etc. under one server). We containerize Triton (using NVIDIA’s NGC container images) and attach it to the GPU nodes. This allows all microservices to query models via REST/gRPC calls to Triton, rather than bundling model runtimes in each service. Triton can even handle **model ensembles** – for example, a multi-step inference pipeline (like an ensemble of an extractive and abstractive summarizer) can be configured inside Triton as a single DAGed inference call ([Serving ML Model Pipelines on NVIDIA Triton Inference Server with Ensemble Models | NVIDIA Technical Blog](https://developer.nvidia.com/blog/serving-ml-model-pipelines-on-nvidia-triton-inference-server-with-ensemble-models/#:~:text=NVIDIA%20Triton%20Inference%20Server%20is,an%20input%20to%20the%20other)), though in our case the multi-level summarization might also be orchestrated at the service logic level. Key models likely to be deployed:
  - *Summarization Models:* e.g. **BART or PEGASUS** transformers for abstractive summaries ([Techniques for automatic summarization of documents using language models | AWS Machine Learning Blog](https://aws.amazon.com/blogs/machine-learning/techniques-for-automatic-summarization-of-documents-using-language-models/#:~:text=Specialized%20summarization%20models)), possibly fine-tuned on scientific text for research papers. For longer documents, a **hierarchical summarizer** (first do section-wise summarization then summarize the summaries) might be implemented.
  - *NER and Classification Models:* e.g. a fine-tuned **BERT** for NER and topic classification, or spaCy models for entity extraction. These could run in Triton or as part of a Python pipeline. Alternatively, modern LLMs via API can do on-the-fly NER and relation extraction using prompt techniques ([Extract knowledge from text: End-to-end information extraction pipeline with spaCy and Neo4j | by Tomaz Bratanic | TDS Archive | Medium](https://medium.com/towards-data-science/extract-knowledge-from-text-end-to-end-information-extraction-pipeline-with-spacy-and-neo4j-502b2b1e0754#:~:text=Identifying%20relevant%20entities%20in%20the,linking%2C%20extracted%20entities%20from%20the)) ([Extract knowledge from text: End-to-end information extraction pipeline with spaCy and Neo4j | by Tomaz Bratanic | TDS Archive | Medium](https://medium.com/towards-data-science/extract-knowledge-from-text-end-to-end-information-extraction-pipeline-with-spacy-and-neo4j-502b2b1e0754#:~:text=Lastly%2C%20the%20IE%20pipeline%20then,we%20have%20the%20following%20text)), but local models ensure data privacy.
  - *Embedding Models:* To support semantic search or recommendations, an embedding model (like **Sentence-BERT** or instructor LLMs) could encode content and user profiles. These embeddings could be stored in a vector index (like FAISS or an Elasticsearch vector store) for similarity queries (e.g. “find related papers” or personalized content ranking).
  - *Trend/Forecast Models:* lightweight predictive models (even simple linear regression or Facebook Prophet) to extrapolate trends in data metrics (growth in papers on topic X, etc.).
  - *Sentiment Models:* e.g. a **DistilBERT sentiment classifier** to apply on news headlines or social posts about companies.
  
  Triton’s advantage is that it maximizes GPU usage across all these models, supporting concurrent inferencing and dynamic batching to boost throughput. As usage grows, we can scale Triton horizontally (multiple instances) and use Kubernetes to load-balance inference requests. We also integrate **NVIDIA TensorRT** optimizations for models where possible (especially if we deploy large language models or need low latency); Triton will serve the TensorRT optimized engines for maximal speed ([Serving ML Model Pipelines on NVIDIA Triton Inference Server with Ensemble Models | NVIDIA Technical Blog](https://developer.nvidia.com/blog/serving-ml-model-pipelines-on-nvidia-triton-inference-server-with-ensemble-models/#:~:text=Tools%20like%20NVIDIA%20TensorRT%20and,when%20performing%20inference%20on%20GPUs)). For any external models (OpenAI, HuggingFace Hub, or Anthropic via API), the platform will have a fallback connector: e.g. if our local summarizer is insufficient for a certain document, it might call OpenAI’s API as backup (with cost controls and caching). This hybrid approach ensures the system is cost-aware – using local models for most work and only paying for external API on rare cases that need a bigger model.

- **Knowledge Storage Layer:** All processed data and knowledge reside in scalable storage:
  - A **Content Database/Search Index** stores the raw ingested items and their generated summaries, tags, and metadata. This could be a NoSQL document store (e.g. MongoDB or Elasticsearch/OpenSearch). Each record contains the original content (or a link if files are stored in object storage), the summary, author/source info, timestamps, and pointers to related entities (like keys for associated knowledge graph nodes). This DB allows the platform to retrieve full context of items on-demand (e.g. when a user clicks a news item to read more) and supports text search if users query keywords. Elasticsearch in particular can also serve semantic search if we index embeddings.
  - A **Knowledge Graph Database** manages the interconnected entity-relation data. A graph DB like Neo4j, TigerGraph, or Neptune (AWS) will allow complex queries like “show all startups in autonomous driving that received funding from investors also funding robotics companies”. The graph’s schema will evolve but initially could include nodes for **Entities** (People, Organizations, Products/Projects, ResearchTopics, etc.) and **Events** (FundingEvent, Publication, Acquisition, BenchmarkResult). Edges capture relationships (e.g. PERSON–<WORKS_AT>–>ORG, ORG–<INVESTED_IN>–>ORG, ORG–<ACQUIRED>–>ORG, PAPER–<PUBLISHED_BY>–>PERSON/ORG, etc.). Each new data item processed can update this graph: e.g., a funding news item creates a FundingEvent node linked to the Company and Investors involved, with properties like amount and date. Over time, this graph becomes a living **knowledge base** that the platform’s AI can utilize for reasoning or to generate insights (“Graph-powered AI”). We will also implement **governance** on this data: periodic reviews and corrections, as well as tooling to merge duplicate nodes or remove incorrect edges (since LLM extractions might have some error rate).
  - Both the content DB and graph DB are accessed via internal APIs. They are deployed on the same Kubernetes cluster or managed services, depending on scale and ops preferences. Regular backups and retention policies are in place (critical for compliance and disaster recovery). We also consider data partitioning by recency (hot recent data vs. cold archive) to keep query performance high.

- **Personalization & User Management:** A central **User Profile service** stores user accounts, their role designation (engineer, researcher, executive, investor, learner, etc.), and preference settings. From day one, the system will require user sign-up and allow them to tailor content preferences (e.g. topics of interest, companies or research areas they want to track, frequency of updates). These preferences feed into the personalization logic of the API. The profile data might include explicit settings (e.g. *“Show more on NLP, less on Vision”*) and implicit behavior data (e.g. click history, which we log with user IDs). A **recommendation engine** can use this data along with the content embeddings or tags to rank and filter what each user sees. Importantly, this personalization is done in a **privacy-compliant** way: all processing is done with user-consented data, and users can export or delete their profile. No personal data is shared externally. For compliance (GDPR/CCPA), we also need to handle Do-Not-Track signals if part of a web app, not use personal data beyond the agreed purpose, and ensure we have consent for any email notifications etc.

- **Extensibility (Plugin Interfaces):** To evolve into a broader ecosystem, the platform is designed with **plugin hooks** from the start. There are two primary plugin types:
  1. **Data Source Plugins:** Third parties or internal teams can add new connectors to ingest data from additional sources (e.g. a plugin to ingest AI policy regulatory updates, or one for patent filings in AI). The ingestion service will have a defined API or template for adding a new source module – e.g. implement a small interface that fetches data and outputs our standard event format. This keeps the core system decoupled from source-specific logic. In the MVP, these might be internally developed, but by Phase 3–4, we can open an SDK for external developers to contribute plugins.
  2. **Analysis/Feature Plugins:** External modules could hook into the processing pipeline or the front-end. For example, a third-party might provide a specialized **metric** (like an “AI Hype Index”) that can be fed into our analytics pipeline or displayed on dashboards. The platform will expose APIs (with authentication and sandboxing) for such modules to register and ingest or consume data. Over time, we could even support **ChatGPT-style plugins** where external services can be invoked from our platform (though that’s more on the consumption side).
  
  Architecturally, supporting plugins means maintaining clear **API contracts** and possibly a **function registry**. E.g., we may offer a gRPC or REST interface where a plugin service can subscribe to certain events (with appropriate authorization) and publish results back. Container orchestration can allow a plugin to be deployed alongside, or an external service can call into our API. Embracing open standards and providing sandbox environments for testing third-party extensions will be key. This extensibility is crucial for keeping the platform’s offerings **evolvable** – we anticipate the AI landscape will change, and new data types (say, multimodal data like videos or code embeddings) could be integrated via plugins without overhauling the core system.

- **Backend API Gateway / Orchestrator:** All client applications (the web UI, mobile, or external integrators) interact with the platform through a unified **API layer**. This can be implemented with a **FastAPI** (Python) or *GraphQL* gateway that aggregates data from the content DB, knowledge graph, and personalization engine to serve tailored results. The API gateway handles **authentication**, enforces authorization (ensuring e.g. an investor user can access premium financial analytics if entitled, etc.), and orchestrates calls to various microservices as needed to fulfill a request. For example, when a dashboard loads for an *AI Engineer*, the API endpoint might fetch the latest content items from the DB, filter or re-rank them according to the user’s profile, perhaps call a text-generation model to compile a “daily brief summary,” and then return the assembled payload. This gateway also exposes endpoints for search queries, for saving user preferences, and for any interactive features (like asking an AI assistant a question about the data). Being implemented in a high-level framework, it can easily call into both the graph database and content store, and even on-the-fly trigger a model inference (via Triton) if a user asks a question that needs NLP (e.g. *“Compare company X and Y’s recent activities”* might trigger the comparative analysis module to run live).

- **Front-End (Role-Based Dashboards):** The user interface will be delivered as a **Next.js** web application (React-based, with server-side rendering for performance). This front-end will offer a **dashboard** experience tailored to each user persona:
  - *AI Engineers/Researchers:* A dashboard showing latest research paper highlights, new open-source projects, and technical breakthroughs. It might include widgets like *“New SOTA results this week”*, code repo trends, and an interactive query interface to the knowledge graph (so they can find related work or bibliographic connections). Technical users get deeper drill-down options (e.g. viewing detailed metrics or model cards for new ML models).
  - *Executives:* A high-level view with trend charts, key news in the AI industry, competitor updates, and market movement summaries. This dashboard emphasizes **aggregated insights** (e.g. “AI investment in healthcare is up 50% this quarter” or “5 new partnerships in autonomous driving this month”) and has less technical detail. Predictive indicators (like forecasts or risk alerts) might be highlighted here. It will be designed for quick scanning – perhaps a “daily snapshot” panel with top 5 things to know.
  - *Investors:* Focused on startups, funding events, and ROI-related metrics. Their dashboard might have a *“Funding Tracker”* listing recent funding rounds, an *“Startup Leaderboard”* highlighting fast-growing companies or notable new ventures, and *sentiment gauges* for public companies in AI. They might also get personalized alerts like “A company in your portfolio just had a major breakthrough” if such data is integrated. The UI could integrate basic financial data (stock prices or valuations) alongside our AI-specific insights, if relevant.
  - *Learners (Students/Enthusiasts):* A more educational, curated experience. This might feature *explainer content* (summaries that are more tutorial in nature), recommended courses or tutorials, and a feed of beginner-friendly articles (“AI News 101”). The personalization here might allow them to follow certain topics to learn about (e.g. “learn about GANs” and then get a sequence of content from introductory to advanced).
  - *General (Cross-role):* While each role has a distinct view, the platform might also allow users to switch context or enable modules from other views. For example, an executive might want to see research highlights occasionally, so they could customize their dashboard to include a “Research Highlights” widget. Thus, the front-end will be widget-based and **configurable** to some extent, with a sensible default layout per persona.

  The front-end design will emphasize **clarity and usability**: using charts, graphs, and summary cards to convey information at a glance. We will likely use a component library or design system for consistency. Each content item (news, paper, etc.) will have a card with its summary, tags, and perhaps a relevance score or “last updated X minutes ago” to stress real-time nature. Users can click through to see more details (where we might show the full content text or an extended summary, and links to source). From a tech stack perspective, Next.js gives us SSR for fast initial load and SEO (if we have any public-facing pages, e.g. a marketing site or maybe publicly accessible portion of the knowledge base). It also supports building a hybrid static/dynamic app – for instance, some pages could be pre-rendered (like a daily public news briefing) while most are behind login and dynamically rendered.

  **Real-Time Updates:** The platform will push updates to the dashboard in real-time as new intelligence comes in. Using WebSockets or Server-Sent Events (via a small realtime service or Next.js API routes), the front-end can receive notifications. For example, if a user’s dashboard is open and a new item relevant to them is processed, a notification or highlight can appear without full refresh. We may incorporate a “real-time feed” ticker for rapidly breaking news. Given Next.js is primarily SSR/React, we’ll integrate a client-side websocket (perhaps using libraries like Socket.IO or leveraging a service like Ably/Pusher) that connects to a **Notification Service** in the backend. This service can subscribe to certain events (like “insight generated” or “alert triggered” events) and broadcast them to online users. Implementation-wise, an **Alert/Notification microservice** can use something like Redis Pub/Sub or a lightweight event broker to receive triggers from the analytics modules and then forward via WebSocket to the UI. This ensures sub-minute end-to-end latency from data arrival to user-visible update, fulfilling the real-time requirement.

- **DevOps & Deployment:** All these services will run on **NVIDIA-optimized Kubernetes** (compatible with NGC). The deployment target is Lambda Labs GPU servers, so we will provision a K8s cluster (using something like kubeadm or a managed k8s if available through Lambda). Each microservice (ingestion, summarization, graph updater, API gateway, etc.) will be a **containerized application** (Docker images). We’ll use the **NVIDIA GPU Operator** on the cluster to manage GPU scheduling and drivers ([Scaling Triton Inference Server — NVIDIA AI Enterprise](https://docs.nvidia.com/ai-enterprise/deployment/natural-language-processing/latest/scaling.html#:~:text=To%20easily%20manage%20GPU%20resources,The%20GPU)) – this operator simplifies enabling GPU support in k8s, making GPUs a schedulable resource for pods. The **Triton Server** runs in its own deployment with one or more pods, each requesting GPU resources (we might start with a single A100 GPU and later scale out; if on A100, we could also use MIG to split GPU for different model pools ([Scaling Triton Inference Server — NVIDIA AI Enterprise](https://docs.nvidia.com/ai-enterprise/deployment/natural-language-processing/latest/scaling.html#:~:text=Scaling%20Triton%20Inference%20Server%20%E2%80%94,The%20GPU))). Other services like the DBs might run as StatefulSets or be managed DB instances. We will use Helm charts or Kubernetes YAML manifests (Infrastructure-as-Code) to define the entire system, so we can deploy reliably and quickly on new environments. For instance, we might maintain a Helm chart for the platform that stands up all components (for MVP, maybe all in one namespace). **brev.dev** comes into play by providing a streamlined development and deployment experience – since brev.dev (now part of NVIDIA) allows developers to spin up cloud GPU dev environments with minimal friction, our team can use it for iterative development of GPU-dependent features (each developer can have a reproducible environment with a slice of GPU to test their microservice, without manual provisioning). Brev.dev can also help in wrapping services for deployment; for example, if brev supports launching a FastAPI or Node service with GPU access quickly, it can accelerate prototyping. In CI/CD, we’ll integrate container build pipelines (e.g. using GitHub Actions or GitLab CI) that build and push images (possibly to NVIDIA NGC private registry or Docker Hub), and a deployment pipeline (maybe Argo CD or Flux) to apply K8s manifests. **Security** is also crucial: we’ll employ best practices like network policies (to isolate services), secrets management (K8s secrets or HashiCorp Vault for API keys like OpenAI), and regular vulnerability scans on images.

In summary, the conceptual blueprint is a **cloud-native, GPU-accelerated microservice architecture**. It cleanly separates concerns – ingestion, processing, storage, and presentation – connected by robust event and API interfaces. This ensures the system is **scalable (horizontally)**, **low-latency** (with GPUs and streaming), and **extensible** (new data sources, models, or features can plug in with minimal rework). By leveraging NVIDIA’s stack (Triton, NGC, GPUs) and modern devops (K8s, brev.dev, Helm), we achieve a fast path from development to production, focusing our energy on the AI and product logic rather than infrastructure heavy-lifting.

## Ingestion & Intelligence Pipeline Architecture

To delve deeper, the **end-to-end data pipeline** – from source ingestion to insight delivery – involves several staged components, each responsible for a part of the data’s journey. Below is the step-by-step pipeline architecture with the roles of each stage:

1. **Source Ingestion & Normalization:**
   - **Mechanism:** The platform will continuously ingest data from numerous sources via a mix of polling, webhooks, and streaming APIs. For example, an RSS feed reader will poll news sites every few minutes for new articles, while an ArXiv monitor might use the API or arXiv’s feed for new paper announcements. GitHub trending data could be pulled via the GitHub API on a schedule, and funding news could come from web scraping specific press release sites or using services like Crunchbase’s API.
   - **Plugin Architecture:** Each source type is implemented as a plugin module in the ingestion service. This means adding a new source (say, a new website or a social media feed) does not require altering the core system – just deploying a new plugin that emits events in the expected format. Initially, we will create core plugins for the highest-value sources. The ingestion service runs these plugins in parallel (e.g., separate async tasks or threads per source) to maximize throughput.
   - **Data Normalization:** Upon fetching raw data (say an HTML page or JSON from an API), the ingestion layer immediately transforms it into a **unified internal format** – essentially creating a data object like: `{id, source, timestamp, raw_content, metadata}`. This ensures downstream stages don’t need to handle myriad formats. Minimal parsing occurs here (e.g., extract main text from HTML, basic cleaning of control characters).
   - **Initial Filtering:** Some obvious filtering happens at ingress: e.g., if an article’s language is not English (and we only support English at MVP), it can be skipped or sent to a translation service if we choose. Also, if we have seen the same content before (duplicate URL or content hash), skip to avoid redundancy. The ingestion service can maintain a short-term cache of seen items (or use an ID from the source to ensure uniqueness).
   - **Event Emission:** Each normalized item is published as an **event/message** to the central bus (e.g., Kafka topic). The event contains the data payload or a pointer to where it’s stored (for very large content, the ingestion might store the raw text in a blob store and just send a reference plus summary metadata to the queue to avoid huge messages).

2. **Validation & Credibility Check:**
   - **Purpose:** Ensure the incoming information is credible and relevant, preventing garbage-in. When a processing worker pulls an event from the queue, the first step is to validate content. This could include checking the source against a whitelist or credibility score (e.g., an unknown blog might be flagged for review or processed differently than an official publication). It might also include basic NLP to ensure the content indeed relates to AI – e.g., if none of our AI-related keywords or ML terms appear, it might be off-topic.
   - **Techniques:** Use a lightweight **classification model** or rule-based filter to tag an item as relevant or not. For credibility, integrate known information: e.g., cross-check if a news piece is being reported by multiple reliable sources (though that might be more advanced and perhaps not in MVP). Potentially maintain a list of known fake-news or clickbait sites to drop. 
   - **Output:** Items that pass validation move forward. If an item fails (irrelevant or dubious), we may drop it or quarantine it for manual verification. This keeps junk from consuming resources further down the pipeline. We log all drops for transparency (so we can later audit if we accidentally filtered something important out).

3. **Natural Language Processing (NLP) Enrichment:**
   - **Summarization:** Using one or multiple models (via Triton), the service generates a brief summary for each content item. For short content (like a 500-word news article), a single-pass transformer summarizer can suffice. For very long content (a 50-page paper), a **multi-pass approach** is used: e.g., break the paper into sections, summarize each, then summarize the summaries to get an overall abstract ([Techniques for automatic summarization of documents using language models | AWS Machine Learning Blog](https://aws.amazon.com/blogs/machine-learning/techniques-for-automatic-summarization-of-documents-using-language-models/#:~:text=There%20are%20several%20techniques%20to,an%20understanding%20of%20complex%20narratives)). This ensures we stay within model context limits while preserving key points. Summaries are targeted to be a few sentences long, capturing the essence and any *insightful data points* (like “achieved X% accuracy” or “raised \$Y million”).
   - **Entity Extraction:** Run NER to pull out names of organizations, people, products, etc. This can be done with a spaCy pipeline or a transformer NER model. We might favor a pre-trained model fine-tuned on news or scientific text to get high accuracy on domain-specific terms (like “GPT-4” as a product, or “OpenAI” as an org). We also capture domain-specific entities such as programming languages, libraries (e.g. “PyTorch”), etc., possibly via a custom dictionary or model.
   - **Entity Linking:** Resolve each extracted entity to a canonical form. We maintain an internal knowledge base (which could simply be a dictionary that maps names to IDs in our knowledge graph, or we use an API like Wikipedia if needed). For instance, link “Meta AI” and “Facebook AI Research” to the same entity if they refer to the same lab. Use context to distinguish ambiguous names (“AWS” as Amazon Web Services vs something else). This step may use an entity linking model or heuristic: e.g., search our knowledge graph for matching labels. The goal is consistency: every mention of a known entity is tagged with an ID.
   - **Topic Tagging:** Classify the content into one or multiple topics. We can have a taxonomy of AI subfields (e.g. NLP, Computer Vision, Reinforcement Learning, Healthcare AI, Autonomous Vehicles, etc.). A multi-label classification model (e.g. fine-tuned BERT or an open source model from HuggingFace) can assign relevant categories. This helps personalize and also helps users filter content by interest.
   - **Sentiment/Emotion (if applicable):** Particularly for news about companies or products, we can run a sentiment analysis to gauge the tone (positive/negative/neutral). For research papers, sentiment isn’t applicable, but for market news it is.
   - **Enrichment Output:** At this stage, we attach to the item: `summary_text`, `entities: [list of (entity_id, entity_name, type)]`, `topics: [list of tags]`, `sentiment_score` (if done), and possibly `embedding_vector` (if we generate it now for search purposes). The enriched item is then saved to the **Content DB** (so we have a record of the processed content), and also forwarded to the next stage (knowledge graph updater) with all this metadata.

4. **Knowledge Graph Update:**
   - **Transformation of Unstructured to Structured Knowledge:** Using the extracted entities and the full or summarized text, this stage identifies relationships and facts to add to the knowledge graph. We employ **relation extraction**, which could be a combination of:
     - Pattern-based rules: e.g., if the summary contains phrases like “X acquired Y” or “Y was acquired by X”, extract an ACQUISITION relation between X and Y with date metadata. We can craft a few regex or dependency parse patterns for common event types (funding, acquisition, partnership, etc.).
     - ML model for relation extraction: Use a fine-tuned transformer that can label relationships between pairs of entities in a sentence (there are models that given a sentence and identified entities, output relations like “PERSON works_for ORG”). This could identify less explicit relations too.
     - LLM-based parsing: For complex or rare cases, we can send the summary and list of entities to an LLM prompt, asking it to output any relationships of interest (this might be slow or costlier, so maybe as a secondary step for items that were not handled by simpler methods).
   - **Knowledge Graph Schema:** We define how different types of content map to graph updates. For example:
     - A *news article about funding* yields a **FundingEvent** node and edges: `Investor --[INVESTED_IN]--> Company` (both Investor and Company are Organization nodes). The event node might attach as `Investor --[PARTICIPATED_IN]--> FundingEvent <-[RECEIVED]-- Company`.
     - A *research paper* yields a **Publication** node and edges: `Researcher --[AUTHORED]--> Publication`, `Publication --[TOPIC]--> TopicNode` (for each topic tag), `Organization --[AFFILIATED_WITH]--> Publication` (if author affiliations known).
     - An *open-source release* (if we track say new GitHub repo announcements) could yield a **Project** node and link to `Organization/Person --[CREATED]--> Project` and `Project --[TECH]--> TechStack` nodes (if we parse it’s built in Python etc., though that might be too granular for now).
     - We also update existing nodes: e.g., increment a count property or update a “last_updated” timestamp on an entity node.
   - **Graph Database Update:** The service uses a Neo4j client or similar to upsert nodes and edges. Given the event-driven nature, we must handle concurrency carefully (two events about the same company arriving simultaneously). Most graph DBs allow MERGE operations to create or find a node by unique key. We will, for instance, use company name as key (plus maybe a type designation) to find or create that node, then create the relationship. Over time, we accumulate a richly interlinked graph. The graph update is done asynchronously relative to the main content processing (so even if it lags by a few seconds, it’s okay, but we aim for near-real-time). 
   - **Quality Control:** We maintain some governance on the KG: if our extraction yields a relation that seems contradictory or highly unlikely, we might mark it for review. For MVP, this might be manual (periodic checks), but eventually, we could have constraints (like if an acquisition funding amount is parsed as ridiculously high, maybe flag).
   - **Use in Downstream:** The knowledge graph is not only stored; its output feeds back into generating **insights and answering complex queries**. For example, once updated, we can query “how many funding events this month” to produce a trend insight.

5. **Insights & Predictions:**
   - **Trend Analysis:** A streaming analytics job or a batch job (running, say, every hour) scans recent data in the content DB and knowledge graph to find trends. Using counts or statistical tests, it identifies things like “Topic X had 5 new papers today vs an average of 1 per day – significant increase” or “The volume of AI hiring announcements has declined this quarter”. These findings are turned into human-readable insights by templates or LLMs. E.g., a template: *“We’ve detected a {increase/decrease} in {metric}: {value} {unit} in {current_period} (vs {prev_period}).”* For more advanced forecasting, we could apply a model (e.g. an LSTM or Prophet) on time series like # of weekly publications in each subfield to project future values, and surface any notable predictions (“Blockchain AI is expected to double its publication rate next year”).
   - **Comparative Insights:** When relevant, the system generates side-by-side comparisons. For instance, if two major models or products are announced in close succession, an automated comparison might be created (covering parameters, performance, etc.). This can use a combination of the knowledge graph (to gather attributes of each item) and an LLM to phrase the comparison. Likewise, investor users might get comparisons like “Company A vs Company B: A has raised more capital but B has grown faster in employee count” if we integrate such data.
   - **Recommendations:** Using collaborative filtering or content-based algorithms on user interaction data, the platform can suggest content to users (“You read a lot about reinforcement learning; here are 3 new items you may find interesting”). Initially, a simple content-based approach using topic overlap or semantic similarity (via embeddings) with what the user has engaged with will be used. As we gather more user data, we could train a recommendation model.
   - **Alert Triggers:** Some insights are time-sensitive and warrant immediate user notification. For example, if an executive user is tracking Company X and a breaking news arrives that “Company X acquired by Y”, the system triggers an **alert**. We define rules per persona – e.g., *Investors* get alerts for big funding rounds or exits, *Engineers* get alerts for major open-source releases or breakthrough research, etc. The alerting component will take these triggers (from analytics or directly from content tags) and push them to the notification service (which then uses WebSocket/email/mobile push as appropriate).

6. **Real-Time Delivery & User Interaction:**
   - **Publishing to Dashboard:** Processed items (with summaries and tags) are now ready to be consumed by users. The platform maintains a **live feed** per user which is essentially a filtered view of the content DB + insights, tailored by their profile. The API gateway aggregates the latest content for each user’s interests and sends it to the front-end on request (e.g., when user opens the app or hits refresh). Because we want real-time, the front-end could also maintain an open channel (WebSocket) to get new items. The system ensures that as soon as an item is fully processed (all enrichment done), it is *available* via the API and/or pushed via the notification system.
   - **Query Handling:** When users actively search or ask questions, the request goes to a **query subsystem**. A search query might retrieve relevant items via Elasticsearch (including semantic matches via embeddings). If a user asks a natural language question (we might include an AI assistant interface by Phase 2 or 3), the backend could use the knowledge graph and LLMs to answer (this is a RAG – Retrieval Augmented Generation – use case, where we’d retrieve pertinent info from our DB/KG and then use an LLM to compose an answer). The architecture supports this because all data is stored and indexed in ways that make retrieval feasible (text index and graph).
   - **Feedback Loop:** Finally, any user feedback (clicks, likes, or explicit feedback like “this was not relevant”) loops back into the system. Such feedback could be ingested as another stream (e.g., to a “user-feedback” topic) and used by the personalization models to adjust recommendations. It can also trigger re-training of models if we incorporate online learning (not in MVP, but later possibly train a model to predict user relevance based on content features and feedback). This closes the loop, continuously improving the quality of delivered insights.

Throughout this pipeline, **performance and latency** considerations are paramount. The use of streaming (message queue) and parallel microservices prevents any single step from becoming a bottleneck – multiple items can be in different processing stages concurrently (e.g., while one article is in summarization, another can be updating the graph, etc.). Critical-path latency mostly comes from model inference (summarization, etc.), which is why we use GPU acceleration and Triton to serve models efficiently. We aim for processing most items end-to-end (from ingestion to ready for user) in well under a minute, ideally a few seconds for shorter items. Some complex items (like a 50-page research paper) might take longer to fully process, but even in those cases, an initial summary could be available quickly, with deeper analysis (like knowledge graph linking) arriving a bit later (the UI can update when new insights for that item are ready).

To summarize, the ingestion & intelligence pipeline is an **automated assembly line of knowledge**:
1. **Ingest** raw data → 
2. **Vet** it for relevance/quality → 
3. **Summarize & tag** it with rich metadata → 
4. **Structure** it into a knowledge graph → 
5. **Analyze** it for higher-order insights → 
6. **Distribute** it in real-time to users. 

This pipeline design ensures a flow of information that is **timely, rich, and ready to be consumed** in customized ways by each end user.

## Role-Based UX Dashboard Framework

The platform’s front-end will present a **unified dashboard** that dynamically adapts to the user’s role (persona) and interests, providing each user with the most relevant insights at a glance. The design approach is to have a core dashboard structure with modular widgets, and a configuration per role that determines which widgets appear, in what layout, and with what default data. Users can further personalize their view, but the role-based defaults serve their typical needs.

**Common Dashboard Elements:** Regardless of role, some elements remain consistent:
- A header with global navigation (allowing switching between major sections like “Feed”, “Search”, “Trends”, “Settings”).
- A notification bell or sidebar for real-time alerts (e.g., “5 new items arrived”).
- A search bar to query the knowledge base (with autosuggest on entities or topics).
- User profile menu for adjusting preferences (like selecting/deselecting topics of interest, switching persona if the user has multiple roles or responsibilities).

Each role then emphasizes certain widgets:

### AI Engineer / Researcher Dashboard
**Focus:** Latest technical content, research developments, and tools.
- **Research Feed:** The center of the page could be a feed of new research paper summaries and code releases. Each item card shows title, source (conference or arXiv), a 2-3 sentence summary, and tags (e.g., “CV”, “Transformers”). If applicable, a badge if it’s a *“Breakthrough”* or *“SOTA”* result.
- **Benchmark Tracker:** A widget that highlights any new state-of-the-art results or notable benchmark achievements. For example, “Imagen v3 achieved a new top-1 accuracy on ImageNet” with previous vs new score. This can use the analytics module’s output.
- **Tool Updates:** An area listing new versions or notable updates of AI frameworks (e.g., “PyTorch 2.x released”) or libraries, pulled maybe from their release notes RSS or GitHub.
- **Knowledge Graph Explorer:** Perhaps a sidebar widget or pop-up that allows the user to select an entity (like a researcher or topic) and see a graph visualization of related entities. For instance, an engineer could explore connections: selecting a neural network architecture shows related papers and authors. This encourages interactive learning from the underlying KG.
- **Custom Queries/Notebook Integration:** Engineers might appreciate the ability to run custom queries (maybe even a Jupyter-like interface or at least an advanced search). Phase 1 likely won’t have a full notebook, but we might integrate with existing tools or provide an API token so they can query data via Python if they want. Eventually, maybe an “Insights IDE” could be part of the UX for advanced users.

### Executive Dashboard
**Focus:** High-level insights, trends, and KPIs for the AI industry.
- **Key Trends Carousel:** At the top, a rotating set of cards each highlighting a major trend or insight (from our trends analytics). E.g., “AI Investment in Q2: Up 20% QoQ ([Techniques for automatic summarization of documents using language models | AWS Machine Learning Blog](https://aws.amazon.com/blogs/machine-learning/techniques-for-automatic-summarization-of-documents-using-language-models/#:~:text=There%20are%20several%20techniques%20to,an%20understanding%20of%20complex%20narratives))”, “Top 3 AI Adoption Challenges in Finance Industry (with links to a detailed report)”. Executives get the story, not just raw data.
- **Market Movements:** A widget with stock or market data for major AI companies (if we integrate a stock API). It could show today’s performance, but more interestingly, overlay our news sentiment. For example, a list of companies with their stock % change and a headline from our feed that might explain that move (e.g., “NVIDIA +3% (on news of record GPU sales)”). This ties our insights to real business metrics.
- **Competitor News & Comparison:** If the user’s company is known (we could have that in their profile, or if they follow certain companies), show a panel that compares key metrics of their company vs competitors: news count, hiring trends (maybe from LinkedIn data if accessible), product releases. This could be in a table or a “versus” chart format.
- **AI Strategy Corner:** Perhaps a section with curated articles or reports on AI strategy, digital transformation case studies, etc., which are more narrative and less technical – catering to the executive’s interest in how AI impacts business outcomes. We might partner with content providers or use our system to identify such content and tag it for execs.
- **Alerts Panel:** Execs might get important alerts pinned, like “Regulatory Alert: EU releases new AI regulation draft” or “Major Data Breach in AI startup” if those occur. These would be drawn from our content but filtered to ones an executive should not miss for risk/compliance reasons.

### Investor Dashboard
**Focus:** Startups, funding events, ROI signals, and network relationships.
- **Funding Feed:** A stream of recent funding announcements with key details (amount, lead investor, series, etc., gleaned from our content). Possibly color-coded by stage (seed, Series A, etc.). Each entry could link to more details in our knowledge graph (e.g., clicking the startup name shows all past funding rounds, investors, maybe founding team from the graph).
- **Top Movers & Shakers:** A widget identifying startups or sectors with momentum. E.g., “Top 5 trending startups this month” (ones mentioned frequently or with high-profile achievements), or “Hot Sectors” (topics that saw a surge in startup activity or funding). This can use our trend analytics on the knowledge graph (like count of funding events per sector).
- **Network Graph of Investments:** A visual widget (maybe using a D3.js graph) that can show the relationship network of investors and companies. An investor user might select a firm (say Sequoia) and see a network of companies it has invested in, with node sizes indicating funding amount or recent news. This helps them identify co-investment patterns or clusters of activity.
- **Portfolio Tracking:** If the user inputs or selects certain companies as their portfolio (maybe not in MVP, but as a feature, they could “follow” companies), then the dashboard can have a section summarizing news just about those followed companies, plus any analytics like “Your portfolio companies have raised X total this year” or “One of your companies might be at risk (if we detect bad press)”.
- **Educational/Market Reports:** Many investors like regular reports (e.g., quarterly “State of AI”). We can have a library of reports or even auto-generated reports that the dashboard links to or summarizes (like a PDF summary of all funding in Q1 2025 etc.). Possibly integration with an existing source or one we generate using our data.

### Learner (Student/Enthusiast) Dashboard
**Focus:** Education, discovery, and guided learning in AI.
- **Learning Pathways:** A featured section might be “Learn about X” – e.g., if a user expressed interest in learning Machine Learning basics, we show a step-by-step content path: intro articles, beginner-friendly videos (maybe linking to YouTube or courseware), and then more advanced topics. This could be curated manually or semi-automated by finding content with increasing complexity on a topic.
- **Simplified News Feed:** A feed of AI news but simplified explanations. Possibly our system could generate *“Explain like I’m 5”* summaries for complex news and show those for learners. Or just pick content sources that are more accessible (like certain blogs). Also include glossaries for jargon in hover tooltips.
- **Quiz/Interactive Element:** To increase engagement, perhaps a quiz widget (“Test your AI knowledge from today’s news”) – something fun like a couple of multiple-choice questions generated from the content. This may be later phase, but it’s appealing for learners.
- **Community Q&A highlights:** If the platform or plugin ecosystem later includes a community aspect (like discussing news), a learner might benefit from seeing a Q&A or FAQ section for topics. Perhaps integration with Stack Exchange or our own system in later phases.

Each persona’s dashboard is **pre-configured** to show what’s most relevant, but users can personalize further:
- They can choose which widgets to hide or show (maybe via a settings toggle or a drag-and-drop layout edit mode if we implement that).
- They can select specific subtopics or entities to “follow”, which will influence the content of certain widgets (e.g., an engineer could follow “GANs” and then the feed emphasizes GAN-related content).
- Conversely, they might mute topics they don’t care about.

**Predictive Insights Integration:** A standout feature of the platform is *predictive insights* – e.g., forecasts, early warnings, etc. These will be visually highlighted, perhaps with special icons or colors. For instance, if our trend model predicts “Investment in biotech AI is likely to surge next quarter”, that might appear as an alert or a highlighted card. On the executive/investor dashboards, such forward-looking insights could be in a dedicated “Outlook” section. On the engineer/researcher side, predictive insights might be less emphasized, but could include things like “Emerging topics to watch: X” based on trend extrapolation.

**UX Considerations:**
- We will use clear **data visualizations** (charts, graphs) wherever it aids comprehension. For example, trending topics widget might be a bar chart of topic frequencies; funding over time might be a line chart. The UI should not overwhelm with text – that’s why summarization is key, to present bite-sized insights.
- Ensure responsiveness: The design should work on various screen sizes since executives might check on mobile. Next.js supports responsive design easily via CSS frameworks (we might use Tailwind or Material UI).
- **Accessibility:** high contrast, screen-reader tags for important info, etc., given corporate environments often require software to be accessible.
- **Internationalization:** Initially likely English-focused, but if targeting global, we’d design with potential i18n in mind (and if later summarization in multiple languages, the UI could switch languages – but MVP likely just English UI).

**Intelligent Automation in UX:** Beyond just displaying info, the platform’s UX will incorporate automation to assist the user:
- If an executive wants a report, they could click “Generate weekly report” and the backend LLM will compile a PDF or presentation outline summarizing that week for them.
- If an engineer wants to compare two models, they might select them and click compare, triggering our comparative analysis feature, then show the result in a modal.
- A chatbot or assistant could be present in the UI (like a little “Ask me anything” AI, possibly powered by GPT but grounded in our data). This assistant can answer questions like “What’s new in AI this week relevant to automotive?” or “Show me major milestones for Google AI in 2024”. This is both a UX feature and an AI feature, potentially Phase 3, but worth considering in design now (to leave space for it).
- **Notifications & Emails:** The UX extends beyond the web app. Users may get email digests (daily or weekly) summarizing key personalized insights (with links back to the platform). Push notifications (if we have a mobile app) or at least email/SMS for important alerts could be offered. All these are configurable by the user to respect preferences.

The **role-based UX framework** thus ensures that each user segment perceives the platform as if it were tailor-made for them. By addressing their unique pain points – whether it’s keeping up with research for an engineer, or seeing the strategic big picture for an executive – we drive engagement and value. The underlying system uses the same data and insights, but the **presentation layer personalizes the storytelling and emphasis**.

From a technical perspective, implementing this in Next.js might mean server-side rendering different pages or components based on role. We could have dynamic routing like `/dashboard/investor`, `/dashboard/engineer` etc., or a single dashboard page that loads a role-specific config via an API call. We will likely maintain a JSON config for each persona that lists which widgets and what data endpoints to call for them. The front-end then assembles the page accordingly. This approach is maintainable – adding a new persona (say “AI Product Manager”) later would be as simple as defining a new config and maybe some new widgets.

In conclusion, the UX is **data-rich but user-friendly**: users get a command center for AI knowledge that feels curated for their needs. Through personalization, predictive analytics integration, and interactive features, the dashboard not only informs users but also empowers them to explore and derive their own insights from the platform’s knowledge hub.

## Backend Stack Evaluation (Rust + FastAPI + Next.js)

To achieve the above architecture, we must choose a backend stack that balances **performance, scalability, developer experience (DX), and rapid iteration**. The proposed stack involves a combination of **Rust** (with frameworks like Actix or Axum) and **Python** (FastAPI), plus a Next.js frontend. We evaluate each component and their interplay:

### Rust (Actix/Axum) Microservices
**Strengths:** Rust is a systems-level language offering **high performance and memory safety**. For building microservices, frameworks like **Actix Web** and **Axum** provide async, multi-threaded web servers that can handle a very high number of requests with minimal latency overhead. Rust’s strong compile-time checks ensure reliability (no null dereferences, data races, etc.), which is valuable in a complex system – it can reduce runtime bugs and crashes. In benchmarks, Actix has shown top-tier throughput among web frameworks, meaning Rust services could comfortably handle heavy loads (e.g., ingesting thousands of events per second or serving large numbers of concurrent API requests) with low CPU usage. This is important for scaling and for real-time performance.

Rust is ideal for **CPU-intensive** tasks or those requiring fine-grained control. For instance, a Rust service could efficiently implement custom data processing (maybe stream processing on the Kafka bus) or do high-speed log ingestion. The **ingestion service** is one candidate for Rust: it might need to handle many network connections (to various APIs) concurrently – Rust’s async runtime excels at that without the overhead of Python threads or GIL. Similarly, if we create a service to do complex analytics or parallel computations (like crunching time-series for trends), Rust can do that faster than Python and without memory blow-ups.

**Weaknesses:** Rust has a steep learning curve, and development speed can be slower due to longer compile times and fighting with the borrow checker (especially for ML developers unfamiliar with it). While Rust has an ecosystem for web and some data processing, its ecosystem for machine learning and scientific computing is still nascent compared to Python’s. Many ML libraries in Rust (e.g. `tch-rs` for Torch, `ndarray` for numeric compute) exist, but they lag behind Python in features and community. This means if we tried to do, say, NLP model inference in Rust, we’d likely rely on FFI to C++ libs or call out to Python anyway, which can be complex. However, since we plan to offload ML inference to Triton, Rust services can simply call Triton via HTTP/gRPC for results, bypassing the need for heavy ML code in Rust itself.

**Actix vs Axum:** Both are excellent. Actix uses an actor model and was known for being extremely fast. Axum is part of the Tokio project, a more modern and simpler approach (tower service middleware). Axum might be a bit easier to work with (more straightforward code, and it benefits from Tokio’s reliability). Actix had some historical unsafety issues but those have been resolved and it’s stable now. If our team is more comfortable with one, that’s fine – either can handle our needs. We might choose **Axum** for its simplicity and because it aligns with the broader Tokio ecosystem (which includes crates for Kafka, etc.). Also Axum might integrate nicely if we use gRPC via Tonic (Rust gRPC library), for internal services.

**Use Cases in our Platform:**  
- **Ingestion Service:** As mentioned, implement in Rust for efficiency with networking and concurrency. It can manage many asynchronous tasks (HTTP requests, feed parsing) with low overhead.
- **Event Stream Processor:** If we implement any streaming computation (for instance, if we use Kafka Streams or similar logic to aggregate data in real-time), doing that in Rust would give us performance and type safety. Rust has Kafka clients (like `rdkafka` which is a wrapper over C library) that are high-performance.
- **API Gateway (possibly):** We could implement the API gateway in Rust for maximal performance, especially if it will handle a lot of requests. Actix/Axum could easily serve as the main REST API. However, this might complicate using Python ML libraries inside the API (like if an endpoint needs to run a quick custom ML operation). We could still call Triton or call a Python service from the Rust API, but that’s cross-service overhead.
- **High-throughput internal tools:** e.g., a log aggregator or a metrics service might be done in Rust to reliably handle volumes.

Rust’s compile-to-native and no runtime means smaller container images and less memory usage at runtime than Python – beneficial in a Kubernetes environment to pack more services per node.

### Python FastAPI (and related Python services)
**Strengths:** Python is the lingua franca of AI and data science. **FastAPI** is a modern, high-performance web framework that is very easy to use for building APIs. It uses Python type hints to create self-documented APIs (with automatic docs via OpenAPI/Swagger) and runs on the performant Uvicorn ASGI server. FastAPI excels in **developer productivity** – one can write an API endpoint in a few lines, integrate with Pydantic for data validation, and call into countless Python libraries. For our platform, Python is almost **unavoidable for the AI/ML components**: handling data frames, using NLP libraries (spaCy, transformers), interacting with Neo4j (via py2neo), etc., all have rich Python support. FastAPI could serve as:
  - The **API Gateway/orchestrator** itself (especially if we want to call Python code or libraries during request handling).
  - A wrapper around certain microservices such as the Summarization service or Knowledge Graph service. For example, a “Summarization Service” could be a FastAPI app that listens for requests (or consumes from queue) and then uses HuggingFace transformers to generate a summary. While we offload heavy model inferencing to Triton, Python might still orchestrate those calls and post-process outputs.

Python’s DX advantage means we can iterate quickly. Our data scientists can prototype in notebooks and then move code into FastAPI endpoints or background tasks with minimal friction. FastAPI also supports **async**, so it can handle many concurrent requests reasonably well (though each request’s heavy computation might still be GIL-bound unless using external libraries that release the GIL, which many scientific libs do in C extensions).

**Weaknesses:** Performance and concurrency limitations. While Uvicorn/FastAPI can handle I/O-bound concurrency well (thanks to async), CPU-bound tasks will be limited by Python’s GIL (Global Interpreter Lock). In our case, if a FastAPI endpoint calls a pure Python function that crunches data for 500ms, it will hold up that worker process. We can mitigate with more workers (Gunicorn can spawn multiple worker processes, effectively multi-processing), at cost of more memory. Python is also heavier on memory footprint per instance. In high-throughput scenarios (like thousands of events per second ingestion), Python could become a bottleneck or incur higher cloud costs to scale out.

However, many of our heavy tasks in Python (like ML inference) actually happen in C/C++ (inside Tensor libraries or via Triton calls), so the GIL might not be the blocker there. For example, calling a HuggingFace transformer model uses PyTorch which releases GIL while doing GPU computations. So FastAPI can still handle concurrent summarization if each call awaits model results (the threads are actually waiting on GPU work). The latency might be dominated by model time rather than Python overhead.

**Interoperability with Rust:** Using both Rust and Python means we have a polyglot microservice environment. They can communicate via REST APIs, gRPC, or message queues. For instance, the Rust ingestion service puts events on Kafka; a Python consumer service picks them up. Or a Rust API gateway could HTTP-call a Python service for a particular task (though intra-cluster calls add latency). Another approach is to keep responsibilities separate: e.g., Rust handles ingestion and maybe static file serving or simple APIs, Python handles AI logic and orchestrating models. They might not need to call each other frequently; they communicate through the database or queue (loose coupling). For example, Python processing writes results to DB; Rust API reads from DB to serve to user. This reduces cross-language call overhead but introduces some duplication of logic perhaps.

**Developer Experience and Team Velocity:** We anticipate needing expertise in both languages. One strategy is **prototyping in Python first**, then optimizing with Rust where needed. For MVP, many components could be Python for speed of development. As we identify bottlenecks, we can rewrite that component in Rust. This way we don’t prematurely optimize and complicate. For instance, start with ingestion in Python using AsyncIO and FastAPI (which can work, maybe using AIOHTTP for feeds), then if it struggles, swap in a Rust service later. The modular architecture facilitates this swap.

### Next.js Frontend
Next.js (React) is quite orthogonal to Rust/Python choice, but integration considerations:
- Next.js will make API calls to our backend. Typically, that’s REST calls to FastAPI or Rust endpoints. It could also use GraphQL if we provide one (we might eventually create a GraphQL gateway for convenience in fetching complex nested data in one go).
- Next.js can also have **API Routes** (serverless functions) which run in Node.js. We might use those for trivial tasks or for bundling static content, but likely our main backend will be separate. However, we could consider an architecture where Next.js’s Node server acts as a BFF (Backend-for-Frontend) proxy to our microservices. In production, though, it’s common to just have Next call the microservice endpoints directly (especially if CORS is handled or if on same domain via a gateway).
- Developer experience: Next.js allows developers to use React for dynamic components and also do server-side rendering easily by fetching data (via `getServerSideProps` or newer data fetching patterns). We should ensure our backend provides endpoints that align with the UI needs (maybe an endpoint that returns all dashboard data in one payload to minimize round-trips, or use SWR/react-query on frontend to call multiple endpoints in parallel).

**Scale & Deployment:** Next.js can be built into a static bundle and served via a CDN for the static parts. For SSR, we’d run a Node process (maybe on the same K8s cluster or a separate service). We should containerize the Next app and deploy it behind a reverse proxy (or use Vercel if we chose to host there initially for convenience, but given everything is on Lambda Labs, likely self-host). Next.js SSR performance is typically good for moderately complex pages, but heavy per-request data fetching could slow it; we can mitigate by caching frequently accessed data at the edge if needed (maybe later). The front-end is less of a bottleneck compared to backend processing though.

### Combined Stack Dynamics
Using **Rust for some services and Python for others** gives us a **best-of-both-worlds** if managed well:
- **Latency-critical or high-concurrency components** (like the streaming ingestion, notification dispatch, possibly the API that needs to handle many persistent connections for websockets) in Rust means they can handle a lot with minimal resource.
- **AI/ML logic and glue** in Python means we leverage the rich AI ecosystem. FastAPI being quite fast itself (it's built on Starlette and Uvicorn which are C speed for network handling) means our API in Python can still be reasonably performant, just not as raw-fast as Rust but often fast enough.

**Deployment Fit:** Both Rust and Python apps containerize nicely. Rust compiles to a single binary – very easy to Dockerize (scratch image, extremely small). Python with FastAPI needs a base image (we can use a slim Python image and pip install requirements). We will multi-stage build to avoid dev dependencies in the final image. With Kubernetes, we can deploy each as separate Deployment. The **NVIDIA GPU** angle doesn’t directly affect Rust vs Python choice for CPU-bound services, but for any service that needs GPU (like if not all models are in Triton, maybe a Python service doing something on GPU), we’d need to use NVIDIA’s base images and request GPU. Rust could also use GPU via CUDA libraries but that’s uncommon for our use-case aside from Triton.

**Developer Workflow:** Team might be split such that some focus on Rust microservices, some on Python. This requires good API/interface definitions between services (e.g., what format messages in Kafka are, or request/response schemas). FastAPI’s automatic docs can help front-end devs see what endpoints exist. We might also auto-generate a client (maybe not needed if using fetch in Next directly). Rust side, if we use gRPC, we could generate Python and TypeScript stubs too.

**Scalability:** We compare how each scales:
- Rust service can handle a lot on one instance; Python might need more instances for same load. But since we are on Kubernetes, scaling horizontally is easy albeit at cost of more memory. For example, if 1 Rust instance = 4 Python instances in throughput, we could just run 4 pods of the Python service. As long as infra cost is acceptable, that’s okay, especially in early stage. So we shouldn’t over-emphasize performance at the expense of delaying development, unless we know we’ll have huge loads immediately.
- Rust’s predictable performance is good for meeting the sub-minute pipeline requirement and gives headroom if user base grows quickly.
- The **latency** difference: for an individual request, Python FastAPI can be very low latency (millis) for trivial logic, but if heavy computation, Rust could do it faster. For something like serving an API response that involves a DB query and formatting, the overhead difference is minor compared to the DB query time perhaps. So it might not matter. But for something like processing 100k events in a stream, Rust clearly wins.

**DX and Hiring:** It might be easier to find developers (especially ML-oriented ones) comfortable with Python. Rust developers are in demand but fewer in number; however, interest in Rust is high. Since our domain is AI, we likely have Python expertise on team. One approach is to write as much as possible in Python for MVP (for speed), and plan Rust refactors gradually. Or identify specific team members to focus on Rust from day 1 for those components. This is feasible since ingestion and maybe the event system can be somewhat separated from the core ML logic.

**Specific Components and Proposed Tech:**

- **Web API (Client-facing):** Option 1: FastAPI (Python) – can directly use Python libs for any on-demand tasks, and can call Triton or DBs. Option 2: Axum (Rust) – very fast, but then if an endpoint requires say generating a PDF report using Python library, Rust would have to call an external service or have that implemented separately. A compromise: use **FastAPI as the main API** for MVP because it simplifies calling into our data stores (there are ORMs/clients like SQLModel, PyMongo, py2neo, etc. we can use directly). We can still optimize certain endpoints if needed by moving them to Rust microservices and calling them. For example, if the search endpoint is too slow in Python, we could implement a small Rust service that queries Elasticsearch and does some filtering faster, but realistically the bottleneck will be Elasticsearch, not Python code.
- **Internal APIs vs Messaging:** We should decide communication patterns. Adopting an **event-driven approach** means less synchronous API calls between microservices. That favors decoupling but eventually you might have many services reading/writing to DBs and topics. For simpler orchestrations, sometimes an internal REST call is fine. E.g., the FastAPI gateway might call a Rust microservice via REST for a specific feature. That’s okay if not too frequent, but introduces network overhead. Alternatively, integrate via the database: e.g., the Rust service writes something to DB, the API just reads from DB, not calling Rust service directly.
- **gRPC**: We might use gRPC for service-to-service comms. Rust has `tonic`, Python has `grpcio` or `grpclib`. gRPC gives us strong interface definitions and efficient binary transfer, good for internal calls or even for clients if we made a public API (though likely we stick to REST/GraphQL for external). If we do go polyglot, gRPC can be nice to define contracts (proto files) that both Rust and Python implement. However, adding gRPC might add complexity early on. We might postpone that and use REST/JSON first (easier debugging, etc.), given our initial scale likely doesn’t demand the last bit of efficiency gRPC gives.

**Summary Table:**

| **Criterion**               | **Rust (Actix/Axum)**                         | **Python (FastAPI)**                        |
|-----------------------------|----------------------------------------------|--------------------------------------------|
| **Performance & Latency**   | Very high performance, low-latency handling of I/O and concurrency. Can handle more throughput per instance (e.g. tens of thousands of reqs/sec) with minimal overhead. Suitable for real-time constraints and heavy multitasking.  ([Serving ML Model Pipelines on NVIDIA Triton Inference Server with Ensemble Models | NVIDIA Technical Blog](https://developer.nvidia.com/blog/serving-ml-model-pipelines-on-nvidia-triton-inference-server-with-ensemble-models/#:~:text=NVIDIA%20Triton%20Inference%20Server%20is,an%20input%20to%20the%20other))Triton integration via HTTP is easy. | Good performance for I/O-bound tasks using async. Latency is low for simple tasks, but heavy CPU-bound tasks are slower than Rust due to interpreter and GIL. Throughput per instance is lower; needs more scaling for high loads. Still, FastAPI on Uvicorn is among the fastest Python frameworks. |
| **Concurrency**            | Excellent: async + multi-threading without GIL. Can utilize multi-core fully in one process. Memory safe concurrency (no data races). Ideal for networking (many simultaneous socket connections, etc.). | Good for I/O concurrency (async). For CPU-bound concurrency, must use multiple workers (multi-processing). Each worker runs on one core (due to GIL). So scales via processes rather than threads. More memory usage when scaling horizontally. |
| **Ecosystem (AI/ML)**      | Growing ecosystem but limited for cutting-edge ML. Can call C/C++ libs (TensorRT, etc.) easily. However, lacks the breadth of ready ML libraries. Need to integrate via FFI or REST to Python for complex ML tasks. Great for systems programming (e.g. writing a custom high-performance service, or using libraries like `polars` for data processing on par with pandas). | Extensive ecosystem: PyTorch, TensorFlow, HuggingFace, spaCy, scikit-learn, Neo4j Python drivers, etc. Anything AI/DS is available. This accelerates development of ML features. FastAPI can easily integrate these libraries inside endpoints or background tasks. For example, calling `transformers.pipeline` for NER directly in code. |
| **Development Speed**      | Slower initially: Rust’s compile-time checks mean more upfront code effort and debugging compile errors. However, once it compiles, it's robust. Fewer runtime surprises. Good for long-term maintenance of critical components. Requires developers skilled in Rust. | Very fast to develop: short edit-run cycle (no compile), dynamic typing (but FastAPI encourages type hints), huge range of libraries to do tasks quickly. Most AI devs comfortable in Python, so implementing features is straightforward. Great for prototyping and iterative development. But need strong testing to catch runtime errors that Rust’s compiler would catch. |
| **Scalability & Efficiency** | Highly efficient use of resources (CPU, memory). Single binary, small memory footprint, no garbage collector pause (just RAII). Suitable for microservices which we want to run at large scale with minimal cost. Also easy to deploy (just run the binary). Harder to cause memory leaks or crashes due to Rust safety. | Scales horizontally but less efficient per process. Memory footprint includes interpreter, etc. Possibly higher cloud costs if many Python instances are needed for equivalent Rust workload. But still scalable – many big systems run on Python by scaling out. Python process might occasionally have memory bloat if not careful (garbage collection overhead). |
| **Integration & Interop**  | Can produce C libraries or command-line tools. Interfacing with Python possible via FFI (e.g. PyO3 can create Python extensions in Rust, but that’s advanced). In microservice context, integration is through network calls or shared data stores. Rust being statically typed means defining clear interfaces. | Easy integration with anything that has a Python API. FastAPI itself can call subprocesses or other APIs. For integration with Rust services, can use REST/gRPC clients in Python to call them. Also easy to use Python as glue: e.g., orchestrator calls Rust service then does something. Python can act as the orchestrator coordinating multi-language pieces. |
| **Developer Skill & DX**   | Need skilled systems or backend engineers comfortable with Rust. Smaller talent pool than Python. But those who know it can be very productive after initial setup. Great community support but documentation sometimes lower-level. Debugging requires understanding of lifetimes, etc. On the flip side, Rust’s type system prevents many bugs. | Python developers are widespread; likely easier to hire/faster onboarding. The development environment is simpler (no compilation, just run). Rich debugging and profiling tools (though Rust also has good ones like `cargo profiler`). Python’s dynamic nature might lead to some runtime errors but testing and type hints mitigate that. |
| **Use in Platform**        | Ideal for **Ingestion Service**, **Event stream processors**, **Notification service**, possibly a high-speed **API gateway** if needed. Also any compute-heavy analytics if we choose (though those could also be done in optimized Python libraries). Rust ensures these components are reliable and fast. | Ideal for **AI processing services** (summarization, NLP tagging) where it can use ML libraries, **API Gateway/Orchestrator** (especially if doing logic with data from DB and calling ML models), **Knowledge Graph manager** (since Python has good Neo4j integration), and general glue logic. Python will likely be used in most of the data-centric microservices. |

Given these considerations, a sensible division of labor is:
- Use **Python/FastAPI** for the core **API backend** and most intelligence services in the MVP. This accelerates development, as one integrated FastAPI app could even handle multiple responsibilities in early version (monolithic approach to start). FastAPI’s async support also means it can serve as an ingestion prototype or handle some concurrency until proven insufficient.
- Introduce **Rust** for specific components where needed. One likely immediate Rust component is the **WebSocket notifications server**: managing many websocket connections in Python can be done (Starlette supports it) but each connection is lightweight in Rust and can handle more clients per node – if we plan to push to potentially thousands of connected users, Rust is a safer bet. Another is a **high-performance ingestion**: if from day one we expect a torrent of data (maybe not; number of sources might be limited initially), Python might suffice with async. But if crawling hundreds of feeds, Rust can do it with less CPU.
- Over time (Phase 2/3), consider migrating the **pipeline orchestrator** to a more structured solution. For example, using Apache Flink or Spark Structured Streaming for some analytics – those are Java/Scala, not our current stack, but just noting alternatives. However, given our size and want for low overhead, probably stick to custom microservices rather than heavy platforms.
- The **backend-for-frontend (BFF)**: We could keep things simple by having the Next.js app communicate directly with the FastAPI gateway for most data. We might not need a separate BFF layer since our FastAPI essentially is the backend. However, if at scale we want to reduce calls, we could incorporate a GraphQL layer (perhaps using Ariadne or Strawberry for Python or even Apollo with a Node BFF that aggregates Python & Rust services). GraphQL could allow the front-end to specify exactly which fields it needs, potentially reducing over-fetching. This is a nice-to-have and could be done later once we see patterns.

**Deployment & Ops:** Both Rust and Python components will be containerized. We ensure our CI/CD can handle multi-language builds: for Rust, `cargo build --release` to get binary; for Python, installing requirements. We might also use Docker buildx to build multi-arch if needed (Lambda likely just x86-64). Monitoring wise, we’ll use something like Prometheus with exporters for both (Rust can expose metrics via e.g. `metrics` crate, Python via Starlette middleware or Prom client lib). Logging: unify log formats (JSON logs) so our ELK or Loki stack can aggregate.

To conclude this evaluation: **Rust + Python hybrid** is a powerful combo that leverages Rust’s efficiency for system components and Python’s flexibility for AI. This aligns with our philosophy of *“not rebuilding commodity tech”*: we use Python where libraries already provide the commodity (NLP, DB clients, etc.), and use Rust where we need to build something custom and high-performance (like the event handling). The modular design on Kubernetes means each service can be implemented in the language best suited for its job. The overhead of a polyglot environment is complexity in development and debugging, but with clear API contracts and containerization, it’s manageable. The fastest path to production likely leans on Python (to get things working), with strategic injection of Rust where absolutely needed for performance or safety.

## Phase-Wise MVP Rollout Plan

Launching such a comprehensive platform requires a phased approach. We will deliver incremental value at each phase, validating the product and feeding back learning into the next iteration. Below is a **phase-wise rollout plan** highlighting what features go live when, why that ordering makes sense, and how each phase builds a foundation for the next:

### **Phase 1: Core MVP – Real-Time AI News & Insights Hub**
**Objective:** Deliver a functional product quickly that addresses a pressing pain point – information overload in AI – in a unique, value-added way. The MVP will focus on a subset of personas (likely AI Engineers/Researchers and Tech Enthusiasts) with core features of ingestion, summarization, and a basic personalized feed. This gets us initial users and feedback.

**Scope & Features:**
- **Data Sources:** Launch with a limited but high-impact set: e.g., top AI news sites/blogs (maybe 5-10 sources like MIT Tech Review AI, arXiv-sanity for recent papers, a couple of popular AI Twitter feeds if possible via scraping API, and major funding news from tech blogs). Keep it text-based sources initially.
- **Ingestion Pipeline:** Fully operational event pipeline for these sources. It will fetch new articles, do summarization and tagging. Knowledge graph extraction can be *minimal* in MVP – perhaps just capturing entities and simple relations (like “Company X – invested in – Company Y” for funding news) to demonstrate the concept. More complex KG use can wait.
- **AI Models:** Use pre-trained summarization (e.g. a small BART model) to generate summaries of each item ([Techniques for automatic summarization of documents using language models | AWS Machine Learning Blog](https://aws.amazon.com/blogs/machine-learning/techniques-for-automatic-summarization-of-documents-using-language-models/#:~:text=Specialized%20summarization%20models)). Use spaCy or a lightweight NER for entities. These will run via Triton or directly through Python (depending on integration complexity – possibly for MVP, directly calling a HuggingFace model in Python might be simpler than setting up Triton, but we aim to get Triton in place early to test that workflow). Ensure summarization is decently good; maybe have human curation on a few outputs initially to fine-tune style.
- **UI (Web Dashboard):** Implement a basic Next.js frontend with a **news feed interface**. Users can sign up (simple auth, or even skip auth in MVP and just show a generic feed to all – though personalization needs login, we might allow Google OAuth for quick onboarding). The feed shows summarized items as they come in, and users can click to expand full text or external link. Provide simple filtering by category (like a toggle for “Research” vs “Industry” news).
- **Personalization:** In the MVP, this could be rudimentary – since we might not have a lot of user data at start, we could allow the user to pick a persona or topics in settings, and then filter the feed accordingly. For example, a user checks “I’m interested in NLP and Startups”, then their feed will prioritize items tagged NLP or startup. The recommendations can be rule-based initially (no sophisticated model yet).
- **Notification/Real-time:** Implement real-time updates on the feed page (maybe using a basic WebSocket or even long-polling). E.g., “New article: ... just added” appears without refresh. This demonstrates the real-time capability from day one.
- **Compliance:** Ensure we have a privacy policy and a way for user to delete their account/data (even if manual). Since MVP likely doesn’t handle personal sensitive data beyond email for login, compliance risk is low, but we’ll put the frameworks in place (use a compliant user auth system, secure cookie practices, etc.).

**Delivery & Value:** This phase delivers an **“AI news dashboard with summaries”** – something like a specialized AI-focused feedreader but augmented with ML (the summarization and tagging). This itself is valuable: it saves users time (they can get key points without reading full articles) and keeps them updated. It’s our hook to attract especially AI practitioners who are time-strapped to follow all developments. We’ll market it as “Real-time AI Insider – get the latest AI developments at a glance.”

**Why this first:** It’s a contained problem with clear data (news/papers) and leverages our strengths in NLP summarization. It doesn’t require the full complex knowledge graph or predictive analytics yet – those are nice additions but not needed to show value. By focusing on this, we also battle-test our ingestion and serving pipeline under a manageable load and ensure the platform’s foundation (Kubernetes, Triton, etc.) works in production. It compounds value by **collecting user behavior data** (what they click, etc.) which we can use to improve personalization later, and by populating our knowledge graph gradually. Even with minimal KG, we’ll start accumulating entities which can be enriched in Phase 2.

**Timeline:** Aim to deliver Phase 1 in ~3 months (assuming team and resources, given the need to set up infrastructure and develop features). Possibly do a closed beta with friendly users to gather feedback on summary quality and feed relevance.

### **Phase 2: Enhanced Personalization, Knowledge Graph & Multi-Persona Expansion**
**Objective:** Build on the MVP by introducing deeper personalization features, expanding content coverage, and leveraging the knowledge graph for more intelligent linking of information. Also begin catering to multiple distinct personas (not just general tech enthusiasts).

**Scope & Features:**
- **User Profiles & Preferences:** Launch full user accounts if not in Phase 1. Users can explicitly set preferences: select persona (if we allow multiple roles), choose topics or companies to follow. Implement a **recommendation engine** that uses their reading history (Phase 1 data) to suggest new content (could be as simple as “more of what you read” based on tags, or a collaborative filter if enough users). This moves the platform from a generic feed to a truly personalized feed.
- **Persona-Specific Dashboards:** Introduce tailored dashboard views. Likely start with 2-3 personas in Phase 2. For example, create a distinct view for **Investors**: include the Funding feed and basic analytics like “total funding this week”. And one for **Researchers**: highlight new arXiv papers in their field, etc. These dashboards will use the same components, just arranged differently with filtering. Collect feedback from each user segment to refine what info they want most.
- **Knowledge Graph User Features:** Make the knowledge graph data visible and usable. For instance, allow users to click an organization name on an article and see a popup with info from our KG: “Organization X: founded 2018 by Y, Funding: $Z, Related news: ...”. This enriches the user experience and differentiates us from standard news aggregators. Also possibly add a simple “Explore connections” page where a user can search an entity and see related entities (a basic graph visualization). This gets users to start trusting and valuing the KG.
- **Content Expansion:** Add more sources, especially to serve the new personas. E.g., for investors, integrate a feed of press releases or a Crunchbase daily export; for researchers, integrate a broader set of journals or maybe Twitter feeds of AI influencers. Also consider ingesting **YouTube or podcast summaries** (maybe use an API to get transcripts then summarize) to broaden content types. Ensure summarization models can handle these (we might use a slightly different model for transcripts if needed).
- **Multi-level Summarization & QA:** Improve the summarization pipeline with multi-level summaries or added QA. For example, generate not just one summary, but also *key points* or *bullet highlights* if the content is long. Perhaps use an LLM to generate a short “insight” from each item (one sentence that is an interesting finding or implication). This could be an additional line in the feed item UI, italicized or highlighted as “Insight: ...”.
- **Search & Query:** Implement a search function so users can query past content or entities. Use the index we have (Elasticsearch). Provide a simple search UI with filters (by date, by source, by entity type). This increases the platform’s utility as a research tool.
- **Notifications & Alerts:** Expand on alerts. Let users opt-in to specific alerts (e.g., “Notify me if any new article about *quantum computing* appears” or “Alert me for any funding above $50M”). The backend can support this by matching new events against these subscriptions. This keeps users engaged off-platform via emails or push.
- **Backend improvements:** By Phase 2, based on Phase 1 load, refactor any bottlenecks. If ingestion in Python struggled, implement the Rust ingestion service now. If summaries were slow, ensure Triton is in use with faster models or scale GPUs. Possibly introduce caching for API responses that are expensive (like daily trend calculations cached for an hour). Also implement more robust monitoring and automated scaling in Kubernetes as usage grows.

**Value Added:** Phase 2 makes the platform **stickier and more intelligent**. Personalization means each user sees content most relevant to them, improving engagement (users feel it’s tailored). The knowledge graph integration provides a *wow factor* and deeper insight (users can connect the dots between items, which a normal news feed wouldn’t give). By catering to specific roles like investors vs researchers, we start addressing the diverse target audience explicitly, which can broaden our user base. We also begin generating proprietary data: our knowledge graph and user preference data which are assets for future features.

**Why Phase 2:** After proving demand with news summarization, it’s logical to enhance personalization to keep users coming back (the novelty of summaries alone might wear off; personalization keeps it relevant). The knowledge graph, a more complex feature, is introduced after we have enough data to populate it from Phase 1. Also, by Phase 2 we likely have more clarity on what relationships are most useful to users, so we can fine-tune our KG extraction and how we present it.

**Phase 2 timeframe:** Next ~3-4 months after Phase 1. It involves a mix of back-end heavy lifting (KG, recsys) and front-end new features (dashboards, search), so stagger development accordingly.

### **Phase 3: Advanced Analytics, Predictions & Automation**
**Objective:** Differentiate the platform with predictive and comparative analytics – moving from just reporting news to providing foresight and deeper analysis. Also, increase automation in delivering insights (like auto-generated reports, AI assistants). By now, we aim to have a solid user base, so we focus on features that increase engagement and possibly premium offerings.

**Scope & Features:**
- **Predictive Insights:** Deploy the trend forecasting models and analytics from our architecture in a user-visible way. For example, launch a **“Trends & Forecasts”** section on the dashboard with charts and predictions: “Topic X is trending up, likely to have Y papers next month (prediction)” or “Investment in AI security startups is projected to grow 2x by next year”. Make sure to present these carefully, perhaps with confidence levels. Possibly offer a monthly “AI Industry Outlook” generated from our data.
- **Comparative Analysis Tools:** Let users compare entities (two companies, two technologies) directly in the UI. This could be a tool where they select two items and the backend uses the knowledge graph + LLM to produce a comparison report ([Extract knowledge from text: End-to-end information extraction pipeline with spaCy and Neo4j | by Tomaz Bratanic | TDS Archive | Medium](https://medium.com/towards-data-science/extract-knowledge-from-text-end-to-end-information-extraction-pipeline-with-spacy-and-neo4j-502b2b1e0754#:~:text=Lastly%2C%20the%20IE%20pipeline%20then,we%20have%20the%20following%20text)). For instance, an investor can compare two startups side by side (we show their funding, key news, maybe a radar chart of strengths), or an engineer compares two models (accuracy, parameters, etc.). This is an advanced feature that leverages our accumulated structured data.
- **AI Assistant (Chatbot):** Integrate a conversational interface where users can ask questions and get answers drawing from our knowledge base. For example, “What were the biggest AI acquisitions this year?” or “List all new AI unicorns (startups with >\$1B valuation) in 2025.” The assistant would use our knowledge graph and content to answer (this might use an open-source LLM fine-tuned on Q&A or just a prompt that cites our DB). This feature can really showcase our platform as an intelligence hub, not just static dashboards.
- **Extensibility & Plugins:** Introduce the third-party plugins concept on a small scale. Perhaps allow certain trusted partners or internal teams to integrate one new data source through our plugin interface as a trial. For example, maybe a partner provides **job postings data** to correlate AI hiring trends – they use our API to plug that in, and we display a new widget “AI Hiring Trends” powered by that plugin. Or allow export plugins: e.g., an integration that automatically posts some insights to a Slack channel for a user’s team. In Phase 3, this could be limited to a few cases to test the ecosystem idea.
- **Mobile App or PWA:** Depending on user demand, consider releasing a simple mobile application (or optimize the PWA) for on-the-go access. Executives and investors especially might prefer a mobile experience. Using React Native or Flutter could be an option leveraging our existing API. If resources allow, this can increase engagement (push notifications about alerts to phone).
- **Monetization groundwork:** If we plan to monetize, Phase 3 is when the product is mature enough to think about premium features. We might introduce a tiered model: free tier gets core news and basic personalization; a premium tier (or enterprise tier) gets advanced analytics, deeper history search, downloadable reports, etc. We can start implementing user roles for that and gating certain features (like advanced compare or forecasts might be premium). However, focus still on growth at this stage; monetization full swing might be Phase 4.

**Value Added:** Phase 3 elevates the platform from reactive to **proactive intelligence**. Users not only get info, they get what it means and what might happen next. This can be a strong selling point for executive and investor personas – e.g., getting forward-looking insights and the ability to query the system like an analyst. The AI assistant and comparative tools add interactive analysis capabilities, making the platform more indispensable as a daily tool. Additionally, by flirting with plugin integrations, we signal that this platform can integrate into workflows (like Slack) and ingest new data (like job stats), which paves the way for becoming a central “hub” in the ecosystem.

**Why Phase 3:** Only after accumulating enough data (content and user interactions) and refining our models in earlier phases can we trust and offer predictive insights. Also, users will by now have gotten used to the platform; giving them new analytic features keeps them interested and can also be used to upsell premium service. Phase 3 is about solidifying the platform’s position as the go-to intelligence source, not just one of many news sites.

**Phase 3 timeline:** Perhaps ~3-6 months after Phase 2, though some pieces like the chatbot might use an existing model via API (which could be quicker to implement if using OpenAI, but we have to consider cost; maybe we fine-tune a local LLM for Q&A by this time given we have data).

### **Phase 4: Ecosystem Expansion and Refinement**
**Objective:** Transform the platform into a comprehensive ecosystem – encouraging third-party extensions, covering all major personas and use cases, and scaling up for enterprise adoption. Also, refine and polish based on all the feedback and data collected in earlier phases.

**Scope & Features:**
- **Open Plugin Ecosystem:** Expand the plugin architecture to outside developers. Provide a **Plugin SDK** or clear docs so others can write data ingestors or analysis modules that plug into our event bus or APIs. Possibly launch a “Plugin Marketplace” where these can be listed if they provide user-facing functionality. For example, a financial data provider might plug in real-time stock info for AI companies; a scholar might build a plugin to integrate citation networks for papers. We ensure sandboxing and review for security of course.
- **Full Persona Coverage:** Add any remaining user types or refine existing ones. For instance, maybe add an **“AI Policy Analyst”** persona focusing on AI ethics, regulations, etc., with its own dashboard of policy news and analysis. Or an **“Educator”** persona that we discovered demand for (with content for teaching AI). We also refine each persona’s experience using the insights from usage data: e.g., if investors never used a certain widget, replace it with something they asked for.
- **Enterprise Features:** If we aim for enterprise clients, add features like team accounts, the ability for a user to create custom dashboards (select widgets and share a dashboard with their team), better export options (PDF reports, CSV data downloads for their analysts). Also, security reviews, single sign-on (SSO) integration for enterprise customers, API access for them to ingest our data into their internal systems if needed (this could be a paid offering).
- **Scalability & Stability:** By Phase 4, we anticipate a larger user base and data volume. We should invest in scaling: e.g., deploy in multiple regions if we have global users to reduce latency, optimize our databases (perhaps moving to more sharded or clustered setups; e.g., use Elastic cluster for search, Neo4j causal cluster for KG for high availability). Also robust backup/restore, failover strategies (maybe multi-zone Kubernetes for HA).
- **Continuous Learning System:** Implement a system to continually improve our models using the data we have. For example, fine-tune our summarization model on summaries that got good user feedback (thumbs up) vs those that didn’t. Use reinforcement learning or active learning to improve recommendation algorithm based on user clicks. This keeps the AI improving with minimal manual intervention.
- **Integration with External AI Services:** Possibly by Phase 4, integrate with large platforms – e.g., allow our platform to feed into a user’s OpenAI ChatGPT plugin (if that’s possible, meaning we become a data provider to external AI assistants). That could increase our reach. Also consider integration with voice assistants – e.g., an Alexa or Google Assistant skill “Ask AI Platform: what’s new today in AI”.
- **Monetization Execution:** By now, we likely have a business model in place. We fully implement paywalls or premium features. E.g., free tier gets last 1 month of content and basic search; paid tier gets full history search and advanced analytics, etc. Or enterprise tier with team collaboration features. We ensure our architecture supports this (tenant isolation if needed, usage tracking for billing).

**Value Added:** Phase 4 positions the platform not just as a product but as an **infrastructure** for AI knowledge in the industry. By opening to plugins and integrations, we encourage network effects – the more third parties enhance the platform, the more indispensable it becomes. Enterprises integrating it into their workflow increases switching costs and solidifies our user base. The continuous learning ensures we maintain top-notch insight quality even as the domain evolves. Essentially, Phase 4 is about **scaling and moat-building** – making it hard for any competitor to catch up due to our rich ecosystem and data.

**Why Phase 4:** Only after proving value to individuals (Phase 1-3) can we attract a broader ecosystem and enterprise trust. Also, by now our system should be stable and secure enough to expose APIs and plugins widely. This phase is about expansion and longevity.

**Beyond Phase 4:** We would continue iterating as AI evolves. Perhaps incorporate new modalities (images, code) if those become crucial (e.g. tracking the latest AI-generated images trends or code gen benchmarks). Or move into facilitating collaboration (maybe allow users to annotate or discuss content on the platform). But those are future possibilities.

Each phase builds on prior ones:
- Phase 1 gives us data and credibility.
- Phase 2 gives us personalization which increases user retention, and initial graph data that powers Phase 3.
- Phase 3’s advanced features set us apart and may attract power users or paying users.
- Phase 4 opens the floodgates to growth via others contributing and embedding our platform in different contexts, solidifying our status as the “definitive hub” for AI intelligence.

Throughout these phases, we remain aligned with our **philosophy**:
- Use off-the-shelf components whenever possible (like using known models in early phases, or existing APIs, to not waste time).
- **Event-driven microservices** from day 1, so adding new components (like a new analytics microservice in Phase 3) is straightforward (just attach it to the event stream).
- Optimize latency at critical points (ensuring real-time delivery remains real-time even as complexity increases – this might involve adding more parallel processing or optimizing model performance in later phases).
- Always consider privacy/compliance at each expansion (e.g., when adding plugins, have a review process and maybe a permission system for what data plugins can access to protect user data; for enterprise, ensure GDPR compliance with data processing agreements, etc.).

Finally, we set **metrics** to determine readiness to move from one phase to the next. For instance:
- Phase 1 -> 2: Achieve, say, X daily active users and evidence that users want personalization (like many using filters) – then greenlight investing in those features.
- Phase 2 -> 3: Achieve Y% retention and enough data that introducing predictive features will be credible. Also ensure our data quality (KG accuracy, etc.) is high to avoid backfiring in predictions.
- Phase 3 -> 4: Attract interest from partners or enterprise (maybe pilot an enterprise client) to validate the need for plugins/integrations and willingness to pay – then expand.

By following this phased approach, we ensure a **fast time to market** with core value (Phase 1 in a few months), then an **iterative enhancement** that compounds our data and user base, leading to a robust, scalable **AI intelligence platform** that stays ahead of the curve in delivering AI insights. Each phase unlocks strategic opportunities: e.g., after Phase 2, we have unique datasets (KG, user prefs) that give us an edge; after Phase 3, we have advanced capabilities possibly unmatched by simple aggregators; after Phase 4, we potentially become the platform others build on or integrate with, fulfilling the vision of a definitive AI ecosystem hub. 

This roadmap allows flexibility – we can adjust specific features per phase based on user feedback or new tech (for example, if a new state-of-the-art summarization model appears in mid 2025, we can slot that in Phase 2 to improve quality). The key is we are delivering continuous value and learning with users, rather than vanishing for a year to build a monolith. It’s truly an **agile, iterative path to a next-gen intelligence platform**.
</file>

<file path="README.md">
# Chimera: Next-Gen NVIDIA-Powered AI Intelligence Platform

A modular, event-driven, GPU-accelerated platform for ingesting, analyzing, and delivering personalized AI intelligence across the global AI ecosystem.

## Architecture Overview

Chimera leverages a highly modular, event-driven microservice architecture orchestrated on Kubernetes, optimized for extreme-scale AI processing using NVIDIA's accelerated computing stack.

* **Microservice Framework & Event Bus**: NATS JetStream for persistent, high-throughput, low-latency messaging
* **GPU Compute Fabric**: Kubernetes with NVIDIA GPU Operator on Lambda Labs GPU instances (A100/H100)
* **Hybrid Backend**: Rust (Axum) for high-throughput services, Python (FastAPI) for ML orchestration
* **Frontend**: Next.js 14+ with role-specific dashboards and real-time updates
* **Intelligence Pipeline**: Sub-minute latency pipeline for data ingestion, validation, enrichment, analysis, and delivery

## Key Components

* **Ingestion Services**: Plugin-based system for diverse data sources
* **Validation Services**: Data credibility assessment
* **NLP Enrichment**: GPU-accelerated summarization and NLP using BART/PEGASUS
* **Knowledge Graph**: Neo4j-based graph with entity and relationship extraction
* **Trend Analysis**: Predictive insights and anomaly detection
* **Personalization Engine**: Vector embeddings for tailored content delivery
* **Delivery Service**: Real-time insights via WebSockets to role-specific dashboards

## Getting Started

See the [Deployment Guide](./docs/deployment/README.md) for instructions on setting up the platform infrastructure.

## Development

* [Architecture Documentation](./docs/architecture/README.md)
* [API Documentation](./docs/api/README.md)
* [Security & Compliance](./docs/security/README.md)

## License

Copyright © 2025 SolnAI
</file>

<file path="requirements.txt">
aiofiles>=23.2.1
boto3>=1.34.0
faiss-cpu>=1.7.4  # Use faiss-gpu for GPU support
numpy>=1.24.0
redis>=5.0.1
loguru>=0.7.2
prometheus-client>=0.19.0
sentence-transformers>=2.2.2
</file>

<file path="specification.md">
Top 10 High-Demand, Low-Supply AI Features (Ranked)

RankFeatureExample User RequestCurrent Support StatusGap & Impact1Long-Term Memory & Persistent Context – AI that remembers prior conversations or user data across sessions.“I wish ChatGPT had built-in long-term memory… It would be so much more useful” – user on X (alextcn on X: "I wish ChatGPT had built-in long-term memory As a ...). OpenAI’s CEO also noted improved memory is among ChatGPT’s most requested features ([Google Gemini now brings receipts to your AI chatsTechCrunch](https://techcrunch.com/2025/02/13/google-gemini-now-brings-receipts-to-your-ai-chats/#:~:text=OpenAI%20CEO%20Sam%20Altman%20has,among%20ChatGPT%E2%80%99s%20most%20requested%20features)).Partially supported: OpenAI’s ChatGPT now offers an “Improved memory” alpha to recall user info across chats for some users, and Google’s Gemini just rolled out cross-chat recall ([Google Gemini now brings receipts to your AI chats2Custom Model Fine-Tuning (Closed Models) – Ability to fine-tune big models like GPT-4 or Claude on one’s own data.“I would also fine-tune GPT-4 to my task if a) it supported it and b) it was cheap” – a developer on Reddit (How important is fine-tuning as foundational models get better?). Many ask when full GPT-4 fine-tuning will be open.Limited support: OpenAI only opened GPT-4 fine-tuning to developers in late 2024 with high costs (≈$25 per 1M tokens) ([OpenAI brings fine-tuning to GPT-4oVentureBeat](https://venturebeat.com/ai/openai-brings-fine-tuning-to-gpt-4o-with-1m-free-tokens-per-day-through-sept-23/#:~:text=tuning%20tools%20provide)). GPT-3.5 can be fine-tuned, but not the latest ChatGPT models earlier. Anthropic’s Claude and Google’s PaLM/Gemini allow fine-tuning only for select enterprise clients, if at all. Open-source LLMs (LLaMA, Mistral) can be fine-tuned freely, but they often trail GPT-4 in quality.3Extended Context Windows – Handling very long inputs (large documents, codebases) without losing context.“ChatGPT struggles with large projects… it can't hold onto big amounts of information like project structures, multiple files, and lots of code.” – user feedback (Improving ChatGPT for Large Projects - OpenAI Developer Forum). Users frequently hit token limits when feeding long texts.Partially supported: Some models pushed context length recently – e.g. Anthropic’s Claude 2 offers up to 100k tokens, and GPT-4 up to 128k in limited beta (What is LLM's Context Window?:Understanding and Working with ...). Most others (GPT-3.5, Llama-2, etc.) top out at 4k–16k tokens. These larger contexts are not widely available to all users (often limited to select tiers or beta programs).High impact: The inability to input entire documents or codebases means AI often “forgets” earlier parts of the input, leading to incoherent or incomplete results. Users must manually chunk and summarize input, adding work and potential error (Improving ChatGPT for Large Projects - OpenAI Developer Forum). Longer context would enable deeper analyses (e.g. processing a book or complex project in one go) – its relative absence is a bottleneck for research and development tasks.4Verified Answers with Sources / Reduced Hallucinations – AI that can cite references or express uncertainty instead of inventing facts.“I wish ChatGPT could say ‘The typical giraffe is purple. Confidence level: 10%.’ … A human can say ‘I don’t know.’ ChatGPT makes something up.” – user on Hacker News ([We come to bury ChatGPT, not to praise itHacker News](https://news.ycombinator.com/item?id=34687083#:~:text=I%20wish%20ChatGPT%20could%20say,Confidence%20level%3A%2010)). Many want the model to provide sources for claims.Poorly supported: By default, ChatGPT and similar LLMs do not cite sources or reliably indicate confidence. Bing Chat and Brave’s LLM incorporate web citations, and tools like Perplexity.ai retrieve sources, but mainstream API models (OpenAI, Anthropic, etc.) only give uncited prose. They can be prompted to provide sources, but often fabricate them (Google Launched ChatGPT rival into SERPS. : r/bigseo). No robust native citation feature is standard yet.5Multi-Modal Input/Output Integration – Unified handling of text, images, audio, etc., in one AI session.“I wish ChatGPT could analyze images in PDFs (Claude has this).” – user question on a community Q&A (BB Digest: ChatGPT vs Consultants - Ben's Bites). Others request features like interpreting charts, generating visuals, or voice responses in the same chat.Limited support: OpenAI’s GPT-4 can see and describe images (Vision mode) and now has voice input/output in ChatGPT, but these are only in certain app versions and not in the API. Google’s Gemini and Bard can accept images and have some voice integration, but full multi-modal reasoning is nascent. No major model yet seamlessly handles text + image + audio + video all together. Different modalities are still largely handled by separate specialized models (e.g. image generators vs. text LLMs).High impact: Users currently juggle multiple tools for different modalities (one model for text, another for images, etc.). This friction means an AI assistant can’t fully “see” what the user sees or produce rich media answers. Many real-world tasks (e.g. analyzing a diagram and explaining it) remain cumbersome. The gap leaves a strong demand for holistic AI that feels more like a human assistant that can see and hear, with current offerings only scratching the surface of that capability.6On-Device / Self-Hosted AI at GPT-4 Level – Running powerful models locally for privacy and offline use.“What’s the best self-hosted/local alternative to GPT-4?” – user on Hacker News (328 upvotes) ([Ask HN: What's the best self hosted/local alternative to GPT-4?Hacker News](https://news.ycombinator.com/item?id=36138224#:~:text=Ask%20HN%3A%20What%27s%20the%20best,193%20comments)). Developers and companies ask for ChatGPT-level models they can run on their own hardware, without sending data to external servers.Largely unsupported: The most advanced models (GPT-4, Claude 2) are closed-source and require cloud compute. Meta’s open LLaMA models (and derivatives like Vicuna, Mistral) can be run locally, but they are generally less capable than GPT-4. Efforts to compress or distill GPT-4-quality knowledge into smaller models are ongoing, but as of 2025 no fully open model matches GPT-4’s overall performance. OpenAI and others have not released top-tier models for on-premise deployment (aside from limited Azure cloud offerings).7Proactive AI & Tool Use Automation – AI that can take actions or multi-step initiatives on its own.“I wish ChatGPT could be proactive – send you new messages… Perfect for a ‘business copilot’ or personal tutor.” – user on Reddit (I wish ChatGPT could be proactive - send you new messages. Perfect for "business copilot", "personal tutor" or "fitness coach" GPTs for example. : r/ChatGPT). Others wish the AI could browse automatically, execute code, or interact with apps without constant prompts.Emerging but not widespread: OpenAI introduced plugins and function calling (allowing ChatGPT to call external APIs or run code in a sandbox), but this is mostly user-initiated. Autonomous agents like AutoGPT and BabyAGI demonstrated multi-step planning by looping the model’s outputs, but these are experimental and often unreliable. No major chat platform currently allows the AI to initiate contact or actions without user prompt (for safety reasons). Tool integration exists (e.g. ChatGPT plugins, Bing’s web browsing), but it’s not universal across models or always accessible to end-users.High impact: The inability of AI to act autonomously means it remains a reactive tool rather than a true assistant. Users must micromanage steps that an agent could handle (e.g. checking calendars, sending emails, performing research). This gap limits productivity gains – for instance, an AI that could monitor information and alert you or perform tasks would be highly valuable, but current AIs can’t unless manually triggered. Many see proactive, agentive AI as the next leap, but it’s under-supplied in current offerings.8Transparent Reasoning & Explainability – Ability to see why the model produced an answer (the logic or chain-of-thought).“ChatGPT’s ‘black box’ reasoning is frustrating… I can’t tell why it made a mistake or how it decided on an answer,” is a common sentiment (e.g. enterprise users want AI decisions explained). (Multiple forum requests)Not implemented: Major LLM providers do not expose the model’s intermediate reasoning. Techniques exist (researchers use chain-of-thought prompting or analyze attention weights), but there’s no user-facing feature to get a rationale step-by-step. In fact, OpenAI explicitly does not let users see GPT-4’s hidden chain-of-thought for safety. Some smaller open models can be prompted to show their reasoning, but it’s not a reliable or built-in feature. Overall, current AI platforms treat the reasoning process as an invisible internal process.High impact: This “black box” issue reduces trust and debuggability of AI. Users can’t easily fix errors or trust complex outputs because they get no insight into the model’s thought process (The Black Box Problem: Opaque Inner Workings of Large Language ...). In fields like medicine or law, lack of explanation is a barrier to adoption. The gap means users have to guess at why the AI said something or run additional prompts to verify reasoning. There is a strong demand for explainable AI decisions, currently unmet by mainstream systems.9Greater Output Controllability (Tone, Style, Persona) – Fine-grained control over how the AI responds (e.g. formal vs. casual, length, persona, etc.).“It’s my most used feature in Claude – the ability to quickly specify how I want the response without having to add it in the prompt. Would love if ChatGPT had it.” – user on OpenAI forum (Feature Request: Response Formats Like Claude). Many users want one-click switches for response style (bullet points, elaborate, concise, etc.).Partially supported: Some platforms offer limited controls – e.g. Bing Chat has “Creative / Balanced / Precise” modes, and OpenAI’s ChatGPT allows custom system instructions for tone. But these are rudimentary. There is no universal, easy UI to adjust voice, style or persona on the fly for most models. Claude’s interface (via Poe) allows preset formats for answers, which users appreciate (Feature Request: Response Formats Like Claude). In general, controlling an LLM still relies on skillful prompting; average users lack a simple knob for creativity or verbosity.High impact: Without easy steerability, users often get suboptimal output format and have to iterate with additional prompts. In professional settings, being able to consistently enforce a style (e.g. polite tone with customers, or terse code comments) is crucial, yet current AI often deviates. The gap leaves users frustrated or forces manual editing. Better controllability is a top ask especially for enterprise and creative uses, to make the AI reliably produce the type of response needed for the context.10Up-to-Date Knowledge Integration – AI that knows about recent events and can access current information by default.“I feel I am severely limited… Much of the prompts I prefer to ask regard the current state of affairs… allowing ChatGPT to browse the internet would be much more beneficial.” – user in r/singularity (I can’t help but feel that allowing to ChatGPT to browse the internet would be much more beneficial : r/singularity). Countless users have asked for removal of the training knowledge cutoff (e.g. “Browsing” mode).Partially supported: Some models have made strides – Microsoft’s Bing Chat (GPT-4 variant) and Google Bard both integrate live web results. OpenAI’s ChatGPT added a browsing feature (via Bing) for Plus users in late 2023. However, the default ChatGPT (and most base models) still operate on static training data (often 1+ year out of date). Many popular LLMs (Llama-2, Claude, etc.) also have knowledge cutoffs and do not update in real-time unless explicitly connected to external tools.High impact: The knowledge cutoff means AI assistants cannot answer many timely questions (news, latest research, stock prices, etc.) without extra steps. Users either get outdated answers or an apology that the model doesn’t have current info. This limits the usefulness of AI in domains like finance, news, or real-time decision-making. While plugins and connected modes exist, the lack of built-in up-to-date knowledge for all users creates a notable gap between user expectations and reality, as people naturally expect an “all-knowing” assistant.Bonus – Closing the Gaps: The AI industry is actively working on these gaps. For instance, personalization and long-term memory are being addressed via “custom GPT” profiles and vector databases to store user data. Fine-tuning barriers are dropping as OpenAI and others roll out model customization options (albeit gradually) and as open-source models improve. Context windows are expanding – research from firms like Anthropic and IBM is pushing toward millions of tokens (I can't emphasize enough how mind-blowing extremely long token ...), potentially obviating the need for chopping data. To reduce hallucinations, there’s a trend toward retrieval-augmented generation (integrating search or a knowledge base by default) and even watermarking model confidence. Multi-modality is set to leap forward with models like Google’s Gemini (built to natively handle text, images, and more) and Meta’s ongoing projects in vision-language integration. We also anticipate more agentive AI: frameworks for safe autonomous tool use and scheduling tasks are in development, which could make proactive AI assistants a reality. Lastly, growing demand for transparency and control is influencing design – future AI APIs may expose reasoning traces or allow user-defined style transformations more readily. In summary, these high-demand features are well recognized, and upcoming models and platform updates throughout 2025 are poised to narrow these feature gaps, making AI tools more capable, trustworthy, and user-aligned.

Sources: The analysis above is grounded in user forums, developer feedback, and industry updates, including Reddit discussions, OpenAI community posts, and recent news from major AI providers. Each feature includes inline citations to specific user requests and reports for verification.

AI-Driven Personalization Engine Technical Specification

Introduction & Goals

The AI-Driven Personalization Engine is the core service in the Synapse Web platform responsible for tailoring content, recommendations, and user experiences to each individual. Its purpose is to leverage advanced AI (as of 2025) to deliver hyper-personalized, predictive, and collaborative knowledge interactions in real time. In essence, this engine serves as the “brain” and memory of Synapse Web: it continuously learns from user behavior and context, and adapts what the user sees or is suggested accordingly (From Apps to Agents: How the AI-Native Tech Stack Is Transforming Software | by BCGonTech Editor | BCGonTech | Medium). By analyzing past interactions and current context, it can anticipate user needs and seamlessly integrate relevant content or workflow adjustments.

Measurable Objectives: The Personalization Engine is designed with strict performance and quality targets to ensure a smooth and effective user experience. Key objectives include:

Low Latency Updates: Personalization updates (from input signal to updated output) under 300 ms in 95% of cases, to maintain real-time feel. Inference for recommendations should typically execute in <300 ms, and profile read/update operations in <100 ms.

High Scalability: Support 10 million active users (with 100k+ concurrent users) without degradation. The system will scale horizontally to handle peak loads and ensure each user’s experience remains snappy.

Recommendation Precision: Achieve >85% precision for adaptive recommendations (e.g. over 85% of recommended items are relevant as measured by user clicks or feedback). This implies sophisticated algorithms that closely track user interests.

High Availability: Ensure an uptime of 99.95% or higher for personalization functionalities, given their critical role in the platform’s value.

Key Features: The engine delivers several essential personalization capabilities to meet the above goals:

Dynamic User Modeling: It builds and maintains a live profile of each user that evolves with their interactions. This model captures interests, expertise level, and preferences, updating continuously as new data streams in.

Implicit & Explicit Signal Ingestion: It ingests a wide array of signals – implicit signals like page views, clicks, time spent, query history; and explicit signals like likes, ratings, or manual preferences. Both types feed into refining the user’s profile and the recommendations.

UI/Workflow Personalization: The engine customizes not just content recommendations, but also aspects of the user interface and workflow. For example, it can reorder sections on a dashboard to surface what’s most relevant, highlight certain knowledge sources, or streamline frequent actions for the user.

Predictive Suggestions: Using predictive AI models, the engine suggests next steps or content proactively. For instance, it might suggest a relevant article, a colleague to collaborate with, or a follow-up question the user might want to explore, based on patterns in the user’s behavior and similar users. These suggestions anticipate needs before the user explicitly searches for them.

By achieving these goals and features, the Personalization Engine will ensure that Synapse Web provides a tailored, context-rich experience for every user, accelerating learning and collaboration across the platform.

Functional Requirements

The AI-Driven Personalization Engine will provide a range of functionality that ingests data about users and content, processes it in real time, and outputs personalized results in context. The following requirements detail what the system must do, including specific user stories and the inputs/outputs of each function.

Multimodal Data Ingestion

Capture Diverse User Signals: The engine shall collect data from multiple sources to understand user behavior and preferences. This includes:

Activity Logs: Clickstream events, page navigation, search queries, time spent on content, scrolling behavior, etc.

Uploaded Content & Contributions: If a user uploads or creates content (e.g. documents, notes), the system analyzes it (extracting text, topics, metadata) to incorporate the user’s content interests.

Natural Language Queries and Interactions: Questions the user asks or chat interactions with the system (if any conversational UI), which reveal intent and knowledge gaps.

Collaborative Signals: Interactions like comments, upvotes, or shares in the collaborative workspace, as well as team/project membership information, to personalize within a group context.

Real-time Stream Processing: Incoming events should be processed in real time. For example, as soon as a user clicks on a new topic or a document, that signal is published to the Personalization Engine. The engine will use stream processing (e.g. via Apache Kafka topics or Apache Flink jobs) to handle high-volume event streams continuously.

Data Enrichment: For each raw signal, the engine may enrich it with additional context. For instance, when ingesting a document view event, it can call a content tagging service to identify the document’s topics or entities, then attach those to the event. When processing a natural language query, it can perform NLP to detect intent or map it to known knowledge graph entities.

Input Validation: All ingested data is validated against expected schemas/formats. Malformed events (e.g. missing user ID or timestamp) are logged and dropped or routed to a dead-letter queue for inspection, ensuring they don’t corrupt the user model.

User Story: “As a user, whenever I interact with content (read an article, ask a question, etc.), the system picks up on those actions and subtly adjusts what it will show me next – without me having to explicitly do anything.” (The engine continuously learns from implicit signals.)

Real-Time User Profile Construction

Profile Aggregation: The engine maintains a User Profile for each user, which is a synthesis of all available data about that user. This profile is constructed and updated in real time. Every new event (from the ingestion pipeline above) should update the profile within a few hundred milliseconds so that subsequent recommendations reflect the latest activity.

Profile Data Elements: A user profile includes:

Identity & Demographics: Basic info like user ID, and possibly role, organization or other attributes (excluding sensitive PII not needed for personalization unless explicitly allowed).

Preferences & Interests: A weighted representation of topics or categories the user is interested in. For example, the profile might store that User123 is 80% interested in “Machine Learning” and 50% in “Data Privacy” based on content consumption.

Behavioral History: Summaries of recent activities (e.g. last 10 documents read, frequently visited sections, recent search queries).

Skill/Knowledge State: In a learning context, the profile might track which skill or knowledge areas the user has mastered vs. which they are exploring, enabling adaptive learning paths.

Embedding Vector: The user may be represented by one or more high-dimensional vectors (embeddings) that capture their preferences in vector space. This is computed via transformer-based encoders on their interaction history or content preferences.

Multimodal Fusion: The profile integrates signals of different types. For example, if a user both reads articles and watches videos (multimodal content), the profile merges these signals (possibly by converting them into a common embedding space or linking them via a knowledge graph). The result is a holistic view rather than siloed per content type.

Contextual Profiles: The engine supports context-specific profiling. A user might have a slightly different persona in different workspaces or projects. The system will maintain context-specific facets (for example, interests relevant to Project X vs Project Y) and apply the appropriate context when generating recommendations. (If the user is currently active in a “Web Development” project space, their profile in that context is used for personalization in that space.)

Privacy Respect & Opt-Out: If a user has not consented to certain data usage (or opts out), the profile should exclude that data. For instance, if a user opts out of personalization based on activity, the engine might revert to a default or minimal profile for them. (This ties in with GDPR compliance – see Privacy requirements.)

User Story: “As a user, I want my recommendations to reflect what I’ve been doing recently. After I start exploring a new topic, I should quickly see more content related to that topic, even if it’s different from what I viewed last month.” (The profile updates in real time to capture this shift.)

User Story: “As an admin, I can create separate spaces or domains (like departments or topics), each with its own personalization rules. The engine should maintain profiles that respect those boundaries, so a user’s behavior in a finance workspace doesn’t overly influence their recommendations in a healthcare workspace.” (Dynamic user modeling with context boundaries.)

Contextual Personalization & Recommendation Generation

Personalized Recommendations: The engine generates a set of recommended items (knowledge articles, documents, people to follow, questions, etc.) tailored to the user. This can be triggered when the user visits a personalized feed, or in response to a query (“What should I learn next?”). Recommendations consider the user’s profile and the context of the request:

If the user is on a certain page (e.g. viewing a document about AI), the engine provides “related content” recommendations relevant to that page’s topic.

If the user is on their home dashboard, the engine provides a diverse feed spanning the user’s top interest areas, recent collaborations, and any novel content deemed relevant.

UI Personalization: The engine’s output can also adjust the UI itself. For example, on a knowledge dashboard, the sections might be reordered: a user heavily interested in code examples might see a “Code Snippets” section at top, whereas another user sees “Tutorial Videos” first. The engine can decide these based on the profile. Similarly, in a workflow (say a multi-step task), the engine might suggest skipping or reordering steps that the user is already familiar with, providing a shortcut.

Predictive Next-Actions: Beyond content, the engine suggests actions. For instance, after a user finishes reading an article, it might proactively suggest “Schedule a meeting with Jane, who wrote this article, for deeper discussion” if collaboration data shows Jane is available and the user often seeks mentorship. Or it might suggest “You’ve completed readings on topic X, consider taking quiz Y to test your knowledge.” These suggestions use predictive models to guess what will benefit the user.

Multi-Modal Recommendations: If the platform contains multiple content types (text, video, Q&A threads, etc.), the engine can mix them. One recommendation list might include a video lecture, a written tutorial, and an interactive demo link, all related to the same topic, giving the user options in their preferred learning style.

Context-Aware Filtering: All recommendations are filtered and tuned to the current context. For example, if the user is currently in a collaborative project context, recommendations might favor content created by team members or relevant to the project’s domain. If the context is a mobile vs desktop client, the engine might choose shorter content for mobile (since attention spans differ). Context is passed as a parameter to the recommendation function.

Rule-based Overrides: The system must allow certain admin-defined rules to influence personalization results. For instance, an administrator could define “In the Onboarding workspace, always surface the ‘Getting Started Guide’ for new users at least once.” The engine will incorporate such rules (which might be simple triggers or more complex logic) so that automated ML recommendations can be overridden or supplemented by deterministic business rules where needed.

User Story: “As a user, when I open the platform, I get a home page that feels like it was made for me. If I’ve been researching topic A a lot, I see new content about A. If I always prefer videos to articles, I start seeing more videos. It adapts automatically as I learn.”

User Story: “As a user, if I’m stuck or not sure what to do next, the system suggests something helpful – like a relevant expert to ask, or a next topic that fits with what I’ve done – without me even asking.” (Predictive, context-aware assistance.)

User Story: “As an admin, I can define personalization rules per workspace or domain. For example, in the ‘Data Science’ workspace I manage, I want the first recommendation to always be our curated tutorial for newcomers. I configure that, and the engine will ensure it’s included for relevant users.”

Inputs, Outputs, and Internal Data Flow

For each core function, the following summarizes the inputs, processing, and outputs:

Signal Ingestion Function: Input: A raw event (JSON or similar) such as {"user_id": 123, "event": "view", "item_id": "doc456", "timestamp": "...", "context": "projectX"} plus possibly additional metadata (device, location, etc.). Processing: validate schema; enrich with content tags (lookup item_id’s metadata); put event on processing queue/stream for profile update; possibly update a real-time counter or trigger directly if needed. Output: A confirmation (HTTP 200 on API ingestion) and internally, an updated profile in memory/store; also an event persisted to a datastore (for audit).

Profile Update Function: (Could be triggered by an event or called periodically) Input: user_id (or event that has user_id). Processing: fetch current profile from store; combine with new signal (e.g., increment counts, update vectors by recomputing or adjusting weights, update recency timestamps); store the updated profile back. This may involve calling ML models – e.g., passing the user’s interaction history to a transformer model that outputs a new embedding, or updating a vector incrementally. Output: updated profile object (often no direct user-facing output, but stored for use by recommendation).

Recommendation Generation Function: Input: user_id (or full user profile) and context (e.g., “home feed” or “current_item=doc456”). Processing:

Retrieve the user’s profile (fast lookup from cache or DB).

Retrieve candidate items using multiple strategies:

Vector Similarity Search: Take the user’s embedding vector and query the vector database for nearest neighbor content vectors (find content similar to user’s overall interests) (Unlocking the Power of Vector Databases in Recommendation Systems | by Juan C Olamendy | Medium) (Candidate Generation | Pinecone).

Knowledge Graph Query: Use the knowledge graph to find related items (e.g., “User is interested in X, find content connected to X in the graph”). For context, also find items related to the current context node (like if current document is Y, get its neighbors in the graph).

Collaborative Filtering / Similar Users: Find users with similar profiles and pick items they liked that this user hasn’t seen.

Rule-based Candidates: Include any admin-specified items (as per rules) or popular trending items if relevant.

Ranking/Scoring: For each candidate, compute a relevance score. This could involve a learned model (e.g., a rerank model or an LLM that given user profile and item description, predicts a relevance score). It might also include diversity logic (ensure not all items are nearly identical) and context relevance boost (items matching current context get higher weight).

Select top N items by score as the final recommendations.

Optionally, format the results (title, snippet, maybe an explanation “because you viewed…”) for output.Output: A list of recommended items (with identifiers, and any metadata needed by UI like title or snippet). For example, an output JSON might look like:

{

"user_id": 123,

"context": "home",

"recommendations": [

{"item_id": "doc789", "reason": "similar_to_interest: AI", "score": 0.95},

{"item_id": "vid456", "reason": "from_project: X", "score": 0.90},

...

]

}

The “reason” field is optional, used for explainability (why it was recommended).

Profile Access Function: Input: user_id and request for profile data. Processing: retrieve profile from data store (with proper auth), possibly filter out sensitive fields depending on who’s requesting (user themselves vs admin). Output: the profile object (or requested part of it). This is mainly for internal use or admin/debug, but users might have the right to see their own profile data as part of GDPR compliance.

All these functional components work together: the ingestion and profile update prepare the data so that when a recommendation is requested, it can be served instantly from an up-to-date profile. The design is both event-driven (profiles updated by events) and on-demand (recommendations computed on request) to balance freshness and performance.

Non-Functional Requirements (NFRs)

Beyond functionality, the Personalization Engine must meet a range of non-functional requirements to ensure it is performant, secure, and maintainable in a production environment:

Performance: The system must operate with minimal latency. Specifically, the engine’s inference latency for generating a set of recommendations should be under 300 ms on average (including model inference and data fetching). Profile read operations (fetching a user profile for personalization) should be extremely fast (<100 ms), likely served from an in-memory cache or fast data store. The pipeline from capturing a user event to updating that user’s profile should ideally be sub-second, aiming for ~300 ms end-to-end so that subsequent recommendations reflect that event. The engine should also handle high throughput – e.g. processing thousands of events per second – using streaming and parallel processing. It will incorporate load shedding or graceful degradation if the system is overwhelmed (for example, temporarily sampling events if input rate is extreme, ensuring core functionality continues).

Scalability: The architecture must support scaling to a large user base (10M+ users) and high concurrent usage (100k simultaneous active users generating events and requests). This will be achieved via horizontal scaling of stateless services (multiple instances behind a load balancer for the API component) and partitioning of stateful stores (sharding the vector database or using a managed service that scales automatically). The design should separate concerns so that different components can scale independently – e.g., the streaming ingestion can scale consumers as event volume grows, the recommender API can scale based on request QPS, and the databases can scale in storage and query throughput. Use of cloud-native, serverless or managed components (like a serverless vector DB) can aid effortless scaling (Candidate Generation | Pinecone). The system must also handle data scale: up to tens of millions of profile records and potentially billions of interaction events. Techniques like approximate nearest neighbor search in vector DB ensure even large volumes can be queried quickly (Unlocking the Power of Vector Databases in Recommendation Systems | by Juan C Olamendy | Medium) (Unlocking the Power of Vector Databases in Recommendation Systems | by Juan C Olamendy | Medium).

Reliability & Availability: The engine should be highly reliable. Target availability is 99.95% or higher, meaning downtime is limited to only a few minutes per month. To achieve this, it will use redundancy: multiple instances in active-active failover configuration. If one instance or microservice fails, another takes over seamlessly (e.g., multiple replicas of the recommendation service behind a load balancer, multi-AZ deployment for databases). The system will implement resilient inference paths – for example, if the primary ML model service is down, a backup model or a cached set of recommendations is used (so the user still gets something, perhaps slightly less personalized, instead of an error). We will also use circuit breakers and retries in inter-service calls: if the vector DB or knowledge graph is not responding, the engine can skip that component after a timeout and still return a partial result rather than nothing. Data integrity and consistency is also part of reliability: the system must handle event processing exactly-once or at-least-once such that profiles don’t miss updates. In case of any inconsistency (e.g., profile fails to update), there should be self-healing mechanisms (like a periodic batch job that reconciles and fixes any discrepancies).

Security: Security is paramount given the personalization engine deals with sensitive user data and preferences. The engine will be built following Zero Trust architecture principles – every access is authenticated, authorized, and encrypted (What Is Zero Trust Architecture? | Microsoft Security). All API calls must include valid authentication tokens (likely OAuth2/JWT from the upstream auth provider), and the engine will enforce role-based access control (RBAC) on its endpoints. For example, a user can only retrieve their own profile (or data explicitly shared with them), whereas an admin might have access to manage profiles within their domain. All network communication will use TLS for encryption in transit, and any sensitive data at rest (personal profiles, raw interaction logs) will be encrypted with strong encryption keys. The system will also be containerized and run in an isolated microservice environment with limited network access, reducing attack surface. Additionally, adhere to least privilege: each microservice or component only has access to the data and secrets it absolutely needs. Regular security audits and penetration testing will be done on this component given its critical nature.

Privacy: The personalization engine must comply with GDPR, PIPEDA, and other data privacy regulations. This includes obtaining and honoring user consent for data collection and personalization. The engine will maintain an audit log of personal data usage – who/what accessed a profile and when – to support transparency. It will also support the right to be forgotten: if a user requests deletion of their data, the engine must delete the user’s profile and associated personal data from all stores (or anonymize it) within the required timeframe. Moreover, any algorithms used should have explainability provisions – especially if they have a significant impact on the user’s opportunities or access to information, we need to be able to explain, at least in general terms, why certain recommendations are made (e.g., “You’re seeing this because you showed interest in ...”). While deep learning models can be opaque, we will include features like storing the top contributing factors for a recommendation (such as content tags or past actions that led to it) to aid in explanation. Privacy by design will be followed, meaning we minimize personal data usage (only data that improves personalization is used, and aggregated/anonymous data is used wherever possible). Differential privacy techniques may be applied when analyzing data in aggregate – for instance, if generating global insights or training global models, ensure that no single user’s data can be pinpointed (adding noise or using federated approaches – see Future Considerations).

Maintainability: The system should be easy to update and extend over time. A modular architecture will ensure different pieces (data ingestion, profile management, recommendation logic, model inference) are separated so that changes in one don’t overly impact others. For example, the recommendation algorithms/models will be encapsulated such that they can be updated or replaced (with a new model version, or even a different approach) without rewriting the whole system. The engine will use feature flags for rolling out new personalization features or model tweaks gradually. For instance, a new algorithm can be deployed but kept disabled by a feature flag, then enabled for a small subset of users for A/B testing, and rolled out fully once validated. The codebase will be well-documented, follow standard frameworks (e.g., using FastAPI for the web service provides built-in structure), and include comprehensive test suites to catch regressions. Logging and monitoring (discussed below) also contribute to maintainability by making it easier to diagnose issues. We aim for a design that any engineering team member can quickly understand: clear interfaces between components, using industry-standard tech (Python, Rust, etc.) rather than overly custom solutions when possible.

In summary, the engine is being built not just for what it does, but how it does it – quickly, at scale, reliably, securely, and in a way that can evolve with the project’s needs.

Technical Design & Architecture

The Personalization Engine will be implemented as a set of cooperating services and components, following a microservice and event-driven architecture. The design emphasizes separation of concerns: ingestion and processing of data streams, model inference, and serving recommendations are handled by specialized components that communicate through well-defined APIs or messaging. Modern languages and frameworks are chosen for each component to optimize performance and developer productivity – for example, high-level orchestration in Python (with FastAPI for web services), performance-critical data processing in Rust (possibly compiled to WebAssembly for portability), and real-time dataflow via established streaming platforms (Kafka/Flink).

Architecture Overview: At a high level, the engine consists of the following major layers and components:

(Knowledge Graph Integration in a RAG architecture) High-level architecture combining a semantic knowledge model layer with vector search and LLM capabilities. In our design, a domain knowledge graph and vector index provide context to personalization models, ensuring recommendations leverage both structured relationships and unstructured embeddings (Knowledge Graph Integration in a RAG architecture). An LLM (or other ML models) can then generate or refine suggestions using this rich context.

From a flow perspective:

Event Ingestion Layer: This is an event stream (Kafka topics or similar) where all user interaction events are published. Producers include frontend apps or other services that send events like “user X viewed item Y”. This layer buffers and distributes events to processing consumers.

Stream Processing & Preprocessing: A real-time processing job (built on Apache Flink or Kafka Streams) subscribes to the events. Written in a high-performance language (Java/Scala for Flink, or Rust via something like Apache Fluvio or custom Rust consumer) it performs tasks like aggregating streams, computing features, and calling out to external enrichers. For instance, it might maintain running counts of certain user actions (for quick features like “how many times in last hour has user done Z”) and push those into the profile store. It could also do things like sessionization (group events into sessions) or detect certain patterns (e.g., the user suddenly consumed lots of content in a new topic – a trigger to maybe diversify recommendations).

User Profile Store / Service: The profile data for each user is stored in a fast database. This could be a NoSQL document store or a graph database or even a specialized user profile service. We are considering using a graph database (Neo4j or Amazon Neptune) to store the user profiles along with their relationships to content and other entities as a knowledge graph. The profile service can be implemented as a Python FastAPI service that provides an API to retrieve or update profiles (backed by the DB). However, for high throughput, many updates might be done asynchronously via the stream processor writing directly to the store, rather than a synchronous web call.

Vector Embedding Service & Store: For handling high-dimensional representations, we incorporate a vector database (such as Pinecone or Weaviate). The content items (documents, etc.) are offloaded to this vector DB by storing their embeddings. We will generate embeddings for content using transformer-based encoders (e.g., use OpenAI’s API or an open-source model to get a vector for each document, possibly combining text, title, etc.). These vectors are indexed in Pinecone/Weaviate for similarity search. Similarly, user profiles can also be represented as vectors and stored here (though they could be recomputed on the fly too). The vector DB provides APIs to query “nearest neighbors” which is crucial for finding related content quickly (Unlocking the Power of Vector Databases in Recommendation Systems | by Juan C Olamendy | Medium) (Unlocking the Power of Vector Databases in Recommendation Systems | by Juan C Olamendy | Medium). This service is likely managed (Pinecone offers a managed serverless vector DB (Candidate Generation | Pinecone)) so the architecture offloads that complexity.

Knowledge Graph / Contextual DB: In addition to vectors, the engine uses a unified knowledge graph to represent relationships: users, content, topics, skills, teams, etc., all interconnected. For example, the graph can represent that User123 is a member of Project Alpha, and Document 456 is tagged with Topic X which is relevant to Project Alpha. The graph can be queried to find things like “other users in Project Alpha who read similar content” or “content related to Topic X that User123 hasn’t seen yet”. This graph can reside in Neo4j or Neptune. The Domain Knowledge Model (taxonomies, ontologies for the domain) acts as a schema for this graph (Knowledge Graph Integration in a RAG architecture), ensuring consistency in how entities are linked. The knowledge graph complements the vector approach with a more explicit, explainable reasoning path.

Recommendation Service (Orchestration Layer): This is the central service (likely Python FastAPI or NodeJS) that clients interact with to get recommendations or personalization decisions. When a request comes in (e.g. GET /recommendations?user=123&context=home), this service orchestrates the personalization logic:

It calls the Profile Store to get user profile (or retrieves from an in-memory cache if recently fetched).

It queries the Vector DB for similar items or uses recent user vectors to find neighbors.

It queries the Knowledge Graph for contextual info (like relationships for diversification or relevant content by tag).

It may call one or more ML model endpoints. For instance, a personalization ML model (which could be a fine-tuned neural network or LLM-based system) that given the profile and candidate items, scores or ranks them. This could be a separate microservice – e.g., a Python or Rust service running the model, or an external ML inference service.

It merges these results and produces the final recommendation list.

It also handles applying any business rules (e.g., filtering out certain content if user shouldn’t see it, enforcing admin overrides) before returning.

This service is stateless (aside from caching) and can be scaled out behind a load balancer. It communicates with other components via APIs (REST/gRPC) or database queries as appropriate.

Model Inference Components: We anticipate using fine-tuned Large Language Models (LLMs) and other ML models as part of the engine’s decision-making. These models might include:

An LLM that can generate personalized suggestions or explanations. For example, after getting a set of candidate items, an LLM could generate a short summary or reasoning for the top few (“Since you showed interest in X, you might like Y and Z which cover the basics and advanced concepts respectively”).

Embedding models (could be on-the-fly via OpenAI API or a hosted model) for computing new embeddings when new content arrives or periodically updating user embeddings.

Possibly a reinforcement learning agent for adjusting recommendations (future).

These can be deployed via a model server. If using Python, frameworks like TensorFlow Serving or PyTorch’s TorchServe or FastAPI itself can serve the models. If performance is critical, models can be implemented in or accelerated by Rust (e.g., using Rust for preprocessing inputs to the model, or using ONNX runtime with a Rust binding for speed).

Some models (like simple scoring functions) might be directly embedded in the Recommendation Service code, while others run in separate processes (to isolate heavy computations).

Edge Personalization (WebLLM): For certain scenarios, the architecture supports running personalization logic on the client side (browser or edge device). Using WebLLM, which enables running LLMs in-browser with WebGPU acceleration, we can perform inference without a server round-trip (MLC | WebLLM: A High-Performance In-Browser LLM Inference Engine). For example, if a particular quick suggestion can be generated by a small model that runs in the user’s browser, we offload it to improve latency and privacy (the data stays on the client) (MLC | WebLLM: A High-Performance In-Browser LLM Inference Engine). A concrete case: a browser-side script could maintain a lightweight model of recent user actions and predict the next UI adaptation immediately. Edge inference might also serve as a fallback if the user is offline or the server cannot be reached; the browser could still produce basic recommendations from a locally cached model. (Of course, client-side models are limited in size/complexity due to device constraints, so this is complementary to server-side heavy lifting.)

Collaborative Filtering Module: (This could be part of the model layer or separate.) For personalization, especially in a knowledge platform, we might incorporate collaborative filtering (users similar to you). A service could periodically compute user-user or user-content similarities offline (e.g., matrix factorization) and store those in the graph or a database. The real-time system can then query those precomputed neighbors as additional candidates. This module likely runs as a batch job (Spark or similar on user interaction history) rather than a live service, but it’s considered in architecture for completeness.

All components communicate through defined Interfaces & APIs (detailed in the next section). The architecture ensures loose coupling: for example, the Recommendation Service doesn’t need to know whether the data came from Kafka or how the model is implemented; it just calls an interface. This makes it easier to replace internals (e.g., swap Kafka for another streaming system, or change the ML model) without affecting the whole system.

Microservice Design Justification: Breaking the engine into microservices (ingestion, profile store, recommendation API, model serving, etc.) allows each to be developed, deployed, and scaled independently (Real-Time Personalization Using Microservices ) (Real-Time Personalization Using Microservices ). For instance, if the ML models need GPU instances, we isolate them in a service that runs on GPU-enabled nodes, separate from the main API. If the streaming throughput grows, we can scale the Kafka/Flink consumers without touching the recommendation logic. It also improves fault isolation – one component failure doesn’t necessarily take down the entire personalization system. We will use gRPC or REST for internal APIs; gRPC is highly performant for service-to-service calls and supports streaming, which might be beneficial for feeding data or real-time updates (Real-Time Personalization Using Microservices ). For example, the profile update pipeline could push updates via gRPC streams to an in-memory service that caches profiles for immediate use.

Data Flow Example: To illustrate, consider a user reading an article:

Frontend sends a “view” event to the ingestion API, which drops it into Kafka.

The stream processor picks it up, enriches it with article tags via the knowledge graph, and updates the user’s interest profile (in the graph DB and possibly recalculates their embedding).

A few seconds later, the user goes to their home feed and the client calls GET /recommendations. The Recommendation Service fetches the now-updated profile (which reflects the article they just read), queries the vector DB for similar items (finding more articles in that topic) and the knowledge graph for related items (maybe finds a related course or an expert user on that topic). It then ranks and returns a blend of these – which now prominently includes content related to the article they just read, matching their immediate interest.

Technology Stack (Summary):

Backend Services: Python (FastAPI) for APIs and orchestration, due to its fast development cycle and rich ML ecosystem; Rust for performance-critical tasks (like a service to do heavy batch computations or real-time signal processing, leveraging Rust’s efficiency and safety).

Data Processing: Kafka for event bus, Apache Flink for complex event processing (if needed for stateful streaming aggregations).

Databases: Pinecone/Weaviate (Vector DB for embeddings), Neo4j/Neptune (Graph DB for knowledge graph and possibly profiles), Redis or similar (caching layer for quick profile access or feature flags).

ML/AI: Use of transformer models (could be hosted via HuggingFace or OpenAI for embeddings and maybe for a generative model). Fine-tuned domain-specific LLMs for the platform’s recommendation nuances. WebAssembly (via tools like wasm-bindgen or TF.js WASM backend) to possibly run smaller models in-browser or in a secure sandbox.

Frontend: Next.js for the web app, which will consume the personalization API and also potentially run edge personalization modules (via WebAssembly and WebLLM as mentioned).

This architecture is designed to be modular, real-time, and intelligent, combining content-based filtering (via embeddings), knowledge-based reasoning (via graphs), and collaborative insight (via user behavior patterns), orchestrated through modern microservices.

Interfaces & APIs

The Personalization Engine exposes several interfaces that other components (like front-end clients or other back-end services) will use. It also consumes certain APIs for enrichment. All APIs follow RESTful principles and use JSON for request/response (except where noted), and are secured with authentication (e.g., an OAuth2 bearer token or similar). Below is a list of the key APIs and interfaces:

Exposed APIs (for clients to use)

GET /profile/{user_id} – Retrieve a user’s personalization profile.

Description: Returns the stored profile for the given user. This can be used by an authorized client to fetch profile details (for example, an admin dashboard to view user interests, or a user downloading their own profile data).

Response: JSON object containing profile fields. For example:

{

"user_id": "123",

"interests": {"Machine Learning": 0.8, "Data Privacy": 0.6},

"recent_activity": ["doc456", "doc789", ...],

"embedding": [0.12, -0.07, ... 768 dimensions ...]

}

The embedding field might be omitted or stored separately for size; including it depends on needs. Sensitive fields (if any) are filtered out unless caller has admin scope.

Auth: Requires scope profile:read for the target user (the user themselves, or an admin with appropriate rights).

Performance: This should be a quick lookup (possibly hitting an in-memory cache). Target <50ms response time.

POST /signals – Submit one or more user interaction signals/events.

Description: This endpoint allows the client (typically the front-end) to send user events to the engine. In many cases, front-ends might send events directly to a Kafka endpoint or analytics service, but this API provides a direct way if needed (it could forward to the Kafka pipeline). Supports batch submission of events for efficiency.

Request Body: JSON, e.g.:

[

{"user_id": "123", "event": "view", "item_id": "doc456", "timestamp": 1691042215000, "context": "workspaceA"},

{"user_id": "123", "event": "like", "item_id": "doc456", "timestamp": 1691042230000}

]

Here we send two events: user 123 viewed doc456 at time and then liked it. The context could be optional (workspace or page context). Additional fields like device or session can be included as well.

Response: HTTP 200 OK with a status message. Possibly queue offsets or event IDs if we want to track them, but typically just acknowledgment.

Auth: The user’s auth token or an internal service token must allow sending events (likely any authenticated user can send their own events; an admin or service could send events on behalf of others for backfill).

Behavior: This API should not perform heavy processing synchronously. It will validate and then enqueue the events to the internal event bus or call the stream processor. It should be durable – perhaps using a fire-and-forget pattern or minimal blocking. If the internal queue is down, it should implement a retry or return an error.

Rate limiting: Might apply to prevent abuse (e.g., a malicious client spamming events).

GET /recommendations/{user_id} – Get personalized recommendations for a user.

Description: Returns a list of recommended items for the specified user, optionally tailored to a given context. This is the main API that the front-end will call to get content suggestions, either for populating a feed or when the user requests recommendations.

Query Parameters:

context (optional) – Provides context for recommendations. For example: context=home for general homepage feed, context=item:doc456 for “related to doc456”, or context=project:abc to indicate we want recommendations in the scope of project ABC. We can define a small syntax for context values.

limit (optional) – Number of recommendations desired (default maybe 10).

Other filters could be included such as type=article if the client only wants certain content types (the engine can also handle that).

Response: JSON like:

{

"user_id": "123",

"context": "home",

"recommendations": [

{

"item_id": "doc789",

"title": "Understanding Differential Privacy",

"type": "article",

"score": 0.92,

"explanation": "Based on your interest in Data Privacy"

},

{

"item_id": "user456",

"name": "Dr. Jane Smith",

"type": "expert",

"score": 0.85,

"explanation": "Collaborator in your project"

}

]

}

Each recommendation has an item_id (which could refer to content or even other entities like a user/expert), a human-readable field (title or name), a type to know what it is, and a score or rank. The explanation is an optional field for transparency (“why am I seeing this?”). In a UI, these might be shown as separate sections if types differ (e.g., content vs people).

Auth: The requesting user must be the user_id themselves (or have rights to get recommendations on their behalf). Typically the user’s own token, or possibly a service with recommendation:read scope for that user.

Behavior: The server will orchestrate as described to gather and rank recommendations. If the context is a specific item or project, it will constrain the candidates to that context (e.g., only items from that project or related to that item). If no context, it’s general. This call should be fast enough for interactive use (<300 ms processing on server) and results cached if the user repeats the request quickly (to avoid duplicate heavy computation). We might implement an ETag or timestamp so the client can ask “give me new recommendations since X” to only get changed items after new events.

Pagination/Streaming: If large lists are needed (not likely for UI, but maybe for an API usage), we could support pagination parameters or use cursoring. Initially, a simple one-page response is fine.

POST /admin/rules – Define or update personalization rules.

Description: (Admin only) Allows administrators to set custom rules that affect the engine’s behavior in specific contexts. For example, pinning certain content, or specifying that in workspace X, topic Y is always boosted.

Request Body: JSON defining the rule, e.g.:

{

"scope": "workspace:ABC",

"condition": {"new_user": true},

"action": {"add_recommendation": "doc123"}

}

This hypothetical rule says: in workspace ABC, if the user is new (perhaps determined by no prior activity), ensure doc123 is added to their recommendations. The exact schema for rules needs definition (could be a simple if-then structure or a small rule language). Rules might also include things like “filter out content with tag X for users of type Y” etc.

Response: 200 OK or error if rule syntax invalid.

Auth: Strictly admin role with a scope like personalization:manage. Regular users wouldn’t have access.

Effect: The personalization engine will need to store these rules (perhaps in the knowledge graph or a separate config store) and apply them during recommendation generation. This API simply provides a way to manage them at runtime.

Note: A retrieval API (GET /admin/rules) would also be provided to list current rules, for admin interfaces.

WebSocket Channel (Real-time Updates): For scenarios where the UI wants to receive personalization updates live (without polling), the engine will support a WebSocket or Server-Sent Events channel. For example, GET /ws/recommendations?user_id=123 could upgrade to a WebSocket that pushes new recommendation data whenever the user’s context changes significantly or a new event triggers an update. This is useful in collaborative scenarios – e.g., if another user’s action creates a recommendation for you, you might get it in real-time (“Colleague added a document that might interest you”). The WebSocket messages would contain similar payloads as the REST API but pushed by server-initiated events.

Consumed APIs (the engine calls these external/internal services)

Content Tagging Service API: GET /content/{item_id}/tags (or a batch endpoint) – Returns metadata about a content item, such as its topics, keywords, or category. The Personalization Engine uses this to enrich raw events and to understand content for the knowledge graph. For example, when a user views item doc456, the engine might call this service to find that doc456 is tagged with Machine Learning and Neural Networks tags, which it then uses to update the user’s interest profile or to find related items. This service might be powered by an NLP pipeline or a lookup of precomputed tags. (If integrated with the knowledge graph, the engine might instead query the graph for this info.)

Collaboration/Organization API: GET /org/teams?user_id=123 – Returns info about the user’s teams, projects, or peers. The engine could call an API from the collaboration module of Synapse Web to get, say, “User123 is in Team X and Team Y, and their mentor is User789”. This information can feed into personalization (like boosting content created by team members, or recommending connecting with their mentor if they haven’t yet). Alternatively, this data could be synced into the knowledge graph in advance. If not, the engine will pull it on demand.

Analytics/Logging API: In addition to processing events for personalization, we likely forward them to a central analytics system for aggregate analysis. The engine might call or stream data to a service that logs events for historical data analysis or A/B test analysis. This is more of a background integration: ensure events get to both personalization logic and analytics database. If using something like Segment or a data warehouse pipeline, the engine will have hooks to send events there as well.

User Profile Service (Auth) API: GET /users/{user_id} – To fetch basic user info (if needed) like name, role, or preferences that are not captured via interactions. For instance, a user might have explicitly stated interests in their profile settings; the engine could call an auth/user service to retrieve those and seed the personalization profile. Or fetch privacy settings/consent flags for the user.

Notification Service API: In some cases, the engine might proactively send notifications. E.g., if a very important recommendation is identified (say, a critical update relevant to the user), the engine could call a notification service POST /notify with user_id and message to push an in-app notification or email. This is more on the edge of personalization vs. user engagement, but it could be utilized for certain features (especially predictive suggestions that are time-sensitive).

All consumed APIs will be accessed with appropriate timeouts and error handling (to not stall the main engine if an external call is slow). Where possible, data from these APIs is cached or pre-loaded (e.g., content tags could be cached in memory or a local DB for quick access).

API Schema Definitions: We will maintain an OpenAPI (Swagger) specification for all exposed APIs, detailing request/response schemas and authentication requirements. This will serve as a contract for front-end and integration teams. Similarly, for internal consumption, we document the expected request/response of external APIs we depend on.

Authentication & Authorization: All external calls to the engine must include an Authorization header. The engine will integrate with the broader Synapse Web auth system (likely JWTs issued by an identity service). It will validate tokens and enforce scopes: for example, token must have uid=={user_id} for accessing that user’s recommendations, or an admin scope for others. The engine might also do additional checks, e.g., if workspace context is provided, verify the user has access to that workspace.

Versioning: The APIs will be versioned (e.g., /v1/recommendations/...) to allow non-breaking changes and future enhancements. The first iteration will start at v1.

WebSocket/Event Interface: The real-time update channel will likely not be open to third-party but used by our own front-end. It will require the same auth token on connect (JWT in query param or during handshake) to authenticate the user. The server can push messages like {"type":"update", "recommendations": [...], "context":"X"} whenever needed.

By defining these interfaces clearly, we enable the front-end and other systems to integrate seamlessly with the personalization engine. The contract ensures that as long as the inputs are provided, the engine will return personalized outputs as specified.

Data Management

Handling data effectively is crucial for the personalization engine, given the large volume of user events and content relationships. This section covers how data is modeled, stored, and governed within the system.

Data Schemas & Models

User Profile Schema: Each user profile can be stored as a structured document (JSON) or as nodes/edges in the knowledge graph. Key fields include:

user_id (string/ID)

preferences: a map of topics/keywords to a weight or score (e.g., "Machine Learning": 0.8). These could be derived from tags of content the user interacted with.

skills or knowledge_levels: (If applicable) map of skill areas to proficiency (e.g., "Python": expert, "Rust": beginner).

recent_items: list of last N items the user engaged with (for recency-based features).

embedding_vector: (optional) an array of float32 of length D (e.g., 512 or 768) representing the user in latent space. If stored, this might be in a separate vector index keyed by user_id.

contextual_profiles: a nested structure if we maintain per-context profiles, e.g.,

"contextual": {

"workspace:ABC": { "preferences": {...}, "recent_items": [...] },

"project:XYZ": { ... }

}

consent_flags: e.g., {"personalization": true, "data_collection": true} to note if user allowed certain uses.

last_updated: timestamp of last profile update.

Content/Item Schema: For each content item (document, etc.) relevant to personalization:

item_id

metadata: title, type, author, etc.

tags: list of tags or topics

embedding_vector: vector representation of the item’s content.

In the knowledge graph, items are nodes connected to tag nodes (topics), to author (user) nodes, etc.

Interaction (Signal) Schema: Each interaction event stored (beyond just transient processing) could be a record like:

event_id, user_id, item_id (or target entity id), event_type (view, like, share, etc.), timestamp, context (if any).

Possibly value (like rating 4 stars, or dwell time 120s).

This can be stored in an append-only log (for audit/training) and/or aggregated for quick use.

Recommendation Output Schema: (For logging results) whenever recommendations are generated and shown to a user, we may log a record of what was shown:

user_id, context, timestamp, recommended_items: list of item_ids that were presented, along with perhaps the model version or strategy used.

This allows analysis of recommendation performance later (especially if we join with what the user clicked).

All these schemas will be defined in a schema registry or documented in the code (e.g., using Pydantic models in Python for API inputs). Using strict schema validation helps catch any unexpected data (for example, if an event missing an item_id arrives, it will be rejected and logged as an error).

Storage Design

We employ a tiered storage strategy to balance speed, cost, and volume:

Hot Storage (Cache): The most frequently needed data (like active user profiles and recent events) will reside in-memory or in a fast cache. We will use Redis or a similar in-memory data store to cache user profile objects and maybe the latest recommendations. This allows the Recommendation Service to fetch profiles with minimal latency and also to store recently computed recommendations (if a user refreshes the page quickly, we can serve from cache instead of recompute). This cache will have eviction policies (LRU or time-based) and isn’t the source of truth, but a performance layer.

Primary Storage (Operational DBs):

Vector Database: Pinecone/Weaviate acts as the primary store for embeddings. We will treat this as the source of truth for the vector representations of items (and possibly users). It provides persistence (backed by its cloud storage) and query capabilities. We should design the index in it properly, e.g., one index for content vectors (with appropriate metadata filters available), and possibly another for user vectors if we choose to store them there. Each item stored could have metadata like tags, timestamp, etc., to allow filtered searches (e.g., search within same workspace or exclude items user has seen (Candidate Generation | Pinecone)).

Knowledge Graph Database: Neo4j or Neptune holds the relationships and potentially the user profiles. For example, a user node connected to topic nodes with a weight property on the edge to indicate interest strength, or a “LIKED” relationship between user and item nodes for items they liked. The graph is useful for complex queries and also for explainability (we can traverse “User -> liked -> Item -> has tag -> Topic -> related Item” to explain a recommendation). This DB persists all such relationships. It may also serve as the profile store (since from the graph one can derive the profile). Alternatively, if we decide to use a document store for profiles:

Document/NoSQL Store: We could use MongoDB/DynamoDB or similar to store the JSON profile documents for each user. This makes retrieving and updating a profile straightforward. Each profile is keyed by user_id. This can work in tandem with the graph: updates could go to both the doc store and the graph (or we could generate the graph view on the fly from the doc, but probably dual-writing for performance).

Relational Store (if needed): Some data might fit a SQL model (like logging events or recommendation logs). A Postgres or cloud data warehouse could store historical interactions for analytics. But for the real-time loop, we rely on the above specialized stores.

Cold Storage (Data Lake): All interaction events and periodic snapshots of profiles will be written to a data lake or archive (e.g., AWS S3, Azure Data Lake) in parquet or JSON format. This is for compliance (audit trail) and for retraining models or performing historical analysis. For example, after a year, the streaming events in Kafka can be compacted or purged, but they would have been saved to S3 already. Cold data is not used in real-time personalization, but is available for offline processing (like building a new model using last year’s data).

Data Volume Considerations: With 10M users, if each profile JSON is ~1KB, that’s on the order of 10 GB which is fine for a database. Vectors might be 768 floats (3KB) per item; if there are 1M content items, that’s ~3GB, again manageable with vector DBs. Interaction logs could be billions of records – hence the need for a data lake to archive.

Data Access Patterns:

Profile reads: very frequent (every recommendation request), hence cached and indexed by user_id for quick lookup.

Profile writes: frequent (every user event leads to a small write/update). Need to handle high write throughput. Techniques: use append-only logs plus periodic merges to avoid contention, or partition by user and use eventually consistent updates. Since each user mainly affects their own profile, we can shard by user_id.

Vector searches: frequent when generating recos. These are read operations on the vector DB, which is optimized for that (approximate nearest neighbor queries).

Graph queries: moderately frequent for context-related lookups, but we will design them to be targeted (like find related items by topic – which can be one-hop or two-hop queries, which graph DBs handle quickly if indexed). Very complex graph algorithms might be done offline if needed.

Batch processing: occasionally, we might recalc something for all users (like a migration or re-embedding). Those would use the cold data with a big data job, not burden the online system.

Data Validation and Quality

To ensure data quality:

All inputs (events, API payloads) are validated via schema (using Pydantic or JSON Schema). This catches structural errors.

We will implement anomaly detection on profile data: e.g., if a user’s interest weights all drop to 0 or skyrocket to 100 unexpectedly, flag that. Or if a profile hasn’t updated in a long time despite events, that means updates might be failing.

The system could have a consistency checker (maybe a periodic job) that verifies consistency between stores: e.g., pick random user, ensure their profile doc, graph node, and vector embedding all correspond (if not, reconcile by regenerating any missing parts). This can catch bugs where, say, vector DB didn’t update an embedding.

When integrating external data (tags, etc.), log if unknown IDs or mismatches occur.

Use unique IDs and idempotency where appropriate: for instance, each event could carry an ID so if it gets processed twice, we detect and avoid double-counting profile updates (exactly-once processing in streaming).

Privacy and Retention

In line with privacy, the data management includes:

PII Handling: Personal data like names or emails would primarily live in the auth system, not in the personalization data (which deals more with behavior data). If any PII (even user_id which could be considered personal) is stored, it’s protected. We might hash user IDs when storing in certain logs to pseudonymize.

Anonymization: For analytics or model training, we can strip direct identifiers and use just aggregated or anonymized data. If sharing any data with third-party services (like using OpenAI API for embeddings – which technically sends content text), ensure we have allowed data to leave the system and not include user-identifiable context if not permitted.

Differential Privacy: In any aggregate reporting from this engine, we consider adding noise. For example, if the system provides a feature like “X% of users in your company liked this document”, it should use DP to not leak anything about a single user’s action if the group is small. This is a future consideration for any user-facing stats.

Data Lifecycle: We will define retention policies: e.g., keep detailed interaction logs for 1 year, then purge or aggregate. Profile data is maintained as long as the user is active; if a user is deleted or inactive beyond a threshold, profile data will be deleted or archived safely.

User Data Export/Delete: The engine must support exporting a user’s profile data (this can be done by collating their profile and relevant history from logs) when requested. And support deletion: that means removing their profile entry, their vectors from the DB (e.g., Pinecone delete by ID), edges in the graph, and flagging or deleting their historical events. We might not physically delete events from cold storage (for historical reasons) but we can at least disassociate the user ID (anonymize them) as an alternative if allowed by policy.

By carefully managing data with these schema definitions, storage tiers, and validation processes, we ensure the personalization engine has high-quality data to work with, which is essential for accurate recommendations. It also ensures compliance and scalability as data grows.

Error Handling & Logging

A robust error handling and logging strategy is in place so that issues in the personalization engine can be detected, diagnosed, and mitigated quickly. The system should be fault-tolerant and transparent in its operations via logs and metrics.

Error Handling Strategy

Graceful Degradation: If a sub-component fails to respond or throws an error, the engine will catch that error and attempt a fallback. For example:

If the ML model service for recommendations is unavailable, the engine might fall back to a simpler content-based recommendation (e.g., using just the vector DB results without advanced re-ranking) and still return a result to the user. It will mark in the response (internally or via an explanation note) that a fallback was used.

If the vector DB query fails (timeout or error), the engine can try a second attempt or fallback to graph-based recommendations or popular items.

If the knowledge graph query fails, it can proceed with just embeddings (maybe sacrificing some context).

In worst-case scenario where no dynamic personalization can be done (e.g., profile missing or all calls failed), the engine returns a default set of content (like trending global content) so the user is never shown an empty page.

Retry Policies: For transient errors (like a momentary network glitch reaching Pinecone or Neo4j), the engine will implement retries with backoff. For example, on a failed call, retry after 100ms, then 200ms, then give up (to keep total time bounded). We will use existing libraries or patterns for this. However, we avoid infinite retries to not hang a user request; typically one or two quick retries within the same request.

Circuit Breakers: If an external dependency is consistently failing (e.g., every call to the tagging service times out), the engine can open a circuit breaker – i.e., stop calling that service for a short period and use cached or default values instead (Top Microservices Design Patterns for Microservices Architecture in ...). During the open state, it immediately skips that call to avoid waste. After a cooldown, it will test the service again. This prevents cascading failure (one slow service making all threads hang).

Data Consistency Errors: If something like a profile update fails (e.g., DB write fails), the system will queue a retry or mark that user profile dirty to retry later. If an event can’t be processed, it goes to a dead-letter queue which triggers an alert. We ensure at least once processing, but if double-processing occurs, we aim for idempotent updates (so applying same event twice doesn’t double count, for example).

Request Validation Errors: If an API request from client is invalid (bad JSON, missing fields), the API will return a 400 Bad Request with an error message detailing the issue. This helps clients correct their usage.

Authentication/Authorization Errors: Return 401/403 as appropriate if token is missing or does not have permission. Include a clear message.

Rate Limiting: If clients exceed rate limits (to protect the system), respond with 429 Too Many Requests. We might implement simple rate checks especially on the POST /signals if needed.

Circuit Breaker for Models: In case an ML model starts producing obviously bad results (could be detected via monitoring), we have the ability to turn it off (via feature flag) and fall back to a previous model or approach. While not automatic error handling, having that toggle is important operationally.

All errors or exceptional conditions should be logged (with appropriate severity) to allow troubleshooting.

Logging and Monitoring

Structured Logging: Every component will emit logs in a structured JSON format (or at least key=value format) rather than plain text. This makes it easier to parse by log management systems (like ELK or CloudWatch). For instance, a log for a recommendation request might look like:

{

"timestamp": "2025-04-02T06:30:15Z",

"level": "INFO",

"service": "recommendation-api",

"user_id": "123",

"context": "home",

"recommendation_count": 10,

"time_ms": 85

}

If an error occurs:

{

"timestamp": "...",

"level": "ERROR",

"service": "recommendation-api",

"error": "VectorDB timeout",

"message": "Failed to retrieve neighbors for user 123 in 200ms, using fallback",

"trace_id": "abcd-1234-efgh-5678"

}

We include a trace_id or correlation ID to tie together logs from a single request. Using an approach like OpenTelemetry, each incoming request gets a unique ID that is passed to downstream calls (e.g., in HTTP headers). All logs related to that request carry the same ID (Real-Time Personalization Using Microservices ). This is extremely helpful for debugging flows across microservices (e.g., linking an API entry log with a model service error log).

Audit Logs: For compliance, any access to profile data outside of the immediate personalization loop (like an admin fetching a profile, or an export) will be logged in an audit log with who accessed what. This is separate from debug logs and is stored securely (perhaps in a write-once store).

Monitoring Metrics: We will collect metrics to monitor the health and performance:

Latency Metrics: e.g., histogram of recommendation generation time, broken down by components (perhaps using tracing spans to measure DB query times, model times, etc.). We’ll track p50, p90, p99 latencies.

Throughput Metrics: number of events processed per second, number of API calls per minute, etc.

Error Rates: count of errors per type (e.g., how many timeouts to vector DB, how many failed recommendations).

Resource Utilization: CPU, memory of each service instance (this is more infra but important to avoid overload).

Profile Update Lag: e.g., metric for how long it takes from an event to profile update (could be measured by timestamp comparisons).

Recommendation Quality Metrics: if online feedback is available (like user clicked a recommendation), log click-through-rate as a metric. Also maybe measure “coverage” (how many unique items are being recommended, to avoid filter bubble).

Drift/Quality Metrics: We might periodically run a job to evaluate model performance (e.g., how current profiles differ from recent behavior) – but those might be offline. At least, monitor distribution of scores (if suddenly all scores drop, something’s off).

Security Metrics: number of unauthorized access attempts, etc., though hopefully none.

All these metrics will be fed into a monitoring system (Prometheus for scraping, Grafana for dashboards and alerts). For example, we’ll set up an alert: “Recommendation latency p99 > 500ms for >5 minutes” triggers an alert to on-call. Or “Error rate of vector DB queries > 5%” triggers investigation.

Traceability: Using distributed tracing (OpenTelemetry or Jaeger) each request can be traced across services. This way, if a particular request is slow, we can see which step took time. This is invaluable for debugging performance issues in such a multi-component system.

Example Log/Alert: If the knowledge graph DB goes down, the logs in recommendation service would show timeouts and usage of fallback. The monitoring system would catch that the error count for “graph_db_timeout” spiked and send an alert to engineers with a message like “KnowledgeGraph queries failing - check Neo4j cluster”. Meanwhile, users still get recommendations (maybe slightly less contextual).

Consistency Checks & Alerts: If the system detects inconsistency (like the anomaly detection mentioned, or a profile update failure), it should log it and possibly emit a metric. For instance, if a dead-letter event happens, increment a “DLQ_event_count” metric and alert if >0 for more than a short period (since ideally that should always be 0). That ensures any lost events are promptly addressed.

In summary, “fail loud, fail safe”: the engine will fail safely by degrading gracefully for the user, but will make noise in the logs/metrics to ensure engineers know something went wrong. The thorough logging and monitoring will allow us to meet our reliability target and quickly improve the system by learning from any issues that arise.

Testing Strategy

To guarantee the quality and reliability of the AI-Driven Personalization Engine, a comprehensive testing strategy will be employed, covering everything from individual functions to the entire system under load, as well as security and privacy aspects.

1. Unit Testing: Every module and microservice in the engine will have unit tests. This includes:

Testing data ingestion parsing logic (e.g., given a sample event JSON, does it produce the correct internal event object or catch errors?).

Testing profile update calculations (feed in some made-up event sequence, verify the profile fields update as expected).

Testing the recommendation ranking function in isolation (inject a fake profile and a set of candidates with known attributes, ensure the ranking logic outputs the expected ordering).

Model unit tests: if we have a custom scoring function or heuristic, test it with fixed inputs. Even for ML components, we can have tests that the interface works (e.g., a dummy model returns a known output for a known input).

Utility functions, e.g., converting vectors, merging profiles, applying a rule, etc., all covered.

We aim for high coverage on critical logic (especially anything involving calculations that could go wrong).

2. Integration Testing: We will simulate end-to-end flows in a controlled environment:

End-to-End Recommendation Flow: Set up a local instance of the profile DB, vector DB (maybe a lightweight in-memory one for test), and model stub. Feed a sequence: user event -> profile update -> recommendation request, and verify the final output uses the event. We might use docker-compose or a test harness that brings up the necessary components. For example, have a test that when user views a document with tag “AI”, then profile shows increased “AI” interest, and a recommendation call returns an “AI”-related item.

Microservice Interaction: Tests that ensure the APIs are properly connected. E.g., call the recommendation API with a known profile in the DB and ensure it calls the underlying model service (we can spy or mock calls in testing).

Streaming Pipeline Test: It's tricky to test Kafka/Flink in unit, but we can simulate by calling the processing function directly with a batch of events and verifying the outputs (profile updates, etc.). For more complex, deploy a test instance of the stream processor that reads from a test topic; produce events; check results in profile store.

Failure Scenarios: Intentionally make a dependency fail in a test (e.g., have the model service endpoint return 500 or time out) and ensure the recommendation still returns something (fallback logic works) and an error is logged. These resilience tests give confidence in graceful degradation.

3. Performance Testing: We will conduct load and stress tests:

Use tools like JMeter, Locust, or k6 to simulate a high volume of API requests to GET /recommendations and POST /signals. For example, ramp up to 1000 req/sec and see if latency stays within limits. This will be done in a staging environment that mirrors production configuration.

Test the streaming pipeline with a high event throughput. Possibly use Kafka performance testing (pushing, say, 10k events/sec) and see if our consumers keep up. Monitor if any lag builds up in processing.

Stress test the vector DB by simulating many concurrent similarity queries; ensure its response time is as advertised. If using a managed service like Pinecone, ensure we are within the usage limits or plan for scaling units.

Memory and CPU profiling under load to detect any bottlenecks or leaks.

We’ll specifically test the 300ms latency requirement: measure the 95th and 99th percentile latencies in the performance test. If it’s higher, we investigate and optimize (maybe caching more, or scaling out).

Scaling tests: Try increasing number of user profiles (we can generate synthetic profiles up to 10 million in a test environment using scripts, to see if the databases can handle it and still query fast).

4. Security Testing:

Penetration Testing: We (or security team) will perform pentesting on the APIs. This includes testing auth (ensure no endpoints allow data without token, test that one user cannot access another’s data by changing the ID, etc.). Also test for injection vulnerabilities (though mostly we use safe libraries, but e.g., ensure no possibility of Cypher injection in graph queries if any user input goes there).

Threat modeling and misuse cases: e.g., test what prevents someone from forging events for another user. We might ensure the POST /signals uses the user’s token and ignores any user_id field (always use authenticated user’s ID to prevent spoofing).

Fuzz Testing: Send random or boundary values to the APIs to see if any unhandled exceptions occur.

Security unit tests: e.g., if role-based access is implemented via middleware, have tests that a normal user token cannot access admin endpoints, etc.

5. Privacy Testing:

Check that when a user opts out or is deleted (simulate that action), their data is indeed no longer used:

Write a test that marks a user’s consent as withdrawn, then generate some events and ensure the profile is not updated or recommendations revert to default.

After “deletion”, ensure calls to profile API return not found, and that their data is gone from DB (or at least inaccessible).

If we provide data export, test that it returns all relevant info and nothing more (no other user’s data).

If we have an explainability feature (like returning explanation fields), test that they are present and make sense for a variety of recommendations.

6. Model Evaluation (Offline testing): The ML components require their own evaluation:

We will use offline datasets (e.g., past interaction logs) to measure recommendation precision/recall, diversity, etc. This isn’t exactly a test that runs in CI/CD, but part of development: before deploying a new model, run it on a test dataset to ensure it meets >85% precision or other metrics. This corresponds to our objective metric.

We might simulate user behavior to test adaptive learning: e.g., create a fake user that changes interests, feed events, verify the engine adapts within a certain number of interactions. This can be automated to ensure the system is responsive to change (this is more like a scenario test).

7. Acceptance Testing: Finally, define acceptance criteria that must be met before launch:

Precision KPI test: Using either offline evaluation or a controlled A/B test, demonstrate that the recommendations have at least 85% precision (which might mean if we show 10 recommendations, on average at least 8.5 are relevant – measured by whether the user engaged with them or by a labeled dataset).

Latency SLA test: In a staging environment under nominal load, 99th percentile latency for recos is < 300ms.

Failover test: Simulate a node failure (kill one instance of a microservice) and verify the system continues serving (the user maybe experiences at most a slight delay but no outage).

Data deletion test: Trigger a GDPR delete for a test user and then attempt to retrieve any of their data through all means – ensure nothing remains. This might involve checking logs, DBs, caches for the user_id.

Explainability check: If explainability is a feature, ensure that for at least X% of recommendations, the system can provide a non-trivial explanation (not just blank or generic text).

8. Continuous Testing and CI/CD: We will integrate these tests into our CI pipeline. Unit tests run on every commit. Integration tests run on merge to main or pre-release. Performance tests might run on demand or on a schedule (since they are heavy). Security tests like dependency vulnerability scanning will also be in CI.

9. Beta Testing: Before full release, we might do a beta with internal users to gather qualitative feedback – this is more UAT (user acceptance testing) to see if the personalization “feels” good, and to catch any corner cases not thought of.

By covering this range of tests, we ensure confidence in the system’s correctness and robustness. This rigorous approach will help catch issues early in development and make sure the Personalization Engine meets its requirements when deployed live.

Deployment & Operations

Deploying the AI-Driven Personalization Engine will leverage modern DevOps practices to ensure reliable releases and easy operations management. The following outlines the deployment architecture, tools, and operational procedures:

Infrastructure-as-Code (IaC): All infrastructure for this engine is defined using IaC (e.g., Terraform or CloudFormation). This includes definitions for Kubernetes clusters, databases (managed services configurations for Pinecone, Neo4j, etc.), networking (VPCs, load balancers), and CI/CD runners. Using IaC ensures that environments (staging, production) are consistent and can be recreated or updated in a controlled way. For example, Terraform scripts will define the EC2 instances or Kubernetes node pools to run our microservices, and resources like Kafka topics or IAM roles with least privileges for each service.

Containerization: Each microservice (the FastAPI app, the Rust stream processor, model server, etc.) is containerized via Docker. We’ll have multi-stage Dockerfiles to produce lightweight images (especially for Python, use slim base images, and for Rust compile to a static binary if possible). These images are stored in a registry and version-tagged with releases.

Orchestration & Deployment: We will likely use Kubernetes to deploy these containers, given the need to scale and manage multiple services. Each component runs as a deployment in K8s, possibly in its own namespace or with proper labels. Services like vector DB might be external cloud services (so not containerized by us) but we’ll integrate via network endpoints. We’ll use Helm charts or Kustomize to manage the K8s config for our app. For continuous deployment, tools like ArgoCD or Flux can watch our git repo with env configs and sync to the cluster.

CI/CD Pipeline: GitHub Actions or GitLab CI will run the CI steps (tests as described in Testing Strategy, linting, container build). On successful pipeline and merge, a CD pipeline can trigger:

Deploy to a staging environment (maybe automatically for each merge to main).

Run integration tests there.

For production, we might use manual approval or an automated canary analysis.

Deployment Strategies: We will minimize downtime through:

Blue-Green Deployment: Maintain two sets of pods (blue and green). Deploy new version to green while blue is serving. Run smoke tests on green (or simply route internal test traffic). Then flip the traffic over to green. If something goes wrong, switch back to blue quickly. This ensures zero downtime and easy rollback.

Canary Releases: For high-risk changes, we can do canary: deploy new version to a small subset of pods (say 10%) and let a fraction of live traffic hit it. Monitor error rates and metrics. If all good, gradually scale it up to 100%. If issues, automatically roll back. This approach works well if we have robust monitoring to detect anomalies.

Kubernetes and service mesh (like Istio) can facilitate traffic splitting for canary. Or simpler, we can have logic in the load balancer or in our app to serve new logic for a random subset of users (feature flag canary).

Scaling & Auto-Scaling:

For stateless services (APIs, model servers), enable Horizontal Pod Autoscaler in Kubernetes: based on CPU or custom metrics (like request latency), spawn more pods. For example, if CPU > 70% for 5 minutes, add another replica. We’ll set a max limit according to our capacity planning.

The Kafka consumers (if on K8s) can also autoscale based on lag in the queue (e.g., if backlog of events grows, increase parallelism).

The vector DB and knowledge graph if managed services usually have their own scaling (Pinecone can scale pods or we pay for higher tier; Neo4j we might need to choose cluster size appropriately and monitor usage to scale up cluster if needed).

We also design for scale-out: if we need to run in multiple regions (for global users to reduce latency), we can deploy a duplicate stack in another region and use a global load balancer or CDN routing. However, that introduces complexity with data sync (the vector DB and graph would need multi-region replication). Initially, likely a single region with CDN edge caching for static content is fine, as the dynamic recos are fast enough.

Health Checks: Each service will expose a health endpoint (e.g., /healthz) that Kubernetes liveness/readiness probes use. The health check will verify basics, like ability to connect to its essential dependency (DB connection alive, etc.). If a service is unhealthy, K8s will restart it or pull it out of rotation. We also incorporate startup readiness – e.g., model server might take time to load a model into memory, so it reports not ready until done to avoid receiving traffic too early.

Logging & Observability: We will aggregate logs from all pods to a centralized system (e.g., use EFK stack: ElasticSearch + Fluentd + Kibana, or a cloud logging service). This allows us to search logs across the cluster easily (which we’ll need when debugging an issue, as described in Logging section). We have Grafana dashboards visualizing key metrics (latencies, throughput, error rates, etc.) over time for each environment. We set up alerts (via something like Prometheus Alertmanager or CloudWatch Alarms) to page on-call engineers for critical issues (like system down or major SLA breach).

Maintenance and Updates:

Rolling Updates: For routine updates, use K8s rolling update (which replaces pods gradually ensuring some stay up).

Database migrations: If we change profile schema or knowledge graph structure, we handle migrations carefully. Possibly version the profile schema and have code handle both until migration done. If using managed DB changes (like adding an index), do that in advance or during a maintenance window if needed.

Backup & Recovery: Regular backups of the knowledge graph DB and any stateful stores will be configured. E.g., nightly backup of Neo4j data and secure copy to storage. Pinecone likely has redundancy, but if it doesn’t, we might snapshot vectors to s3 periodically. In case of data corruption or bug that ruins profiles, we should have the ability to restore from a backup or recompute from event log (the event log is effectively a source of truth too – we could replay events to rebuild profiles if needed).

Failover/Disaster Recovery: With 99.95% uptime target, we plan for multi-AZ deployment at least (so an AZ outage doesn’t take us down). Multi-region active-passive could be a future plan: keep a warm standby in another region, and if region outage, switch DNS. But that might be beyond initial scope.

Secrets Management: API keys (for external services like OpenAI, or DB credentials) will be stored securely (Kubernetes secrets or a vault service) and not in code. Rotation of keys will be practiced periodically.

Operational Procedures:

We will have runbooks for common scenarios: e.g., “If recommendations slow down, check X, Y, Z (possibly vector DB status, etc.)”.

On-call rotation established for the team to handle after-hours issues.

Regularly review logs and user feedback to catch things like irrelevant recommendations or any potential bias issues (operational monitoring of recommendation quality, not just system metrics).

By leveraging these deployment practices, the team can confidently push out updates to the personalization engine and maintain it with minimal downtime. Continuous deployment with safeguards (blue-green, canaries) means we can iterate fast on ML models and features without major service interruptions. Good observability ensures we catch issues and scale as usage grows.

Future Considerations

Looking beyond the initial implementation, there are several advanced features and improvements we may consider for future versions of the Personalization Engine. These are not in scope for the MVP but are noted here as possible roadmap items and to ensure the architecture can accommodate them:

Federated Learning: In the future, we might move toward a federated learning approach for model training to enhance privacy. Instead of centralizing all user interaction data to train a global model, we could send a generic model to user devices (or edge servers), train on local data, and only send back model updates (gradients) that are aggregated into the global model. This would allow personalization models to learn from user data without that raw data leaving the user’s device. For example, a next-best-content model could gradually improve by training on usage patterns on each client. Federated learning would require careful orchestration (scheduling rounds, handling clients with varying availability, ensuring model convergence) and addressing challenges like higher client resource usage and potential model divergence. Our architecture’s modular model component could be extended to support a federated training coordinator if this path is pursued. We’d also incorporate differential privacy in the model updates to prevent leakage of personal data through gradients.

Reinforcement Learning & Bandit Algorithms: The engine currently will use mostly supervised or content-based models. We could introduce reinforcement learning (RL) to continuously optimize recommendations based on feedback. A multi-armed bandit system could try slightly different recommendations (exploration) and learn which strategies yield the best engagement (exploitation) for each user. For instance, the engine could maintain a few different recommendation approaches (one more exploration-heavy, one conservative) and dynamically allocate which approach to use for a user session, learning over time what works best. We already handle some aspects (like re-ranking and contextual bandits as mentioned (Real-Time Personalization Using Microservices )), but a full RL loop might involve a policy network or Q-learning that considers long-term user satisfaction (not just immediate click). Implementing this would require a reward metric (e.g., user returns next day = good, or explicit thumbs-up on recos) and possibly simulation. It’s complex, so we note it for future research.

Personalized AI Agents (Persona Simulation): As AI agent technology matures, we could deploy a personal AI agent for each user that simulates their behavior or acts on their behalf to discover content. For example, an agent that knows the user’s interests could proactively scour the knowledge base (via queries to the vector DB and knowledge graph) and prepare a briefing for the user each morning. Another angle is to simulate user personas in testing: we could have a set of virtual users with defined interests run through the system to see how it responds, which helps tuning. Additionally, “persona simulation” could refer to giving the system an understanding of archetypal personas so it can classify users into segments for cold-start (e.g., new user appears similar to the “Data Scientist persona” so initially show what works for that persona until enough personal data is gathered). Incorporating an agent per user might leverage LLMs heavily and would need careful resource management, but our architecture with edge WebLLM and strong backends could allow each user essentially a mini-agent that continuously learns and interacts with the central engine.

Cross-Platform Personalization: If Synapse Web expands to multiple interfaces (web, mobile, AR/VR), the engine will need to personalize across them. In the future, consider context like device type more strongly (e.g., what to recommend on a wearable device might differ). The architecture might include an API for device-specific recommendations.

Enhanced Explainability & User Control: We might provide users with an interface to see “Why was this recommended?” and to allow them to give feedback like “This isn’t relevant” or “Show more of this”. Future iterations could incorporate that feedback loop robustly – possibly fine-tuning the user’s profile or even updating model parameters on the fly for that user. This would increase transparency and user trust. We could also explore using LLMs to generate natural language explanations for recommendations, pulling from the knowledge graph (e.g., “Because you took course X, and many who did also enjoyed article Y, we thought you’d like Y”).

Contextual Multi-user Personalization: In collaborative scenarios, recommendations might be tailored to a group of users (e.g., suggesting content that would benefit an entire team). Future extensions could have a notion of a “team profile” computed as an aggregate of individual profiles and then personalize for the team context. For example, recommending a document to all members of a project team because it’s relevant to the project and none have seen it yet.

Scaling to Global Knowledge: If the knowledge base gets extremely large (millions of items), we might need to incorporate more sophisticated search/indexing techniques, like hierarchical vector indexes, or topic-based sharding of the knowledge graph. We might also explore hybrid search (combining keyword + vector) for better precision. Ensuring the engine continues to perform with an ever-growing content repository will be an ongoing challenge.

Real-time Collaborative Filtering Updates: Currently, collaborative filtering is likely offline, but in future, if we can update similarity matrices in real-time (with incremental matrix factorization or using embeddings and approximate similarity, which we already partly do), then recommendations can instantly reflect trending content among similar users. This would blur the line between pure content-based and collaborative methods.

Increased Use of Knowledge Graph for Reasoning: We can deepen the use of the knowledge graph by applying graph algorithms (like PageRank, community detection) to find influential content or emerging interest clusters and feed that into personalization. Also, integrating external knowledge bases or ontologies could enhance understanding (for example, if two different terms are synonyms, the knowledge graph could connect them so the engine knows interest in X implies interest in Y).

Each of these future features will be assessed for feasibility and impact. The current architecture has been designed with extensibility in mind, so that adding new modules or replacing components (e.g., swapping out the recommendation model with an RL agent) can be done with minimal changes to the surrounding system. We will revisit this specification regularly as the project evolves to incorporate the most promising improvements in pursuit of ever more effective and intelligent personalization.

Constraints & Assumptions

In designing this specification, we operate under several constraints and assumptions which define the boundaries of the solution:

Technology Stack Commitment: We have decided on certain technologies for consistency with the Synapse Web project:

Backend: Python will be used for the API and orchestration layer (leveraging its rich ML/AI libraries and ease of writing web services with FastAPI). Rust will be used for components where performance is critical (e.g., the streaming event processor, or any module that does heavy computation at scale) because of its memory safety and speed. The combination allows us to write high-level logic in Python and offload tight loops or concurrency-sensitive tasks to Rust.

Frontend: Next.js (React) is assumed for building the Synapse Web user interface. This matters to personalization because Next.js can do server-side rendering or client-side data fetching – we assume the personalization API will be called from the client-side for now (unless we do some server-side personalization rendering).

WebAssembly: We assume we can compile certain logic (especially Rust code) to WebAssembly for use either in the browser or in serverless edge environments. This means any algorithm we implement in Rust could potentially run in a WebAssembly VM if needed (for example, the WebLLM stuff or some lightweight profile update logic on client).

Vector Database: We will use a managed vector DB service (most likely Pinecone to start, given its maturity, or Weaviate if self-hosting). We assume this service can handle our scale and offers the needed features (similarity search, metadata filtering, horizontal scaling).

Knowledge Graph: We assume availability of a graph database like Neo4j (if we self-host on a server or cluster) or Amazon Neptune (if we go cloud-managed on AWS). We will design the schema to fit on these. Also assume graph queries can be made fast enough (<100ms for relevant queries) with proper indexing.

LLM and Embeddings: We might use external APIs (OpenAI) for some tasks (embedding generation initially, maybe even some recommendations). We assume we have the budget and allowance to call these services, or that we will have internal models running. If external, latency and cost must be considered (embedding each document via API might be slow/expensive, so perhaps done offline). This spec assumes some form of transformer model availability.

User Identity & Auth: We assume that user authentication is handled by an upstream service (users log in via SSO or similar) and that the personalization engine will receive a user ID (or token mapping to it) with each request or event. We don’t manage passwords or basic auth here. We trust the user_id provided in tokens and use it as key for profile. We also assume the system can use these IDs to integrate with other services (e.g., content service, where item IDs are consistent across systems).

Cold Start Assumption: New users with no history will get a generic experience or one based on minimal info (like declared interests). We assume this is acceptable and not fully solved by the engine for now (though we have plans like persona defaults). Similarly, new content with no interactions might rely on content-based similarity only.

Content Scope: The personalization is scoped to the knowledge platform’s content. We assume a relatively structured content set (documents, tutorials, Q&As) and some user-generated content. We are not initially dealing with, say, product recommendations or ads – so the domain is focused on knowledge and collaboration content. This means evaluation of “precision” can be done via relevance to user’s learning goals, which we measure via engagement.

Real-time Collaboration Constraints: If multiple users are editing a document simultaneously (just an example), personalization could potentially intervene (like suggest collaborators). But our engine is read-only in terms of content – it doesn’t modify content, only suggests. We assume we won’t interfere with real-time editing sessions beyond maybe suggestions in a side panel.

Resource Constraints: We assume we have cloud infrastructure to scale horizontally. However, we should be mindful of cost: running a large LLM for every user request might be too expensive. That’s partly why we consider edge LLMs and smaller models. We design the system such that expensive operations (like re-embedding a user’s history) are done sparingly (maybe only when there’s significant change, not every click).

Zero Trust Environment: Because the broader project is Zero Trust, we must assume no implicit trust even inside our network. Every service call must be authenticated/authorized (we might need to issue service identities and tokens for internal calls). We assume we have infrastructure for service mesh or similar to manage this. This adds overhead but is mandated. Also, we assume all data must be encrypted at rest by policy – our chosen DBs support this (e.g., Neo4j enterprise with encryption, Pinecone likely encrypts data at rest).

Compliance: We assume GDPR, etc., apply (since PIPEDA was mentioned for Canada, and likely global use as well). So things like data residency might come up (if users in EU, data might need to stay in EU). We might later deploy regional instances for compliance – but initially assume we can host data in one region with user consent.

Scaling Limits: The 10M user and 100k concurrent are our design targets. If the platform wildly succeeds beyond that, further architecture work (like splitting by region or multi-master setups) will be needed. We assume for now we don’t exceed that by an order of magnitude.

Feature Flags & Configuration: We assume the capability to use a feature flag system (could be LaunchDarkly or a simple config service) is available to toggle features. If not, we will implement a basic one (e.g., config file that can be updated and engine re-reads periodically).

Time Horizon: This spec is designed for the technology landscape of 2025. We assume current trends (LLMs, vector DBs, WebGPU in browsers, etc.) remain stable or improve. If any tech becomes obsolete or has a big breakthrough (like a new way to do personalization), we’ll adapt then. But for now, we base it on what’s cutting-edge but proven as of 2025.

Developer Skills: The implementation team is assumed to be proficient in the chosen stack (Python, Rust, ML pipelines). Training or ramp-up for specialized areas (like knowledge graph modeling) might be needed, but the design tries to use familiar concepts.

Any changes in these assumptions might require revisiting the design. We have architected the solution to be as flexible as possible given these constraints. For instance, if we needed to swap out Pinecone for another DB, or if auth changes, we can handle that by adjusting the corresponding interface without a complete redesign.

Clarification & Open Questions

While we have detailed the specification, some aspects remain ambiguous or require clarification from stakeholders. Before finalizing implementation, we should clarify the following points:

Scope of UI/Workflow Personalization: We need more detail on what UI elements are in scope for personalization. For example, can the engine rearrange entire dashboard layouts, or is it limited to choosing content within predefined widgets? Clarifying this will impact how the front-end and engine coordinate (e.g., a list of possible UI variants might be needed if layout changes are allowed). Open question: Which parts of the Synapse Web UI should the personalization engine control or influence directly?

Admin Personalization Rules Syntax: The specification introduces the idea of admin-defined rules (e.g., always recommend X for new users in workspace Y). We need to clarify how complex these rules can be. Are they simple if-then statements configured via an admin UI, or something more programmatic (scripting)? And how will admins provide these rules? Open question: Define the format and capabilities of customization rules that admins can set (so we can design the rules engine appropriately).

Explainability Requirements: We mention providing “explanations” for recommendations for transparency. We need to confirm to what extent this is required in the UI. Should every recommendation have a user-visible reason? Or is this more for internal audit and occasional user requests? Explaining recommendations from complex models can be challenging. Open question: What level of explainability is expected by end users (versus just internal use)? Depending on the answer, we might need to adjust data we store (to generate reasons) or allocate time for an explainability module.

Edge Inference Use Cases: The inclusion of WebLLM and client-side inference is forward-looking. We should clarify which specific scenarios we plan to support in phase 1 vs later. For instance, will we actually deploy a mini model to the browser to do any part of personalization at launch, or is this just an option down the road? Open question: Confirm if edge personalization (WebLLM in browser) is in the initial scope or purely a future consideration. If initial, we need to decide what model and for what purpose (e.g., local re-ranking? local data privacy mode?).

Precision Metric Definition: We have a goal “precision >85% on adaptive recommendations.” How exactly will this be measured and on what dataset? For instance, is this precision@5 on a test set of user queries, or click-through-rate in production? Clarifying this will guide how we evaluate the system during development. Open question: Define the evaluation methodology for the 85% precision target (so the team can verify they meet the objective).

Knowledge Graph Scope & Maintenance: We assume building a knowledge graph with user/content/ontology. We need clarity on who curates the ontology (domain knowledge model) and how dynamic it is. If the domain concepts change, do we update the graph schema frequently? Also, are there existing data sources we can ingest to build this graph or is it entirely built from scratch within Synapse Web content? Open question: Get details on the source and upkeep of the knowledge graph taxonomy/ontology (to ensure our design aligns with data availability).

Privacy and Consent UI: While we handle opt-out internally, the implementation team needs to know how users express consent or opt-out in the UI so we can integrate that. Open question: How does a user opt out or adjust personalization settings in Synapse Web, and what is the expected behavior from the engine when they do? This affects whether we, for example, completely stop tracking events for that user or just stop using them for recommendations, etc.

Feature Flag Mechanism: We assume feature flags for maintainability. It’s not specified if Synapse Web already has an infrastructure for that. If not, we might need to implement a simple one. Open question: Does the broader platform provide a feature flag service, or should the personalization team include one? This will clarify how we toggle experimental features safely.

Each of these points should be discussed with product owners and relevant stakeholders. We have flagged them here to ensure they are resolved before or during development to avoid rework. By addressing these ambiguities early, we can finalize the architecture and proceed with a clear understanding, thereby reducing the risk of misunderstandings in implementation. clarified now
</file>

<file path="vector_store_resilience.md">
# Vector Store Resilience Strategy

## Overview

This document details the comprehensive backup and monitoring strategy implemented for the Redis vector storage system, addressing one of the key technical risks identified in the original specification.

## 1. Backup Architecture

- **Hybrid Storage Approach**
  - Redis RDB snapshots for metadata persistence
  - FAISS index serialization for vector data
  - S3 with AES-256 encryption for durable storage
  - Local caching for fast recovery

## 2. Operational Safeguards

- **Consistency Guarantees**
  - Atomic backup operations
  - Metadata validation
  - Index size verification
  - Redis key count validation

## 3. Monitoring & Observability

- **Prometheus Metrics**
  ```
  vector_store_backup_duration_seconds
  vector_store_backup_success_total
  vector_store_backup_failure_total
  vector_store_backup_size_bytes
  vector_store_vector_count
  vector_store_operation_duration_seconds
  vector_store_operation_errors_total
  vector_store_last_backup_timestamp
  vector_store_backup_age_seconds
  ```

## 4. Recovery Objectives

- **RTO (Recovery Time Objective)**
  - Local restore: < 5 minutes
  - S3 restore: < 15 minutes
- **RPO (Recovery Point Objective)**
  - Default: 24 hours (daily backups)
  - Configurable based on business requirements

## 5. Retention Policy

- **Tiered Retention**
  - Daily backups: 7 days
  - Weekly backups: 4 weeks
  - Monthly backups: 6 months
- **Automated Cleanup**
  - Policy-based retention enforcement
  - Secure deletion from both local and S3 storage

## 6. Implementation Details

- **Technology Stack**
  - Redis for metadata storage
  - FAISS for vector indices
  - S3 for durable backup storage
  - Prometheus for monitoring
  - Kubernetes CronJob for scheduling

## 7. Operational Benefits

- **Risk Mitigation**
  - Addresses data durability concerns
  - Provides disaster recovery capabilities
  - Enables point-in-time recovery
- **Monitoring & Alerting**
  - Real-time backup status tracking
  - Performance metrics collection
  - Automated failure detection
- **Compliance Support**
  - Audit trail of backup operations
  - Encrypted storage for sensitive data
  - Configurable retention policies

This enhancement significantly strengthens the platform's resilience and directly addresses one of the key technical risks identified in the original specification. The implementation provides a production-grade solution that balances performance, reliability, and operational simplicity while meeting industry best practices for data protection and monitoring.
</file>

<file path="microservices/python/knowledge-graph/app/nats_client.py">
"""NATS client wrapper for knowledge graph service."""
from typing import Optional

from nats_lib import EnhancedNatsClient, NatsConfig


class KnowledgeGraphNatsClient(EnhancedNatsClient):
    """Knowledge graph specific NATS client."""
    
    def __init__(
        self,
        nats_url: str = "nats://localhost:4222",
        user: Optional[str] = None,
        password: Optional[str] = None,
    ):
        """Initialize the NATS client.
        
        Args:
            nats_url: URL of the NATS server
            user: Optional username for authentication
            password: Optional password for authentication
        """
        config = NatsConfig(
            urls=nats_url,
            user=user,
            password=password,
            stream_domain="chimera",
        )
        
        super().__init__(
            config=config,
            service_name="knowledge-graph",
        )
</file>

<file path="microservices/python/knowledge-graph/requirements.txt">
fastapi==0.108.0
uvicorn==0.25.0
pydantic==2.5.3
neo4j==5.16.0
nats-py==2.6.0
python-dotenv==1.0.0
loguru==0.7.2
asyncio==3.4.3
numpy==1.26.3
tenacity==8.2.3
prometheus-client==0.17.1
opentelemetry-api==1.21.0
opentelemetry-sdk==1.21.0
opentelemetry-exporter-otlp==1.21.0
pybreaker==1.0.1
-e ../common/nats_lib
</file>

<file path="microservices/python/ml-orchestrator/app/nats_client.py">
"""NATS client wrapper for ML orchestrator service."""
from typing import List, Optional, Union

from nats_lib import EnhancedNatsClient, NatsConfig


class MLOrchestratorNatsClient(EnhancedNatsClient):
    """ML orchestrator specific NATS client."""
    
    def __init__(
        self,
        nats_urls: Union[str, List[str]],
        use_tls: bool = True,
        user: Optional[str] = None,
        password: Optional[str] = None,
        token: Optional[str] = None,
        stream_domain: str = "chimera",
        num_shards: int = 10,
    ):
        """Initialize the NATS client.
        
        Args:
            nats_urls: URL(s) of the NATS server
            use_tls: Whether to use TLS for connection
            user: Optional username for authentication
            password: Optional password for authentication
            token: Optional token for authentication
            stream_domain: JetStream domain
            num_shards: Number of shards for high-throughput streams
        """
        config = NatsConfig(
            urls=nats_urls,
            use_tls=use_tls,
            user=user,
            password=password,
            token=token,
            stream_domain=stream_domain,
            num_shards=num_shards,
        )
        
        super().__init__(
            config=config,
            service_name="ml-orchestrator",
        )
</file>

<file path="microservices/python/ml-orchestrator/requirements.txt">
fastapi==0.108.0
uvicorn==0.25.0
pydantic==2.5.3
nats-py==2.6.0
tritonclient==2.40.0
numpy==1.26.3
python-dotenv==1.0.0
loguru==0.7.2
asyncio==3.4.3
tenacity==8.2.3
typing-extensions==4.9.0
aiokafka==0.10.0
prometheus-client==0.17.1
opentelemetry-api==1.21.0
opentelemetry-sdk==1.21.0
opentelemetry-exporter-otlp==1.21.0
pybreaker==1.0.1
-e ../common/nats_lib
</file>

<file path="microservices/python/personalization-engine/app/nats_client.py">
"""NATS client wrapper for personalization engine."""
from typing import Optional

from nats_lib import EnhancedNatsClient, NatsConfig


class PersonalizationNatsClient(EnhancedNatsClient):
    """Personalization engine specific NATS client."""
    
    def __init__(
        self,
        nats_url: str = "nats://localhost:4222",
        user: Optional[str] = None,
        password: Optional[str] = None,
        stream_name: str = "chimera",
        consumer_name: str = "personalization-engine",
    ):
        """Initialize the NATS client.
        
        Args:
            nats_url: URL of the NATS server
            user: Optional username for authentication
            password: Optional password for authentication
            stream_name: Name of the JetStream stream
            consumer_name: Name of the JetStream consumer
        """
        config = NatsConfig(
            urls=nats_url,
            user=user,
            password=password,
            stream_domain=stream_name,
        )
        
        super().__init__(
            config=config,
            service_name=consumer_name,
        )
</file>

<file path="microservices/python/personalization-engine/app/vector_store.py">
import asyncio
import json
import os
import time
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional, Tuple, Union, cast

import aiofiles
import boto3
import faiss
import numpy as np
import redis.asyncio as redis
from botocore.exceptions import ClientError
from loguru import logger
from prometheus_client import Counter, Gauge, Histogram
from sentence_transformers import SentenceTransformer

# Prometheus metrics
BACKUP_DURATION: Histogram = Histogram(
    'vector_store_backup_duration_seconds',
    'Time spent performing backup operations',
    ['operation']
)
BACKUP_SUCCESS: Counter = Counter(
    'vector_store_backup_success_total',
    'Number of successful backup operations',
    ['type']
)
BACKUP_FAILURE: Counter = Counter(
    'vector_store_backup_failure_total',
    'Number of failed backup operations',
    ['type']
)
BACKUP_SIZE: Gauge = Gauge(
    'vector_store_backup_size_bytes',
    'Size of the latest backup',
    ['component']
)
VECTOR_COUNT: Gauge = Gauge(
    'vector_store_vector_count',
    'Number of vectors in store',
    ['index_type']
)
VECTOR_OPERATION_DURATION: Histogram = Histogram(
    'vector_store_operation_duration_seconds',
    'Time spent on vector operations',
    ['operation']
)
VECTOR_OPERATION_ERRORS: Counter = Counter(
    'vector_store_operation_errors_total',
    'Number of vector operation errors',
    ['operation']
)
LAST_BACKUP_TIMESTAMP: Gauge = Gauge(
    'vector_store_last_backup_timestamp',
    'Timestamp of the last successful backup'
)
BACKUP_AGE: Gauge = Gauge(
    'vector_store_backup_age_seconds',
    'Age of the latest backup in seconds'
)

class VectorStore:
    """Vector store for managing embeddings and similarity search."""

    def __init__(
        self, 
        model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
        dimension: int = 384,
        redis_url: str = "redis://localhost:6379",
        index_name: str = "chimera-vectors",
        user_index_name: str = "chimera-users",
        backup_dir: str = "/tmp/vector_store_backup",
        s3_bucket: Optional[str] = None,
        s3_prefix: str = "vector_store_backups",
    ) -> None:
        """Initialize the vector store.
        
        Args:
            model_name: Name of the sentence transformer model to use
            dimension: Dimension of the embeddings
            redis_url: URL of the Redis server
            index_name: Name of the index for content embeddings
            user_index_name: Name of the index for user embeddings
            backup_dir: Local directory for storing backups
            s3_bucket: Optional S3 bucket for backup storage
            s3_prefix: Prefix for S3 backup storage path
        """
        self.model_name = model_name
        self.dimension = dimension
        self.redis_url = redis_url
        self.index_name = index_name
        self.user_index_name = user_index_name
        self.backup_dir = backup_dir
        self.s3_bucket = s3_bucket
        self.s3_prefix = s3_prefix
        
        self.model: Optional[SentenceTransformer] = None
        self.redis: Optional[redis.Redis] = None
        self.content_index: Optional[faiss.IndexFlatIP] = None
        self.user_index: Optional[faiss.IndexFlatIP] = None
        self.initialized: bool = False

        # Ensure backup directory exists
        os.makedirs(self.backup_dir, exist_ok=True)

        # Update initial vector counts
        VECTOR_COUNT.labels(index_type='content').set(0)
        VECTOR_COUNT.labels(index_type='user').set(0)

    async def initialize(self) -> None:
        """Initialize the vector store components."""
        try:
            # Initialize the embedding model
            self.model = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: SentenceTransformer(self.model_name)
            )
            logger.info(f"Initialized SentenceTransformer model: {self.model_name}")
            
            # Initialize Redis connection
            self.redis = redis.from_url(self.redis_url)
            logger.info(f"Connected to Redis at {self.redis_url}")
            
            # Initialize FAISS indices
            self.content_index = faiss.IndexFlatIP(self.dimension)  # Inner product for cosine similarity
            self.user_index = faiss.IndexFlatIP(self.dimension)
            
            # Load existing vectors from Redis if available
            await self._load_indices()
            
            self.initialized = True
            logger.info("Vector store initialization complete")
            
        except Exception as e:
            logger.error(f"Failed to initialize vector store: {e}")
            raise RuntimeError(f"Vector store initialization failed: {str(e)}")

    async def close(self) -> None:
        """Close the vector store connection."""
        if self.redis:
            await self.redis.close()
            logger.info("Redis connection closed")

    async def is_healthy(self) -> bool:
        """Check if the vector store is healthy."""
        if not self.initialized or not self.redis:
            return False
            
        try:
            await self.redis.ping()
            return True
        except Exception:
            return False

    async def _load_indices(self) -> None:
        """Load existing embeddings from Redis into FAISS indices."""
        try:
            # Load content embeddings
            content_keys = await self.redis.keys(f"{self.index_name}:*")
            if content_keys:
                logger.info(f"Loading {len(content_keys)} content embeddings from Redis")
                
                # Reset the index
                self.content_index = faiss.IndexFlatIP(self.dimension)
                
                # Load embeddings in batches to avoid memory issues
                batch_size = 1000
                for i in range(0, len(content_keys), batch_size):
                    batch = content_keys[i:i+batch_size]
                    
                    # Get embeddings and metadata
                    pipe = self.redis.pipeline()
                    for key in batch:
                        pipe.get(key)
                    
                    results = await pipe.execute()
                    
                    # Parse and add to index
                    embeddings = []
                    for data in results:
                        if data:
                            item = json.loads(data)
                            if "embedding" in item:
                                embeddings.append(np.array(item["embedding"], dtype=np.float32))
                    
                    if embeddings:
                        embeddings_array = np.vstack(embeddings).astype(np.float32)
                        self.content_index.add(embeddings_array)
                
                logger.info(f"Loaded {self.content_index.ntotal} content embeddings into FAISS index")
            
            # Load user embeddings
            user_keys = await self.redis.keys(f"{self.user_index_name}:*")
            if user_keys:
                logger.info(f"Loading {len(user_keys)} user embeddings from Redis")
                
                # Reset the index
                self.user_index = faiss.IndexFlatIP(self.dimension)
                
                # Load embeddings
                pipe = self.redis.pipeline()
                for key in user_keys:
                    pipe.get(key)
                
                results = await pipe.execute()
                
                # Parse and add to index
                embeddings = []
                for data in results:
                    if data:
                        user = json.loads(data)
                        if "embedding" in user:
                            embeddings.append(np.array(user["embedding"], dtype=np.float32))
                
                if embeddings:
                    embeddings_array = np.vstack(embeddings).astype(np.float32)
                    self.user_index.add(embeddings_array)
                
                logger.info(f"Loaded {self.user_index.ntotal} user embeddings into FAISS index")
            
        except Exception as e:
            logger.error(f"Error loading indices from Redis: {e}")
            # Initialize empty indices if loading fails
            self.content_index = faiss.IndexFlatIP(self.dimension)
            self.user_index = faiss.IndexFlatIP(self.dimension)

    async def _generate_embedding(self, text: str) -> np.ndarray:
        """Generate an embedding vector for the given text.
        
        Args:
            text: The text to generate an embedding for
            
        Returns:
            np.ndarray: The embedding vector
            
        Raises:
            RuntimeError: If the model is not initialized
        """
        if not self.model:
            raise RuntimeError("Model not initialized")
            
        embedding = await asyncio.get_event_loop().run_in_executor(
            None,
            lambda: self.model.encode(text, convert_to_numpy=True)
        )
        return embedding.astype(np.float32)

    async def add_item(
        self, 
        item_id: str, 
        text: str, 
        metadata: Dict[str, Any]
    ) -> None:
        """Add an item to the vector store.
        
        Args:
            item_id: Unique identifier for the item
            text: Text content to generate embedding from
            metadata: Additional metadata to store with the item
            
        Raises:
            RuntimeError: If the store is not initialized
        """
        if not self.initialized or not self.redis or not self.content_index:
            raise RuntimeError("Vector store not initialized")
            
        try:
            with VECTOR_OPERATION_DURATION.labels(operation='add_item').time():
            # Generate embedding
            embedding = await self._generate_embedding(text)
            
            # Store in Redis
            data = {
                "id": item_id,
                "embedding": embedding.tolist(),
                "metadata": metadata,
                    "timestamp": datetime.now(timezone.utc).isoformat()
                }
                await self.redis.set(
                    f"{self.index_name}:{item_id}",
                    json.dumps(data)
                )
            
            # Add to FAISS index
                self.content_index.add(embedding.reshape(1, -1))
            
                # Update metrics
                await self._update_metrics()
            
        except Exception as e:
            VECTOR_OPERATION_ERRORS.labels(operation='add_item').inc()
            logger.error(f"Failed to add item {item_id}: {e}")
            raise

    async def add_user(
        self, 
        user_id: str, 
        interests: str,
        preferences: Dict[str, Any],
        role: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> None:
        """Add a user profile to the vector store.
        
        Args:
            user_id: Unique identifier for the user
            interests: Text describing user interests
            preferences: User preferences dictionary
            role: Optional user role
            metadata: Additional metadata to store
            
        Raises:
            RuntimeError: If the store is not initialized
        """
        if not self.initialized or not self.redis or not self.user_index:
            raise RuntimeError("Vector store not initialized")
            
        try:
            with VECTOR_OPERATION_DURATION.labels(operation='add_user').time():
                # Generate embedding from interests
            embedding = await self._generate_embedding(interests)
            
            # Store in Redis
            data = {
                "id": user_id,
                "embedding": embedding.tolist(),
                    "interests": interests,
                "preferences": preferences,
                "role": role,
                "metadata": metadata or {},
                    "timestamp": datetime.now(timezone.utc).isoformat()
                }
                await self.redis.set(
                    f"{self.user_index_name}:{user_id}",
                    json.dumps(data)
                )
            
            # Add to FAISS index
                self.user_index.add(embedding.reshape(1, -1))
            
                # Update metrics
                await self._update_metrics()
            
        except Exception as e:
            VECTOR_OPERATION_ERRORS.labels(operation='add_user').inc()
            logger.error(f"Failed to add user {user_id}: {e}")
            raise

    async def update_user(
        self, 
        user_id: str, 
        interests: str,
        preferences: Dict[str, Any],
        role: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> None:
        """Update a user profile in the vector store.
        
        Args:
            user_id: Unique identifier for the user
            interests: Updated text describing user interests
            preferences: Updated user preferences dictionary
            role: Optional updated user role
            metadata: Additional metadata to store
            
        Raises:
            RuntimeError: If the store is not initialized
            KeyError: If the user does not exist
        """
        if not self.initialized or not self.redis or not self.user_index:
            raise RuntimeError("Vector store not initialized")
            
        try:
            with VECTOR_OPERATION_DURATION.labels(operation='update_user').time():
            # Check if user exists
                existing_data = await self.redis.get(f"{self.user_index_name}:{user_id}")
                if not existing_data:
                    raise KeyError(f"User {user_id} not found")
            
            # Generate new embedding
            embedding = await self._generate_embedding(interests)
            
                # Store updated data in Redis
            data = {
                "id": user_id,
                "embedding": embedding.tolist(),
                    "interests": interests,
                "preferences": preferences,
                "role": role,
                "metadata": metadata or {},
                    "timestamp": datetime.now(timezone.utc).isoformat()
                }
                await self.redis.set(
                    f"{self.user_index_name}:{user_id}",
                    json.dumps(data)
                )
                
                # Update FAISS index (remove old and add new)
                # Note: This is a simplification. In production, we'd need a more
                # sophisticated way to update vectors in FAISS
                self.user_index = faiss.IndexFlatIP(self.dimension)
                await self._load_indices()
                
                # Update metrics
                await self._update_metrics()
                
        except KeyError:
            VECTOR_OPERATION_ERRORS.labels(operation='update_user').inc()
            logger.error(f"User {user_id} not found")
            raise
        except Exception as e:
            VECTOR_OPERATION_ERRORS.labels(operation='update_user').inc()
            logger.error(f"Failed to update user {user_id}: {e}")
            raise

    async def get_user(self, user_id: str) -> Optional[Dict[str, Any]]:
        """Retrieve a user profile from the vector store.
        
        Args:
            user_id: Unique identifier for the user
            
        Returns:
            Optional[Dict[str, Any]]: User profile data if found, None otherwise
            
        Raises:
            RuntimeError: If the store is not initialized
        """
        if not self.initialized or not self.redis:
            raise RuntimeError("Vector store not initialized")
            
        try:
            with VECTOR_OPERATION_DURATION.labels(operation='get_user').time():
                data = await self.redis.get(f"{self.user_index_name}:{user_id}")
            if data:
                    return cast(Dict[str, Any], json.loads(data))
            return None
            
        except Exception as e:
            VECTOR_OPERATION_ERRORS.labels(operation='get_user').inc()
            logger.error(f"Failed to get user {user_id}: {e}")
            raise

    async def find_similar_to_user(
        self, 
        user_id: str,
        content_types: Optional[List[str]] = None,
        limit: int = 10
    ) -> List[Dict[str, Any]]:
        """Find content similar to a user's interests.
        
        Args:
            user_id: Unique identifier for the user
            content_types: Optional list of content types to filter by
            limit: Maximum number of results to return
            
        Returns:
            List[Dict[str, Any]]: List of similar content items
            
        Raises:
            RuntimeError: If the store is not initialized
            KeyError: If the user does not exist
        """
        if not self.initialized or not self.redis or not self.content_index:
            raise RuntimeError("Vector store not initialized")
            
        try:
            with VECTOR_OPERATION_DURATION.labels(operation='find_similar').time():
            # Get user embedding
                user_data = await self.get_user(user_id)
                if not user_data or "embedding" not in user_data:
                    raise KeyError(f"User {user_id} not found or has no embedding")
                    
                user_embedding = np.array(user_data["embedding"], dtype=np.float32)
                
                # Search content index
                D, I = self.content_index.search(
                    user_embedding.reshape(1, -1),
                    limit
                )
                
                # Get content items
                results = []
                async with self.redis.pipeline() as pipe:
                    # Get all content keys
                    content_keys = await self.redis.keys(f"{self.index_name}:*")
                    if not content_keys:
                        return []
                        
                    # Get content data
                    for key in content_keys:
                        pipe.get(key)
                    content_data = await pipe.execute()
                    
                    # Filter and sort by similarity
                    for i, idx in enumerate(I[0]):
                        if idx < len(content_data) and content_data[idx]:
                            item = json.loads(content_data[idx])
                    
                # Apply content type filter if specified
                            if (content_types and 
                                "metadata" in item and 
                                "content_type" in item["metadata"] and
                                item["metadata"]["content_type"] not in content_types):
                        continue
                
                            item["similarity"] = float(D[0][i])
                            results.append(item)
                
                if len(results) >= limit:
                    break
            
            return results
            
        except KeyError:
            VECTOR_OPERATION_ERRORS.labels(operation='find_similar').inc()
            logger.error(f"User {user_id} not found")
            raise
        except Exception as e:
            VECTOR_OPERATION_ERRORS.labels(operation='find_similar').inc()
            logger.error(f"Failed to find similar content for user {user_id}: {e}")
            raise

    async def search(
        self, 
        query: str,
        content_types: Optional[List[str]] = None,
        limit: int = 10,
        user_embedding: Optional[List[float]] = None,
        user_weight: float = 0.3
    ) -> List[Dict[str, Any]]:
        """Search for content using a text query and optional user context.
        
        Args:
            query: Search query text
            content_types: Optional list of content types to filter by
            limit: Maximum number of results to return
            user_embedding: Optional user embedding for personalized results
            user_weight: Weight to apply to user similarity (0-1)
            
        Returns:
            List[Dict[str, Any]]: List of search results
            
        Raises:
            RuntimeError: If the store is not initialized
            ValueError: If user_weight is not between 0 and 1
        """
        if not self.initialized or not self.redis or not self.content_index:
            raise RuntimeError("Vector store not initialized")
            
        if not 0 <= user_weight <= 1:
            raise ValueError("user_weight must be between 0 and 1")
            
        try:
            with VECTOR_OPERATION_DURATION.labels(operation='search').time():
                # Generate query embedding
                query_embedding = await self._generate_embedding(query)
                
                # Combine with user embedding if provided
                if user_embedding is not None:
                    user_embedding_array = np.array(user_embedding, dtype=np.float32)
                    combined_embedding = (
                        (1 - user_weight) * query_embedding +
                        user_weight * user_embedding_array
                    )
                    # Normalize the combined embedding
                    combined_embedding /= np.linalg.norm(combined_embedding)
                else:
                    combined_embedding = query_embedding
                
                # Search content index
                D, I = self.content_index.search(
                    combined_embedding.reshape(1, -1),
                    limit
                )
                
                # Get content items
                results = []
                async with self.redis.pipeline() as pipe:
                    # Get all content keys
                    content_keys = await self.redis.keys(f"{self.index_name}:*")
                    if not content_keys:
                        return []
                        
                    # Get content data
                    for key in content_keys:
                        pipe.get(key)
                    content_data = await pipe.execute()
                    
                    # Filter and sort by similarity
                    for i, idx in enumerate(I[0]):
                        if idx < len(content_data) and content_data[idx]:
                            item = json.loads(content_data[idx])
                            
                            # Apply content type filter if specified
                            if (content_types and 
                                "metadata" in item and 
                                "content_type" in item["metadata"] and
                                item["metadata"]["content_type"] not in content_types):
                                continue
                                
                            item["similarity"] = float(D[0][i])
                            results.append(item)
                            
                            if len(results) >= limit:
                                break
                
                return results
                
        except Exception as e:
            VECTOR_OPERATION_ERRORS.labels(operation='search').inc()
            logger.error(f"Failed to search: {e}")
            raise

    async def _update_metrics(self) -> None:
        """Update Prometheus metrics for the vector store."""
        try:
            # Update vector counts
            if self.content_index:
                VECTOR_COUNT.labels(index_type='content').set(self.content_index.ntotal)
            if self.user_index:
                VECTOR_COUNT.labels(index_type='user').set(self.user_index.ntotal)
        except Exception as e:
            logger.error(f"Failed to update metrics: {e}")

    @VECTOR_OPERATION_DURATION.labels(operation='backup').time()
    async def backup(
        self,
        backup_id: Optional[str] = None,
        upload_to_s3: bool = True
    ) -> str:
        """Create a backup of the vector store.
        
        Args:
            backup_id: Optional identifier for the backup
            upload_to_s3: Whether to upload the backup to S3
            
        Returns:
            str: The backup ID
            
        Raises:
            RuntimeError: If the store is not initialized
            ValueError: If S3 upload is requested but not configured
        """
        if not self.initialized or not self.redis:
            raise RuntimeError("Vector store not initialized")
            
        if upload_to_s3 and not self.s3_bucket:
            raise ValueError("S3 bucket not configured")
            
        try:
            # Generate backup ID if not provided
            backup_id = backup_id or datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')
            backup_path = os.path.join(self.backup_dir, backup_id)
            os.makedirs(backup_path, exist_ok=True)
            
            # Save Redis data
            redis_path = os.path.join(backup_path, "redis_data.json")
            async with aiofiles.open(redis_path, 'w') as f:
                # Get all keys and data
                all_keys = []
                all_keys.extend(await self.redis.keys(f"{self.index_name}:*"))
                all_keys.extend(await self.redis.keys(f"{self.user_index_name}:*"))
                
                # Get all values
                pipe = self.redis.pipeline()
                for key in all_keys:
                    pipe.get(key)
                all_values = await pipe.execute()
                
                # Create backup data
                backup_data = {
                    key: value.decode() if isinstance(value, bytes) else value
                    for key, value in zip(all_keys, all_values)
                    if value is not None
                }
                
                await f.write(json.dumps(backup_data, indent=2))
            
            # Save FAISS indices
            if self.content_index:
                content_path = os.path.join(backup_path, "content_index.faiss")
                faiss.write_index(self.content_index, content_path)
                
            if self.user_index:
                user_path = os.path.join(backup_path, "user_index.faiss")
                faiss.write_index(self.user_index, user_path)
            
            # Create metadata
            metadata = {
                "id": backup_id,
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "content_vectors": self.content_index.ntotal if self.content_index else 0,
                "user_vectors": self.user_index.ntotal if self.user_index else 0,
                "redis_keys": len(all_keys),
                "model_name": self.model_name,
                "dimension": self.dimension
            }
            
            metadata_path = os.path.join(backup_path, "metadata.json")
            async with aiofiles.open(metadata_path, 'w') as f:
                await f.write(json.dumps(metadata, indent=2))
            
            # Upload to S3 if requested
            if upload_to_s3:
                s3 = boto3.client('s3')
                for root, _, files in os.walk(backup_path):
                    for file in files:
                        local_path = os.path.join(root, file)
                        s3_key = os.path.join(
                            self.s3_prefix,
                            backup_id,
                            file
                        )
                        s3.upload_file(
                            local_path,
                            self.s3_bucket,
                            s3_key,
                            ExtraArgs={'ServerSideEncryption': 'AES256'}
                        )
            
            # Update metrics
            BACKUP_SUCCESS.labels(type='full').inc()
            LAST_BACKUP_TIMESTAMP.set(time.time())
            
            # Calculate and set backup size
            total_size = 0
            for root, _, files in os.walk(backup_path):
                for file in files:
                    total_size += os.path.getsize(os.path.join(root, file))
            BACKUP_SIZE.labels(component='total').set(total_size)
            
            logger.info(f"Backup completed: {backup_id}")
            return backup_id
            
        except Exception as e:
            BACKUP_FAILURE.labels(type='full').inc()
            logger.error(f"Backup failed: {e}")
            raise

    @VECTOR_OPERATION_DURATION.labels(operation='restore').time()
    async def restore(
        self,
        backup_id: str,
        download_from_s3: bool = True
    ) -> None:
        """Restore the vector store from a backup.
        
        Args:
            backup_id: Identifier of the backup to restore
            download_from_s3: Whether to download the backup from S3
            
        Raises:
            RuntimeError: If the store is not initialized
            ValueError: If S3 download is requested but not configured
            FileNotFoundError: If the backup does not exist
        """
        if not self.initialized or not self.redis:
            raise RuntimeError("Vector store not initialized")
            
        if download_from_s3 and not self.s3_bucket:
            raise ValueError("S3 bucket not configured")
            
        try:
            backup_path = os.path.join(self.backup_dir, backup_id)
            
            # Download from S3 if requested
            if download_from_s3:
                os.makedirs(backup_path, exist_ok=True)
                s3 = boto3.client('s3')
                
                # List and download all backup files
                prefix = os.path.join(self.s3_prefix, backup_id)
                response = s3.list_objects_v2(
                    Bucket=self.s3_bucket,
                    Prefix=prefix
                )
                
                if 'Contents' not in response:
                    raise FileNotFoundError(f"Backup {backup_id} not found in S3")
                    
                for obj in response['Contents']:
                    key = obj['Key']
                    filename = os.path.basename(key)
                    local_path = os.path.join(backup_path, filename)
                    
                    s3.download_file(
                        self.s3_bucket,
                        key,
                        local_path
                    )
            
            # Verify backup exists
            if not os.path.exists(backup_path):
                raise FileNotFoundError(f"Backup {backup_id} not found")
            
            # Load metadata
            metadata_path = os.path.join(backup_path, "metadata.json")
            async with aiofiles.open(metadata_path, 'r') as f:
                metadata = json.loads(await f.read())
            
            # Verify compatibility
            if metadata['dimension'] != self.dimension:
                raise ValueError(
                    f"Backup dimension ({metadata['dimension']}) does not match "
                    f"current dimension ({self.dimension})"
                )
            
            # Restore Redis data
            redis_path = os.path.join(backup_path, "redis_data.json")
            async with aiofiles.open(redis_path, 'r') as f:
                redis_data = json.loads(await f.read())
            
            # Clear existing data
            await self.redis.delete(
                *(await self.redis.keys(f"{self.index_name}:*")),
                *(await self.redis.keys(f"{self.user_index_name}:*"))
            )
            
            # Restore data
            pipe = self.redis.pipeline()
            for key, value in redis_data.items():
                pipe.set(key, value)
            await pipe.execute()
            
            # Restore FAISS indices
            content_path = os.path.join(backup_path, "content_index.faiss")
            if os.path.exists(content_path):
                self.content_index = faiss.read_index(content_path)
            
            user_path = os.path.join(backup_path, "user_index.faiss")
            if os.path.exists(user_path):
                self.user_index = faiss.read_index(user_path)
            
            # Update metrics
            await self._update_metrics()
            
            logger.info(f"Restore completed from backup: {backup_id}")
            
        except Exception as e:
            VECTOR_OPERATION_ERRORS.labels(operation='restore').inc()
            logger.error(f"Restore failed: {e}")
            raise

    async def verify_backup(self, backup_id: str) -> Dict[str, Any]:
        """Verify the integrity of a backup.
        
        Args:
            backup_id: Identifier of the backup to verify
            
        Returns:
            Dict[str, Any]: Verification results including metadata and integrity checks
            
        Raises:
            RuntimeError: If the store is not initialized
            FileNotFoundError: If the backup does not exist
        """
        if not self.initialized:
            raise RuntimeError("Vector store not initialized")
            
        try:
            backup_path = os.path.join(self.backup_dir, backup_id)
            if not os.path.exists(backup_path):
                raise FileNotFoundError(f"Backup {backup_id} not found")
            
            # Load and verify metadata
            metadata_path = os.path.join(backup_path, "metadata.json")
            async with aiofiles.open(metadata_path, 'r') as f:
                metadata = json.loads(await f.read())
            
            # Load and verify indices
            content_path = os.path.join(backup_path, "content_index.faiss")
            user_path = os.path.join(backup_path, "user_index.faiss")
            
            content_index = faiss.read_index(content_path) if os.path.exists(content_path) else None
            user_index = faiss.read_index(user_path) if os.path.exists(user_path) else None
            
            # Load and verify Redis data
            redis_path = os.path.join(backup_path, "redis_data.json")
            async with aiofiles.open(redis_path, 'r') as f:
                redis_data = json.loads(await f.read())
            
            # Compile verification results
            return {
                "backup_id": backup_id,
                "timestamp": metadata["timestamp"],
                "content_index": {
                    "expected_size": metadata["content_vectors"],
                    "actual_size": content_index.ntotal if content_index else 0,
                    "verified": (content_index.ntotal if content_index else 0) == metadata["content_vectors"]
                },
                "user_index": {
                    "expected_size": metadata["user_vectors"],
                    "actual_size": user_index.ntotal if user_index else 0,
                    "verified": (user_index.ntotal if user_index else 0) == metadata["user_vectors"]
                },
                "redis_data": {
                    "expected_keys": metadata["redis_keys"],
                    "actual_keys": len(redis_data),
                    "verified": len(redis_data) == metadata["redis_keys"]
                },
                "model": {
                    "name": metadata["model_name"],
                    "dimension": metadata["dimension"],
                    "compatible": metadata["dimension"] == self.dimension
                }
            }
            
        except Exception as e:
            VECTOR_OPERATION_ERRORS.labels(operation='verify').inc()
            logger.error(f"Failed to verify backup {backup_id}: {e}")
            raise

    async def list_backups(self) -> List[Dict[str, Any]]:
        """List all available backups.
        
        Returns:
            List[Dict[str, Any]]: List of backup metadata, sorted by timestamp descending
            
        Raises:
            RuntimeError: If the store is not initialized
        """
        if not self.initialized:
            raise RuntimeError("Vector store not initialized")
            
        try:
            backups = []
            
            # List local backups
            if os.path.exists(self.backup_dir):
                for backup_id in os.listdir(self.backup_dir):
                    backup_path = os.path.join(self.backup_dir, backup_id)
                    metadata_path = os.path.join(backup_path, "metadata.json")
                    
                    if os.path.isfile(metadata_path):
                        async with aiofiles.open(metadata_path, 'r') as f:
                            metadata = json.loads(await f.read())
                            backups.append(metadata)
            
            # List S3 backups if configured
            if self.s3_bucket:
                s3 = boto3.client('s3')
                paginator = s3.get_paginator('list_objects_v2')
                
                async for page in paginator.paginate(
                    Bucket=self.s3_bucket,
                    Prefix=f"{self.s3_prefix}/"
                ):
                    for obj in page.get('Contents', []):
                        if obj['Key'].endswith('metadata.json'):
                            response = s3.get_object(
                                Bucket=self.s3_bucket,
                                Key=obj['Key']
                            )
                            metadata = json.loads(response['Body'].read())
                            
                            # Add S3-specific metadata
                            metadata['storage'] = 's3'
                            metadata['size'] = obj['Size']
                            metadata['last_modified'] = obj['LastModified'].isoformat()
                            
                            backups.append(metadata)
            
            # Sort by timestamp descending
            backups.sort(
                key=lambda x: datetime.fromisoformat(x['timestamp']),
                reverse=True
            )
            
            return backups
            
        except Exception as e:
            VECTOR_OPERATION_ERRORS.labels(operation='list_backups').inc()
            logger.error(f"Failed to list backups: {e}")
            raise

    async def cleanup_old_backups(
        self,
        retain_days: int = 7,
        retain_weekly: int = 4,
        retain_monthly: int = 6
    ) -> None:
        """Clean up old backups based on retention policy.
        
        Args:
            retain_days: Number of daily backups to retain
            retain_weekly: Number of weekly backups to retain
            retain_monthly: Number of monthly backups to retain
            
        Raises:
            RuntimeError: If the store is not initialized
        """
        if not self.initialized:
            raise RuntimeError("Vector store not initialized")
            
        try:
            backups = await self.list_backups()
            if not backups:
                return
                
            now = datetime.now(timezone.utc)
            to_delete = set()
            to_keep = set()
            
            # Group backups by time period
            daily_backups = []
            weekly_backups = []
            monthly_backups = []
            
            for backup in backups:
                timestamp = datetime.fromisoformat(backup['timestamp'])
                age = now - timestamp
                
                if age.days < retain_days:
                    daily_backups.append(backup)
                elif age.days < retain_days + (retain_weekly * 7):
                    if timestamp.weekday() == 0:  # Monday
                        weekly_backups.append(backup)
                elif age.days < retain_days + (retain_weekly * 7) + (retain_monthly * 30):
                    if timestamp.day == 1:  # First of month
                        monthly_backups.append(backup)
                else:
                    to_delete.add(backup['id'])
            
            # Keep the most recent backups according to retention policy
            to_keep.update(b['id'] for b in daily_backups[:retain_days])
            to_keep.update(b['id'] for b in weekly_backups[:retain_weekly])
            to_keep.update(b['id'] for b in monthly_backups[:retain_monthly])
            
            # Delete old backups
            for backup in backups:
                if backup['id'] not in to_keep:
                    backup_path = os.path.join(self.backup_dir, backup['id'])
                    if os.path.exists(backup_path):
                        shutil.rmtree(backup_path)
                    
                    # Delete from S3 if configured
                    if self.s3_bucket:
                        s3 = boto3.client('s3')
                        prefix = os.path.join(self.s3_prefix, backup['id'])
                        
                        # List and delete all backup files
                        response = s3.list_objects_v2(
                            Bucket=self.s3_bucket,
                            Prefix=prefix
                        )
                        
                        if 'Contents' in response:
                            for obj in response['Contents']:
                                s3.delete_object(
                                    Bucket=self.s3_bucket,
                                    Key=obj['Key']
                                )
            
            logger.info(
                f"Cleaned up old backups: kept {len(to_keep)} backups, "
                f"deleted {len(to_delete)} backups"
            )
            
        except Exception as e:
            VECTOR_OPERATION_ERRORS.labels(operation='cleanup').inc()
            logger.error(f"Failed to clean up old backups: {e}")
            raise
</file>

<file path="microservices/python/personalization-engine/requirements.txt">
aiofiles>=23.2.1
boto3>=1.34.0
botocore>=1.34.0
faiss-cpu>=1.7.4
fastapi>=0.109.0
loguru>=0.7.2
numpy>=1.24.4
pydantic>=2.5.3
python-dotenv>=1.0.0
redis>=5.0.1
sentence-transformers>=2.2.2
uvicorn>=0.27.0
nats-py==2.6.0
tenacity==8.2.3
prometheus-client==0.17.1
opentelemetry-api==1.21.0
opentelemetry-sdk==1.21.0
opentelemetry-exporter-otlp==1.21.0
pybreaker==1.0.1
-e ../common/nats_lib
</file>

<file path="SolnAI.code-workspace">
{
	"folders": [
		{
			"path": "/home/dislove/文档/SolnAI-agents-active/SolnAI"
		}
	]
}
</file>

</files>
